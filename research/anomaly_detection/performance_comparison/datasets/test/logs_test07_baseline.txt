opensearch  | [2025-12-22T02:06:05,674][INFO ][o.o.j.s.JobSweeper       ] [demo-node] Running full sweep
kafka       | [2025-12-22 02:05:49,362] INFO [LocalLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Rolled new log segment at offset 2488899 in 4 ms. (kafka.log.LocalLog)
kafka       | [2025-12-22 02:05:49,362] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 2488899 with 0 producer ids in 0 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
kafka       | [2025-12-22 02:05:53,359] INFO [SnapshotGenerator id=1] Creating new KRaft snapshot file snapshot 00000000000002488908-0000000008 because we have replayed at least 2800 bytes. (org.apache.kafka.image.publisher.SnapshotGenerator)
kafka       | [2025-12-22 02:05:53,368] INFO [SnapshotEmitter id=1] Successfully wrote snapshot 00000000000002488908-0000000008 (org.apache.kafka.image.publisher.SnapshotEmitter)
kafka       | [2025-12-22 02:06:04,366] INFO [LocalLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Rolled new log segment at offset 2488929 in 4 ms. (kafka.log.LocalLog)
kafka       | [2025-12-22 02:06:04,366] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 2488929 with 0 producer ids in 0 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
kafka       | [2025-12-22 02:06:12,864] INFO [SnapshotGenerator id=1] Creating new KRaft snapshot file snapshot 00000000000002488947-0000000008 because we have replayed at least 2800 bytes. (org.apache.kafka.image.publisher.SnapshotGenerator)
kafka       | [2025-12-22 02:06:12,874] INFO [SnapshotEmitter id=1] Successfully wrote snapshot 00000000000002488947-0000000008 (org.apache.kafka.image.publisher.SnapshotEmitter)
kafka       | [2025-12-22 02:06:19,371] INFO [LocalLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Rolled new log segment at offset 2488959 in 5 ms. (kafka.log.LocalLog)
kafka       | [2025-12-22 02:06:19,371] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 2488959 with 0 producer ids in 0 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
kafka       | [2025-12-22 02:06:30,623] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Incremented log start offset to 2488791 due to snapshot generated (kafka.log.UnifiedLog)
kafka       | [2025-12-22 02:06:30,623] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Deleting segments due to log start offset 2488791 breach: LogSegment(baseOffset=2488749, size=2160, lastModifiedTime=1766369088837, largestRecordTimestamp=1766369088814) (kafka.log.UnifiedLog)
kafka       | [2025-12-22 02:06:30,624] INFO [MetadataLog partition=__cluster_metadata-0, nodeId=1] Marking snapshot OffsetAndEpoch(offset=2488752, epoch=8) for deletion because its timestamp (1766369075310) is now (1766369190623) older than the
kafka       | retention (60000) (kafka.raft.KafkaMetadataLog)
kafka       | [2025-12-22 02:06:30,624] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Incremented log start offset to 2488830 due to snapshot generated (kafka.log.UnifiedLog)
kafka       | [2025-12-22 02:06:30,626] INFO [LocalLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Deleting segment files LogSegment(baseOffset=2488629, size=2160, lastModifiedTime=1766369028820, largestRecordTimestamp=1766369028796) (kafka.log.LocalLog$)
kafka       | [2025-12-22 02:06:30,626] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Deleting segments due to log start offset 2488830 breach: LogSegment(baseOffset=2488779, size=2160, lastModifiedTime=1766369103842, largestRecordTimestamp=1766369103818) (kafka.log.UnifiedLog)
kafka       | [2025-12-22 02:06:30,626] INFO Deleted log /tmp/kafka-logs/__cluster_metadata-0/00000000000002488629.log.deleted. (org.apache.kafka.storage.internals.log.LogSegment)
kafka       | [2025-12-22 02:06:30,626] INFO Deleted offset index /tmp/kafka-logs/__cluster_metadata-0/00000000000002488629.index.deleted. (org.apache.kafka.storage.internals.log.LogSegment)
kafka       | [2025-12-22 02:06:30,627] INFO Deleted time index /tmp/kafka-logs/__cluster_metadata-0/00000000000002488629.timeindex.deleted. (org.apache.kafka.storage.internals.log.LogSegment)
kafka       | [2025-12-22 02:06:30,627] INFO [MetadataLog partition=__cluster_metadata-0, nodeId=1] Marking snapshot OffsetAndEpoch(offset=2488791, epoch=8) for deletion because its timestamp (1766369094816) is now (1766369190624) older than the
kafka       | retention (60000) (kafka.raft.KafkaMetadataLog)
kafka       | [2025-12-22 02:06:30,627] INFO Deleted producer state snapshot /tmp/kafka-logs/__cluster_metadata-0/00000000000002488629.snapshot.deleted (org.apache.kafka.storage.internals.log.SnapshotFile)
kafka       | [2025-12-22 02:06:30,627] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Incremented log start offset to 2488869 due to snapshot generated (kafka.log.UnifiedLog)
kafka       | [2025-12-22 02:06:30,627] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Deleting segments due to log start offset 2488869 breach: LogSegment(baseOffset=2488809, size=2160, lastModifiedTime=1766369118845, largestRecordTimestamp=1766369118822),LogSegment(baseOffset=2488839, size=2160, lastModifiedTime=1766369133850, largestRecordTimestamp=1766369133827) (kafka.log.UnifiedLog)
kafka       | [2025-12-22 02:06:30,627] INFO Deleted snapshot files for snapshot OffsetAndEpoch(offset=2488635, epoch=8). (org.apache.kafka.snapshot.Snapshots)
kafka       | [2025-12-22 02:06:30,627] INFO [LocalLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Deleting segment files LogSegment(baseOffset=2488659, size=2160, lastModifiedTime=1766369043824, largestRecordTimestamp=1766369043801) (kafka.log.LocalLog$)
kafka       | [2025-12-22 02:06:30,627] INFO Deleted log /tmp/kafka-logs/__cluster_metadata-0/00000000000002488659.log.deleted. (org.apache.kafka.storage.internals.log.LogSegment)
kafka       | [2025-12-22 02:06:30,627] INFO Deleted offset index /tmp/kafka-logs/__cluster_metadata-0/00000000000002488659.index.deleted. (org.apache.kafka.storage.internals.log.LogSegment)
kafka       | [2025-12-22 02:06:30,628] INFO Deleted time index /tmp/kafka-logs/__cluster_metadata-0/00000000000002488659.timeindex.deleted. (org.apache.kafka.storage.internals.log.LogSegment)
kafka       | [2025-12-22 02:06:30,628] INFO Deleted producer state snapshot /tmp/kafka-logs/__cluster_metadata-0/00000000000002488659.snapshot.deleted (org.apache.kafka.storage.internals.log.SnapshotFile)
kafka       | [2025-12-22 02:06:30,628] INFO [MetadataLog partition=__cluster_metadata-0, nodeId=1] Marking snapshot OffsetAndEpoch(offset=2488830, epoch=8) for deletion because its timestamp (1766369114321) is now (1766369190627) older than the
kafka       | retention (60000) (kafka.raft.KafkaMetadataLog)
kafka       | [2025-12-22 02:06:30,628] INFO Deleted snapshot files for snapshot OffsetAndEpoch(offset=2488674, epoch=8). (org.apache.kafka.snapshot.Snapshots)
kafka       | [2025-12-22 02:06:30,630] INFO [LocalLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Deleting segment files LogSegment(baseOffset=2488689, size=2160, lastModifiedTime=1766369058829, largestRecordTimestamp=1766369058805),LogSegment(baseOffset=2488719, size=2160, lastModifiedTime=1766369073832, largestRecordTimestamp=1766369073809) (kafka.log.LocalLog$)
kafka       | [2025-12-22 02:06:30,630] INFO Deleted log /tmp/kafka-logs/__cluster_metadata-0/00000000000002488689.log.deleted. (org.apache.kafka.storage.internals.log.LogSegment)
kafka       | [2025-12-22 02:06:30,630] INFO Deleted offset index /tmp/kafka-logs/__cluster_metadata-0/00000000000002488689.index.deleted. (org.apache.kafka.storage.internals.log.LogSegment)
kafka       | [2025-12-22 02:06:30,630] INFO Deleted time index /tmp/kafka-logs/__cluster_metadata-0/00000000000002488689.timeindex.deleted. (org.apache.kafka.storage.internals.log.LogSegment)
kafka       | [2025-12-22 02:06:30,630] INFO Deleted log /tmp/kafka-logs/__cluster_metadata-0/00000000000002488719.log.deleted. (org.apache.kafka.storage.internals.log.LogSegment)
kafka       | [2025-12-22 02:06:30,630] INFO Deleted offset index /tmp/kafka-logs/__cluster_metadata-0/00000000000002488719.index.deleted. (org.apache.kafka.storage.internals.log.LogSegment)
kafka       | [2025-12-22 02:06:30,630] INFO Deleted time index /tmp/kafka-logs/__cluster_metadata-0/00000000000002488719.timeindex.deleted. (org.apache.kafka.storage.internals.log.LogSegment)
kafka       | [2025-12-22 02:06:30,630] INFO Deleted producer state snapshot /tmp/kafka-logs/__cluster_metadata-0/00000000000002488689.snapshot.deleted (org.apache.kafka.storage.internals.log.SnapshotFile)
kafka       | [2025-12-22 02:06:30,630] INFO Deleted producer state snapshot /tmp/kafka-logs/__cluster_metadata-0/00000000000002488719.snapshot.deleted (org.apache.kafka.storage.internals.log.SnapshotFile)
kafka       | [2025-12-22 02:06:30,630] INFO Deleted snapshot files for snapshot OffsetAndEpoch(offset=2488713, epoch=8). (org.apache.kafka.snapshot.Snapshots)
grafana     | logger=ngalert.state.manager rule_uid=des78nlna99tsf org_id=1 t=2025-12-22T02:06:20.008096191Z level=error msg="Error in expanding template" error="failed to expand template '{{- $labels := .Labels -}}{{- $values := .Values -}}{{- $value := .Value -}}The 95th percentile response time for operation {{ $labels.service_namespace\n    }}/{{ $labels.service_name }} \"{{ $labels.http_request_method }} {{\n    $labels.http_route }}\" has been\n    above xxx seconds for 2 minutes on {{ $labels.service_instance_id}}. Current\n    value: {{ .Value | humanizeDuration }}.': error executing template __alert_CartAddItemHighLatency: template: __alert_CartAddItemHighLatency:5:23: executing \"__alert_CartAddItemHighLatency\" at <humanizeDuration>: error calling humanizeDuration: strconv.ParseFloat: parsing \"\": invalid syntax"
grafana     | logger=ngalert.sender.router rule_uid=des78nlna99tsf org_id=1 t=2025-12-22T02:06:20.021773089Z level=info msg="Sending alerts to local notifier" count=1
prometheus  | time=2025-12-22T02:05:54.518Z level=ERROR source=write_handler.go:719 msg="Error appending remote write" component=web err="too old sample; too old sample; too old sample; too old sample; too old sample; too old sample; too old sample"
otel-collector  | 2025-12-22T02:05:49.806Z	info	Logs	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 2}
otel-collector  | 2025-12-22T02:05:49.827Z	info	Traces	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "resource spans": 1, "spans": 1}
otel-collector  | 2025-12-22T02:05:50.533Z	info	Metrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 4, "data points": 7}
otel-collector  | 2025-12-22T02:05:50.533Z	info	Metrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 29, "data points": 34}
otel-collector  | 2025-12-22T02:05:50.546Z	info	Metrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 12, "metrics": 63, "data points": 110}
otel-collector  | 2025-12-22T02:05:53.591Z	info	Metrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 25, "data points": 40}
otel-collector  | 2025-12-22T02:05:53.807Z	info	Logs	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 2}
otel-collector  | 2025-12-22T02:05:53.810Z	info	Metrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 24, "data points": 39}
otel-collector  | 2025-12-22T02:05:54.188Z	info	Metrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 5, "data points": 34}
otel-collector  | 2025-12-22T02:05:54.274Z	info	Metrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 16, "data points": 61}
otel-collector  | 2025-12-22T02:05:54.277Z	info	Metrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 15, "data points": 60}
otel-collector  | 2025-12-22T02:05:54.381Z	info	Logs	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 1}
otel-collector  | 2025-12-22T02:05:54.403Z	info	Traces	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "resource spans": 1, "spans": 1}
otel-collector  | 2025-12-22T02:05:54.517Z	info	Metrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 6, "data points": 16}
otel-collector  | 2025-12-22T02:05:54.518Z	error	internal/queue_sender.go:49	Exporting failed. Dropping data.	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "otlphttp/prometheus", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "error": "not retryable error: Permanent error: rpc error: code = Unknown desc = error exporting items, request to http://prometheus:9090/api/v1/otlp/v1/metrics responded with HTTP Status Code 500", "dropped_items": 16}
otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal.NewQueueSender.func1
otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queue_sender.go:49
otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/queuebatch.(*disabledBatcher[...]).Consume
otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queuebatch/disabled_batcher.go:23
otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/queue.(*asyncQueue[...]).Start.func1
otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queue/async_queue.go:49
otel-collector  | 2025-12-22T02:05:54.527Z	info	Metrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 51, "data points": 181}
otel-collector  | 2025-12-22T02:05:54.527Z	error	internal/base_exporter.go:114	Exporting failed. Rejecting data.	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "error": "sending queue is full", "rejected_items": 66}
otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send
otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/base_exporter.go:114
otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal.NewMetricsRequest.newConsumeMetrics.func1
otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/new_request.go:176
otel-collector  | go.opentelemetry.io/collector/consumer.ConsumeMetricsFunc.ConsumeMetrics
otel-collector  | 	go.opentelemetry.io/collector/consumer@v1.45.0/metrics.go:27
otel-collector  | go.opentelemetry.io/collector/service/internal/refconsumer.refMetrics.ConsumeMetrics
otel-collector  | 	go.opentelemetry.io/collector/service@v0.139.0/internal/refconsumer/metrics.go:29
otel-collector  | go.opentelemetry.io/collector/internal/fanoutconsumer.(*metricsConsumer).ConsumeMetrics
otel-collector  | 	go.opentelemetry.io/collector/internal/fanoutconsumer@v0.139.0/metrics.go:71
otel-collector  | go.opentelemetry.io/collector/processor/processorhelper.NewMetrics.func1
otel-collector  | 	go.opentelemetry.io/collector/processor/processorhelper@v0.139.0/metrics.go:71
otel-collector  | go.opentelemetry.io/collector/consumer.ConsumeMetricsFunc.ConsumeMetrics
otel-collector  | 	go.opentelemetry.io/collector/consumer@v1.45.0/metrics.go:27
otel-collector  | go.opentelemetry.io/collector/service/internal/refconsumer.refMetrics.ConsumeMetrics
otel-collector  | 	go.opentelemetry.io/collector/service@v0.139.0/internal/refconsumer/metrics.go:29
otel-collector  | go.opentelemetry.io/collector/processor/processorhelper.NewMetrics.func1
otel-collector  | 	go.opentelemetry.io/collector/processor/processorhelper@v0.139.0/metrics.go:71
otel-collector  | go.opentelemetry.io/collector/consumer.ConsumeMetricsFunc.ConsumeMetrics
otel-collector  | 	go.opentelemetry.io/collector/consumer@v1.45.0/metrics.go:27
otel-collector  | go.opentelemetry.io/collector/service/internal/refconsumer.refMetrics.ConsumeMetrics
otel-collector  | 	go.opentelemetry.io/collector/service@v0.139.0/internal/refconsumer/metrics.go:29
otel-collector  | go.opentelemetry.io/collector/consumer.ConsumeMetricsFunc.ConsumeMetrics
otel-collector  | 	go.opentelemetry.io/collector/consumer@v1.45.0/metrics.go:27
otel-collector  | go.opentelemetry.io/collector/internal/fanoutconsumer.(*metricsConsumer).ConsumeMetrics
otel-collector  | 	go.opentelemetry.io/collector/internal/fanoutconsumer@v0.139.0/metrics.go:60
otel-collector  | github.com/open-telemetry/opentelemetry-collector-contrib/connector/spanmetricsconnector.(*connectorImp).exportMetrics
otel-collector  | 	github.com/open-telemetry/opentelemetry-collector-contrib/connector/spanmetricsconnector@v0.139.0/connector.go:269
otel-collector  | github.com/open-telemetry/opentelemetry-collector-contrib/connector/spanmetricsconnector.(*connectorImp).Start.func1
otel-collector  | 	github.com/open-telemetry/opentelemetry-collector-contrib/connector/spanmetricsconnector@v0.139.0/connector.go:224
otel-collector  | 2025-12-22T02:05:54.527Z	error	spanmetricsconnector@v0.139.0/connector.go:270	Failed ConsumeMetrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "spanmetrics", "otelcol.component.kind": "connector", "otelcol.signal": "traces", "otelcol.signal.output": "metrics", "error": "sending queue is full"}
otel-collector  | github.com/open-telemetry/opentelemetry-collector-contrib/connector/spanmetricsconnector.(*connectorImp).exportMetrics
otel-collector  | 	github.com/open-telemetry/opentelemetry-collector-contrib/connector/spanmetricsconnector@v0.139.0/connector.go:270
otel-collector  | github.com/open-telemetry/opentelemetry-collector-contrib/connector/spanmetricsconnector.(*connectorImp).Start.func1
otel-collector  | 	github.com/open-telemetry/opentelemetry-collector-contrib/connector/spanmetricsconnector@v0.139.0/connector.go:224
otel-collector  | 2025-12-22T02:05:54.577Z	info	Metrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 8, "data points": 9}
otel-collector  | 2025-12-22T02:05:54.799Z	info	Metrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 28, "data points": 40}
otel-collector  | 2025-12-22T02:05:54.918Z	info	Metrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 14, "data points": 57}
otel-collector  | 2025-12-22T02:05:55.245Z	info	Logs	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 1}
otel-collector  | 2025-12-22T02:05:55.821Z	info	Logs	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 1}
otel-collector  | 2025-12-22T02:05:56.077Z	info	Traces	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "resource spans": 1, "spans": 2}
otel-collector  | 2025-12-22T02:05:58.786Z	info	Logs	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 1}
otel-collector  | 2025-12-22T02:05:59.831Z	info	Traces	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "resource spans": 1, "spans": 1}
otel-collector  | 2025-12-22T02:06:00.554Z	error	scraperhelper@v0.139.0/obs_metrics.go:61	Error scraping metrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "hostmetrics", "otelcol.component.kind": "receiver", "otelcol.signal": "metrics", "error": "failed to read usage at /hostfs/etc/otelcol-config-extras.yml: no such file or directory; failed to read usage at /hostfs/etc/otelcol-config.yml: no such file or directory"}
otel-collector  | go.opentelemetry.io/collector/scraper/scraperhelper.wrapObsMetrics.func1
otel-collector  | 	go.opentelemetry.io/collector/scraper/scraperhelper@v0.139.0/obs_metrics.go:61
otel-collector  | go.opentelemetry.io/collector/scraper.ScrapeMetricsFunc.ScrapeMetrics
otel-collector  | 	go.opentelemetry.io/collector/scraper@v0.139.0/metrics.go:24
otel-collector  | go.opentelemetry.io/collector/scraper/scraperhelper.scrapeMetrics
otel-collector  | 	go.opentelemetry.io/collector/scraper/scraperhelper@v0.139.0/controller.go:256
otel-collector  | go.opentelemetry.io/collector/scraper/scraperhelper.NewMetricsController.func1
otel-collector  | 	go.opentelemetry.io/collector/scraper/scraperhelper@v0.139.0/controller.go:228
otel-collector  | go.opentelemetry.io/collector/scraper/scraperhelper.(*controller[...]).startScraping.func1
otel-collector  | 	go.opentelemetry.io/collector/scraper/scraperhelper@v0.139.0/controller.go:175
otel-collector  | 2025-12-22T02:06:00.895Z	info	Logs	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 1}
otel-collector  | 2025-12-22T02:06:01.043Z	info	Traces	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "resource spans": 1, "spans": 2}
otel-collector  | 2025-12-22T02:06:03.593Z	info	Metrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 25, "data points": 40}
otel-collector  | 2025-12-22T02:06:04.414Z	info	Traces	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "resource spans": 1, "spans": 1}
otel-collector  | 2025-12-22T02:06:04.527Z	info	Metrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 51, "data points": 181}
otel-collector  | 2025-12-22T02:06:04.811Z	info	Logs	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 2}
otel-collector  | 2025-12-22T02:06:05.141Z	info	Logs	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 2}
otel-collector  | 2025-12-22T02:06:05.538Z	info	Traces	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "resource spans": 1, "spans": 4}
otel-collector  | 2025-12-22T02:06:05.822Z	info	Logs	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 1}
otel-collector  | 2025-12-22T02:06:06.079Z	info	Traces	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "resource spans": 1, "spans": 2}
otel-collector  | 2025-12-22T02:06:09.834Z	info	Traces	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "resource spans": 1, "spans": 1}
otel-collector  | 2025-12-22T02:06:10.533Z	info	Metrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 29, "data points": 34}
otel-collector  | 2025-12-22T02:06:10.533Z	info	Metrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 4, "data points": 7}
otel-collector  | 2025-12-22T02:06:10.545Z	info	Metrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 12, "metrics": 63, "data points": 110}
otel-collector  | 2025-12-22T02:06:13.593Z	info	Metrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 25, "data points": 40}
otel-collector  | 2025-12-22T02:06:13.815Z	info	Logs	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 2}
otel-collector  | 2025-12-22T02:06:13.971Z	info	Logs	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 1}
otel-collector  | 2025-12-22T02:06:14.423Z	info	Traces	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "resource spans": 1, "spans": 1}
otel-collector  | 2025-12-22T02:06:14.528Z	info	Metrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 51, "data points": 181}
otel-collector  | 2025-12-22T02:06:15.168Z	info	Logs	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 2}
otel-collector  | 2025-12-22T02:06:15.825Z	info	Logs	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 1}
otel-collector  | 2025-12-22T02:06:16.079Z	info	Traces	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "resource spans": 1, "spans": 2}
otel-collector  | 2025-12-22T02:06:19.819Z	info	Logs	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 2}
otel-collector  | 2025-12-22T02:06:19.838Z	info	Traces	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "resource spans": 1, "spans": 1}
otel-collector  | 2025-12-22T02:06:20.534Z	info	Metrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 4, "data points": 7}
otel-collector  | 2025-12-22T02:06:20.534Z	info	Metrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 29, "data points": 34}
otel-collector  | 2025-12-22T02:06:20.553Z	info	Metrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 12, "metrics": 63, "data points": 110}
otel-collector  | 2025-12-22T02:06:23.593Z	info	Metrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 25, "data points": 40}
otel-collector  | 2025-12-22T02:06:24.433Z	info	Traces	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "resource spans": 1, "spans": 1}
otel-collector  | 2025-12-22T02:06:24.526Z	info	Metrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 51, "data points": 181}
otel-collector  | 2025-12-22T02:06:25.157Z	info	Logs	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 2}
otel-collector  | 2025-12-22T02:06:25.826Z	info	Logs	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 1}
otel-collector  | 2025-12-22T02:06:26.079Z	info	Traces	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "resource spans": 1, "spans": 2}
otel-collector  | 2025-12-22T02:06:29.841Z	info	Traces	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "resource spans": 1, "spans": 1}
otel-collector  | 2025-12-22T02:06:29.846Z	info	Logs	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 1}
otel-collector  | 2025-12-22T02:06:30.533Z	info	Metrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 29, "data points": 34}
otel-collector  | 2025-12-22T02:06:30.533Z	info	Metrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 4, "data points": 7}
otel-collector  | 2025-12-22T02:06:30.548Z	info	Metrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 12, "metrics": 63, "data points": 110}
otel-collector  | 2025-12-22T02:06:30.823Z	info	Logs	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 31}
