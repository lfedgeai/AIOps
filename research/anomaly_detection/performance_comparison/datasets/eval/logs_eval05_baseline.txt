kafka  | [2025-12-22 01:23:18,103] INFO [SnapshotGenerator id=1] Creating new KRaft snapshot file snapshot 00000000000002483799-0000000008 because we have replayed at least 2800 bytes. (org.apache.kafka.image.publisher.SnapshotGenerator)
kafka  | [2025-12-22 01:23:18,113] INFO [SnapshotEmitter id=1] Successfully wrote snapshot 00000000000002483799-0000000008 (org.apache.kafka.image.publisher.SnapshotEmitter)
kafka  | [2025-12-22 01:23:18,607] INFO [LocalLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Rolled new log segment at offset 2483799 in 4 ms. (kafka.log.LocalLog)
kafka  | [2025-12-22 01:23:18,607] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 2483799 with 0 producer ids in 0 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
kafka  | [2025-12-22 01:23:30,615] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Incremented log start offset to 2483643 due to snapshot generated (kafka.log.UnifiedLog)
kafka  | [2025-12-22 01:23:30,615] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Deleting segments due to log start offset 2483643 breach: LogSegment(baseOffset=2483589, size=2160, lastModifiedTime=1766366508078, largestRecordTimestamp=1766366508053) (kafka.log.UnifiedLog)
kafka  | [2025-12-22 01:23:30,616] INFO [MetadataLog partition=__cluster_metadata-0, nodeId=1] Marking snapshot OffsetAndEpoch(offset=2483604, epoch=8) for deletion because its timestamp (1766366500551) is now (1766366610615) older than the
kafka  | retention (60000) (kafka.raft.KafkaMetadataLog)
kafka  | [2025-12-22 01:23:30,616] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Incremented log start offset to 2483682 due to snapshot generated (kafka.log.UnifiedLog)
kafka  | [2025-12-22 01:23:30,618] INFO [LocalLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Deleting segment files LogSegment(baseOffset=2483469, size=2160, lastModifiedTime=1766366448063, largestRecordTimestamp=1766366448038) (kafka.log.LocalLog$)
kafka  | [2025-12-22 01:23:30,618] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Deleting segments due to log start offset 2483682 breach: LogSegment(baseOffset=2483619, size=2160, lastModifiedTime=1766366523082, largestRecordTimestamp=1766366523057),LogSegment(baseOffset=2483649, size=2160, lastModifiedTime=1766366538086, largestRecordTimestamp=1766366538061) (kafka.log.UnifiedLog)
kafka  | [2025-12-22 01:23:30,618] INFO Deleted log /tmp/kafka-logs/__cluster_metadata-0/00000000000002483469.log.deleted. (org.apache.kafka.storage.internals.log.LogSegment)
kafka  | [2025-12-22 01:23:30,618] INFO Deleted offset index /tmp/kafka-logs/__cluster_metadata-0/00000000000002483469.index.deleted. (org.apache.kafka.storage.internals.log.LogSegment)
kafka  | [2025-12-22 01:23:30,618] INFO Deleted time index /tmp/kafka-logs/__cluster_metadata-0/00000000000002483469.timeindex.deleted. (org.apache.kafka.storage.internals.log.LogSegment)
kafka  | [2025-12-22 01:23:30,619] INFO Deleted producer state snapshot /tmp/kafka-logs/__cluster_metadata-0/00000000000002483469.snapshot.deleted (org.apache.kafka.storage.internals.log.SnapshotFile)
kafka  | [2025-12-22 01:23:30,619] INFO [MetadataLog partition=__cluster_metadata-0, nodeId=1] Marking snapshot OffsetAndEpoch(offset=2483643, epoch=8) for deletion because its timestamp (1766366520056) is now (1766366610616) older than the
kafka  | retention (60000) (kafka.raft.KafkaMetadataLog)
kafka  | [2025-12-22 01:23:30,619] INFO Deleted snapshot files for snapshot OffsetAndEpoch(offset=2483487, epoch=8). (org.apache.kafka.snapshot.Snapshots)
kafka  | [2025-12-22 01:23:30,619] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Incremented log start offset to 2483721 due to snapshot generated (kafka.log.UnifiedLog)
kafka  | [2025-12-22 01:23:30,621] INFO [LocalLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Deleting segment files LogSegment(baseOffset=2483499, size=2160, lastModifiedTime=1766366463066, largestRecordTimestamp=1766366463041),LogSegment(baseOffset=2483529, size=2160, lastModifiedTime=1766366478070, largestRecordTimestamp=1766366478045) (kafka.log.LocalLog$)
kafka  | [2025-12-22 01:23:30,621] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Deleting segments due to log start offset 2483721 breach: LogSegment(baseOffset=2483679, size=2160, lastModifiedTime=1766366553090, largestRecordTimestamp=1766366553066) (kafka.log.UnifiedLog)
kafka  | [2025-12-22 01:23:30,621] INFO Deleted log /tmp/kafka-logs/__cluster_metadata-0/00000000000002483499.log.deleted. (org.apache.kafka.storage.internals.log.LogSegment)
kafka  | [2025-12-22 01:23:30,622] INFO Deleted offset index /tmp/kafka-logs/__cluster_metadata-0/00000000000002483499.index.deleted. (org.apache.kafka.storage.internals.log.LogSegment)
kafka  | [2025-12-22 01:23:30,622] INFO Deleted time index /tmp/kafka-logs/__cluster_metadata-0/00000000000002483499.timeindex.deleted. (org.apache.kafka.storage.internals.log.LogSegment)
kafka  | [2025-12-22 01:23:30,622] INFO [MetadataLog partition=__cluster_metadata-0, nodeId=1] Marking snapshot OffsetAndEpoch(offset=2483682, epoch=8) for deletion because its timestamp (1766366539562) is now (1766366610619) older than the
kafka  | retention (60000) (kafka.raft.KafkaMetadataLog)
kafka  | [2025-12-22 01:23:30,622] INFO Deleted log /tmp/kafka-logs/__cluster_metadata-0/00000000000002483529.log.deleted. (org.apache.kafka.storage.internals.log.LogSegment)
kafka  | [2025-12-22 01:23:30,622] INFO Deleted offset index /tmp/kafka-logs/__cluster_metadata-0/00000000000002483529.index.deleted. (org.apache.kafka.storage.internals.log.LogSegment)
kafka  | [2025-12-22 01:23:30,622] INFO Deleted time index /tmp/kafka-logs/__cluster_metadata-0/00000000000002483529.timeindex.deleted. (org.apache.kafka.storage.internals.log.LogSegment)
kafka  | [2025-12-22 01:23:30,622] INFO Deleted producer state snapshot /tmp/kafka-logs/__cluster_metadata-0/00000000000002483499.snapshot.deleted (org.apache.kafka.storage.internals.log.SnapshotFile)
kafka  | [2025-12-22 01:23:30,622] INFO Deleted producer state snapshot /tmp/kafka-logs/__cluster_metadata-0/00000000000002483529.snapshot.deleted (org.apache.kafka.storage.internals.log.SnapshotFile)
kafka  | [2025-12-22 01:23:30,622] INFO Deleted snapshot files for snapshot OffsetAndEpoch(offset=2483526, epoch=8). (org.apache.kafka.snapshot.Snapshots)
kafka  | [2025-12-22 01:23:30,622] INFO [LocalLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Deleting segment files LogSegment(baseOffset=2483559, size=2160, lastModifiedTime=1766366493074, largestRecordTimestamp=1766366493049) (kafka.log.LocalLog$)
kafka  | [2025-12-22 01:23:30,623] INFO Deleted log /tmp/kafka-logs/__cluster_metadata-0/00000000000002483559.log.deleted. (org.apache.kafka.storage.internals.log.LogSegment)
kafka  | [2025-12-22 01:23:30,623] INFO Deleted offset index /tmp/kafka-logs/__cluster_metadata-0/00000000000002483559.index.deleted. (org.apache.kafka.storage.internals.log.LogSegment)
kafka  | [2025-12-22 01:23:30,623] INFO Deleted time index /tmp/kafka-logs/__cluster_metadata-0/00000000000002483559.timeindex.deleted. (org.apache.kafka.storage.internals.log.LogSegment)
kafka  | [2025-12-22 01:23:30,623] INFO Deleted producer state snapshot /tmp/kafka-logs/__cluster_metadata-0/00000000000002483559.snapshot.deleted (org.apache.kafka.storage.internals.log.SnapshotFile)
kafka  | [2025-12-22 01:23:30,623] INFO Deleted snapshot files for snapshot OffsetAndEpoch(offset=2483565, epoch=8). (org.apache.kafka.snapshot.Snapshots)
kafka  | [2025-12-22 01:23:33,609] INFO [LocalLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Rolled new log segment at offset 2483829 in 3 ms. (kafka.log.LocalLog)
kafka  | [2025-12-22 01:23:33,609] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 2483829 with 0 producer ids in 0 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
kafka  | [2025-12-22 01:23:37,607] INFO [SnapshotGenerator id=1] Creating new KRaft snapshot file snapshot 00000000000002483838-0000000008 because we have replayed at least 2800 bytes. (org.apache.kafka.image.publisher.SnapshotGenerator)
kafka  | [2025-12-22 01:23:37,611] INFO [SnapshotEmitter id=1] Successfully wrote snapshot 00000000000002483838-0000000008 (org.apache.kafka.image.publisher.SnapshotEmitter)
kafka  | [2025-12-22 01:23:48,615] INFO [LocalLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Rolled new log segment at offset 2483859 in 4 ms. (kafka.log.LocalLog)
kafka  | [2025-12-22 01:23:48,615] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 2483859 with 0 producer ids in 0 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
kafka  | [2025-12-22 01:23:57,113] INFO [SnapshotGenerator id=1] Creating new KRaft snapshot file snapshot 00000000000002483877-0000000008 because we have replayed at least 2800 bytes. (org.apache.kafka.image.publisher.SnapshotGenerator)
kafka  | [2025-12-22 01:23:57,123] INFO [SnapshotEmitter id=1] Successfully wrote snapshot 00000000000002483877-0000000008 (org.apache.kafka.image.publisher.SnapshotEmitter)
prometheus  | time=2025-12-22T01:23:54.491Z level=ERROR source=write_handler.go:719 msg="Error appending remote write" component=web err="too old sample; too old sample; too old sample; too old sample; too old sample; too old sample; too old sample"
grafana     | logger=ngalert.state.manager rule_uid=des78nlna99tsf org_id=1 t=2025-12-22T01:23:20.006330194Z level=error msg="Error in expanding template" error="failed to expand template '{{- $labels := .Labels -}}{{- $values := .Values -}}{{- $value := .Value -}}The 95th percentile response time for operation {{ $labels.service_namespace\n    }}/{{ $labels.service_name }} \"{{ $labels.http_request_method }} {{\n    $labels.http_route }}\" has been\n    above xxx seconds for 2 minutes on {{ $labels.service_instance_id}}. Current\n    value: {{ .Value | humanizeDuration }}.': error executing template __alert_CartAddItemHighLatency: template: __alert_CartAddItemHighLatency:5:23: executing \"__alert_CartAddItemHighLatency\" at <humanizeDuration>: error calling humanizeDuration: strconv.ParseFloat: parsing \"\": invalid syntax"
grafana     | logger=ngalert.sender.router rule_uid=des78nlna99tsf org_id=1 t=2025-12-22T01:23:20.018171719Z level=info msg="Sending alerts to local notifier" count=1
otel-collector  | 2025-12-22T01:23:15.978Z	info	Traces	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "resource spans": 1, "spans": 2}
otel-collector  | 2025-12-22T01:23:18.769Z	info	Logs	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 4}
otel-collector  | 2025-12-22T01:23:20.534Z	info	Metrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 4, "data points": 7}
otel-collector  | 2025-12-22T01:23:20.534Z	error	internal/base_exporter.go:114	Exporting failed. Rejecting data.	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "error": "sending queue is full", "rejected_items": 34}
otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send
otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/base_exporter.go:114
otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal.NewMetricsRequest.newConsumeMetrics.func1
otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/new_request.go:176
otel-collector  | go.opentelemetry.io/collector/consumer.ConsumeMetricsFunc.ConsumeMetrics
otel-collector  | 	go.opentelemetry.io/collector/consumer@v1.45.0/metrics.go:27
otel-collector  | go.opentelemetry.io/collector/service/internal/refconsumer.refMetrics.ConsumeMetrics
otel-collector  | 	go.opentelemetry.io/collector/service@v0.139.0/internal/refconsumer/metrics.go:29
otel-collector  | go.opentelemetry.io/collector/internal/fanoutconsumer.(*metricsConsumer).ConsumeMetrics
otel-collector  | 	go.opentelemetry.io/collector/internal/fanoutconsumer@v0.139.0/metrics.go:71
otel-collector  | go.opentelemetry.io/collector/processor/processorhelper.NewMetrics.func1
otel-collector  | 	go.opentelemetry.io/collector/processor/processorhelper@v0.139.0/metrics.go:71
otel-collector  | go.opentelemetry.io/collector/consumer.ConsumeMetricsFunc.ConsumeMetrics
otel-collector  | 	go.opentelemetry.io/collector/consumer@v1.45.0/metrics.go:27
otel-collector  | go.opentelemetry.io/collector/service/internal/refconsumer.refMetrics.ConsumeMetrics
otel-collector  | 	go.opentelemetry.io/collector/service@v0.139.0/internal/refconsumer/metrics.go:29
otel-collector  | go.opentelemetry.io/collector/processor/processorhelper.NewMetrics.func1
otel-collector  | 	go.opentelemetry.io/collector/processor/processorhelper@v0.139.0/metrics.go:71
otel-collector  | go.opentelemetry.io/collector/consumer.ConsumeMetricsFunc.ConsumeMetrics
otel-collector  | 	go.opentelemetry.io/collector/consumer@v1.45.0/metrics.go:27
otel-collector  | go.opentelemetry.io/collector/service/internal/refconsumer.refMetrics.ConsumeMetrics
otel-collector  | 	go.opentelemetry.io/collector/service@v0.139.0/internal/refconsumer/metrics.go:29
otel-collector  | go.opentelemetry.io/collector/consumer.ConsumeMetricsFunc.ConsumeMetrics
otel-collector  | 	go.opentelemetry.io/collector/consumer@v1.45.0/metrics.go:27
otel-collector  | go.opentelemetry.io/collector/internal/fanoutconsumer.(*metricsConsumer).ConsumeMetrics
otel-collector  | 	go.opentelemetry.io/collector/internal/fanoutconsumer@v0.139.0/metrics.go:60
otel-collector  | go.opentelemetry.io/collector/scraper/scraperhelper.scrapeMetrics
otel-collector  | 	go.opentelemetry.io/collector/scraper/scraperhelper@v0.139.0/controller.go:265
otel-collector  | go.opentelemetry.io/collector/scraper/scraperhelper.NewMetricsController.func1
otel-collector  | 	go.opentelemetry.io/collector/scraper/scraperhelper@v0.139.0/controller.go:228
otel-collector  | go.opentelemetry.io/collector/scraper/scraperhelper.(*controller[...]).startScraping.func1
otel-collector  | 	go.opentelemetry.io/collector/scraper/scraperhelper@v0.139.0/controller.go:175
otel-collector  | 2025-12-22T01:23:20.550Z	info	Metrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 12, "metrics": 63, "data points": 110}
otel-collector  | 2025-12-22T01:23:22.416Z	info	Traces	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "resource spans": 1, "spans": 1}
otel-collector  | 2025-12-22T01:23:23.340Z	info	Logs	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 1}
otel-collector  | 2025-12-22T01:23:23.592Z	info	Metrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 25, "data points": 40}
otel-collector  | 2025-12-22T01:23:23.950Z	info	Traces	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "resource spans": 1, "spans": 1}
otel-collector  | 2025-12-22T01:23:24.528Z	info	Metrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 51, "data points": 181}
otel-collector  | 2025-12-22T01:23:25.167Z	info	Logs	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 2}
otel-collector  | 2025-12-22T01:23:25.445Z	info	Logs	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 1}
otel-collector  | 2025-12-22T01:23:25.979Z	info	Traces	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "resource spans": 1, "spans": 2}
otel-collector  | 2025-12-22T01:23:28.341Z	info	Logs	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 1}
otel-collector  | 2025-12-22T01:23:30.472Z	info	Metrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 31, "data points": 442}
otel-collector  | 2025-12-22T01:23:30.534Z	info	Metrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 4, "data points": 7}
otel-collector  | 2025-12-22T01:23:30.534Z	info	Metrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 29, "data points": 34}
otel-collector  | 2025-12-22T01:23:30.548Z	info	Metrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 12, "metrics": 63, "data points": 110}
otel-collector  | 2025-12-22T01:23:30.774Z	info	Logs	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 31}
otel-collector  | 2025-12-22T01:23:32.424Z	info	Traces	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "resource spans": 1, "spans": 1}
otel-collector  | 2025-12-22T01:23:33.342Z	info	Logs	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 1}
otel-collector  | 2025-12-22T01:23:33.593Z	info	Metrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 25, "data points": 40}
otel-collector  | 2025-12-22T01:23:33.776Z	info	Logs	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 2}
otel-collector  | 2025-12-22T01:23:33.953Z	info	Traces	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "resource spans": 1, "spans": 1}
otel-collector  | 2025-12-22T01:23:34.061Z	info	Metrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 31, "data points": 442}
otel-collector  | 2025-12-22T01:23:34.526Z	info	Metrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 51, "data points": 181}
otel-collector  | 2025-12-22T01:23:35.139Z	info	Logs	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 2}
otel-collector  | 2025-12-22T01:23:35.980Z	info	Traces	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "resource spans": 1, "spans": 2}
otel-collector  | 2025-12-22T01:23:37.778Z	info	Logs	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 2}
otel-collector  | 2025-12-22T01:23:40.533Z	info	Metrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 29, "data points": 34}
otel-collector  | 2025-12-22T01:23:40.533Z	info	Metrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 4, "data points": 7}
otel-collector  | 2025-12-22T01:23:40.549Z	info	Metrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 12, "metrics": 63, "data points": 110}
otel-collector  | 2025-12-22T01:23:42.431Z	info	Traces	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "resource spans": 1, "spans": 1}
otel-collector  | 2025-12-22T01:23:43.346Z	info	Logs	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 1}
otel-collector  | 2025-12-22T01:23:43.486Z	info	Logs	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 1}
otel-collector  | 2025-12-22T01:23:43.593Z	info	Metrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 25, "data points": 40}
otel-collector  | 2025-12-22T01:23:43.956Z	info	Traces	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "resource spans": 1, "spans": 1}
otel-collector  | 2025-12-22T01:23:44.528Z	info	Metrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 51, "data points": 181}
otel-collector  | 2025-12-22T01:23:45.141Z	info	Logs	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 2}
otel-collector  | 2025-12-22T01:23:45.981Z	info	Traces	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "resource spans": 1, "spans": 2}
otel-collector  | 2025-12-22T01:23:48.782Z	info	Logs	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 2}
otel-collector  | 2025-12-22T01:23:50.533Z	info	Metrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 29, "data points": 34}
otel-collector  | 2025-12-22T01:23:50.534Z	info	Metrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 4, "data points": 7}
otel-collector  | 2025-12-22T01:23:50.549Z	info	Metrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 12, "metrics": 63, "data points": 110}
otel-collector  | 2025-12-22T01:23:52.438Z	info	Traces	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "resource spans": 1, "spans": 1}
otel-collector  | 2025-12-22T01:23:53.350Z	info	Logs	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 1}
otel-collector  | 2025-12-22T01:23:53.591Z	info	Metrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 25, "data points": 40}
otel-collector  | 2025-12-22T01:23:53.811Z	info	Metrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 24, "data points": 39}
otel-collector  | 2025-12-22T01:23:53.959Z	info	Traces	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "resource spans": 1, "spans": 1}
otel-collector  | 2025-12-22T01:23:54.188Z	info	Metrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 5, "data points": 34}
otel-collector  | 2025-12-22T01:23:54.236Z	info	Metrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 8, "data points": 9}
otel-collector  | 2025-12-22T01:23:54.272Z	info	Metrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 15, "data points": 60}
otel-collector  | 2025-12-22T01:23:54.273Z	info	Metrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 16, "data points": 61}
otel-collector  | 2025-12-22T01:23:54.490Z	info	Metrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 6, "data points": 16}
otel-collector  | 2025-12-22T01:23:54.491Z	error	internal/queue_sender.go:49	Exporting failed. Dropping data.	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "otlphttp/prometheus", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "error": "not retryable error: Permanent error: rpc error: code = Unknown desc = error exporting items, request to http://prometheus:9090/api/v1/otlp/v1/metrics responded with HTTP Status Code 500", "dropped_items": 16}
otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal.NewQueueSender.func1
otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queue_sender.go:49
otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/queuebatch.(*disabledBatcher[...]).Consume
otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queuebatch/disabled_batcher.go:23
otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/queue.(*asyncQueue[...]).Start.func1
otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queue/async_queue.go:49
otel-collector  | 2025-12-22T01:23:55.154Z	info	Logs	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 2}
otel-collector  | 2025-12-22T01:23:55.981Z	info	Traces	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "resource spans": 1, "spans": 2}
otel-collector  | 2025-12-22T01:23:56.499Z	info	Logs	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 1}
otel-collector  | 2025-12-22T01:23:57.785Z	info	Logs	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 2}
otel-collector  | 2025-12-22T01:24:00.533Z	info	Metrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 4, "data points": 7}
otel-collector  | 2025-12-22T01:24:00.534Z	info	Metrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 29, "data points": 34}
otel-collector  | 2025-12-22T01:24:00.541Z	info	Metrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 2, "data points": 6}
otel-collector  | 2025-12-22T01:24:00.551Z	info	Metrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 12, "metrics": 63, "data points": 110}
otel-collector  | 2025-12-22T01:24:00.551Z	error	scraperhelper@v0.139.0/obs_metrics.go:61	Error scraping metrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "hostmetrics", "otelcol.component.kind": "receiver", "otelcol.signal": "metrics", "error": "failed to read usage at /hostfs/etc/otelcol-config-extras.yml: no such file or directory; failed to read usage at /hostfs/etc/otelcol-config.yml: no such file or directory"}
otel-collector  | go.opentelemetry.io/collector/scraper/scraperhelper.wrapObsMetrics.func1
otel-collector  | 	go.opentelemetry.io/collector/scraper/scraperhelper@v0.139.0/obs_metrics.go:61
otel-collector  | go.opentelemetry.io/collector/scraper.ScrapeMetricsFunc.ScrapeMetrics
otel-collector  | 	go.opentelemetry.io/collector/scraper@v0.139.0/metrics.go:24
otel-collector  | go.opentelemetry.io/collector/scraper/scraperhelper.scrapeMetrics
otel-collector  | 	go.opentelemetry.io/collector/scraper/scraperhelper@v0.139.0/controller.go:256
otel-collector  | go.opentelemetry.io/collector/scraper/scraperhelper.NewMetricsController.func1
otel-collector  | 	go.opentelemetry.io/collector/scraper/scraperhelper@v0.139.0/controller.go:228
otel-collector  | go.opentelemetry.io/collector/scraper/scraperhelper.(*controller[...]).startScraping.func1
otel-collector  | 	go.opentelemetry.io/collector/scraper/scraperhelper@v0.139.0/controller.go:175
otel-collector  | 2025-12-22T01:24:00.552Z	info	Metrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 8, "metrics": 27, "data points": 813}
otel-collector  | 2025-12-22T01:24:00.852Z	info	Metrics	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 36, "data points": 77}
otel-collector  | 2025-12-22T01:24:00.961Z	info	Traces	{"resource": {"service.instance.id": "b1ac9be9-8a1d-476f-adc0-e9edb71eecb6", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "resource spans": 1, "spans": 2}
jaeger          | 2025-12-22T01:23:28.223Z	warn	grpc@v1.75.0/clientconn.go:1407	[core] [Channel #1 SubChannel #7]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:4317", ServerName: "localhost:4317", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:4317: connect: connection refused"	{"resource": {"service.instance.id": "ff61fea4-57c8-41c0-8e36-604e6373ce66", "service.name": "jaeger-query", "service.version": "v2.11.0"}, "grpc_log": true}
jaeger          | 2025-12-22T01:23:49.319Z	warn	grpc@v1.75.0/clientconn.go:1407	[core] [Channel #1 SubChannel #8]grpc: addrConn.createTransport failed to connect to {Addr: "[::1]:4317", ServerName: "localhost:4317", }. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:4317: connect: connection refused"	{"resource": {"service.instance.id": "ff61fea4-57c8-41c0-8e36-604e6373ce66", "service.name": "jaeger-query", "service.version": "v2.11.0"}, "grpc_log": true}
