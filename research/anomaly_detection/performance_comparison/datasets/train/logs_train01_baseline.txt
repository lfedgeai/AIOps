accounting  | accounting  | Accounting service started
accounting  | accounting  | [CORECLR_ENABLE_PROFILING, 1]
accounting  | accounting  | [CORECLR_PROFILER, {918728DD-259F-4A6A-AC2B-B85E1B658318}]
accounting  | accounting  | [CORECLR_PROFILER_PATH, /app/OpenTelemetry.AutoInstrumentation.Native.so]
accounting  | accounting  | [DOTNET_RUNNING_IN_CONTAINER, true]
accounting  | accounting  | [DOTNET_STARTUP_HOOKS, /app/OpenTelemetry.AutoInstrumentation.StartupHook.dll]
accounting  | accounting  | [DOTNET_VERSION, 8.0.22]
accounting  | accounting  | [KAFKA_ADDR, kafka:9092]
accounting  | accounting  | [OTEL_DOTNET_AUTO_HOME, /app]
accounting  | accounting  | [OTEL_DOTNET_AUTO_RULE_ENGINE_ENABLED, false]
accounting  | accounting  | [OTEL_DOTNET_AUTO_TRACES_ADDITIONAL_SOURCES, Accounting.Consumer]
accounting  | accounting  | [OTEL_DOTNET_AUTO_TRACES_ENTITYFRAMEWORKCORE_INSTRUMENTATION_ENABLED, false]
accounting  | accounting  | [OTEL_EXPORTER_OTLP_ENDPOINT, http://otel-collector:4318]
accounting  | accounting  | [OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE, cumulative]
accounting  | accounting  | [OTEL_RESOURCE_ATTRIBUTES, service.namespace=opentelemetry-demo,service.version=2.1.3]
accounting  | accounting  | [OTEL_SERVICE_NAME, accounting]
accounting  | accounting  | %3|1770440520.468|FAIL|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: Connect to ipv4#172.22.0.23:9092 failed: Connection refused (after 1ms in state CONNECT)
accounting  | accounting  | %3|1770440520.468|ERROR|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: 1/1 brokers are down
accounting  | accounting  | info: Accounting.Consumer[0]
accounting  | accounting  |       Connecting to Kafka: kafka:9092
accounting  | accounting  | %3|1770440520.518|FAIL|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: Connect to ipv4#172.22.0.23:9092 failed: Connection refused (after 0ms in state CONNECT, 1 identical error(s) suppressed)
accounting  | accounting  | %3|1770440520.599|ERROR|rdkafka#consumer-1| [thrd:app]: rdkafka#consumer-1: kafka:9092/bootstrap: Connect to ipv4#172.22.0.23:9092 failed: Connection refused (after 1ms in state CONNECT)
accounting  | accounting  | %3|1770440520.599|ERROR|rdkafka#consumer-1| [thrd:app]: rdkafka#consumer-1: kafka:9092/bootstrap: Connect to ipv4#172.22.0.23:9092 failed: Connection refused (after 0ms in state CONNECT, 1 identical error(s) suppressed)
ad  | ad  | 2026-02-07 05:02:03 - oteldemo.AdService - Ad service started, listening on 9555 trace_id= span_id= trace_flags= 
cart  | cart  | info: cart.cartstore.ValkeyCartStore[0]
cart  | cart  |       GetCartAsync called with userId=
checkout  | checkout  | panic: runtime error: invalid memory address or nil pointer dereference [recovered]
checkout  | checkout  | 	panic: runtime error: invalid memory address or nil pointer dereference
checkout  | checkout  | [signal SIGSEGV: segmentation violation code=0x1 addr=0x58 pc=0xcc52c0]
checkout  | checkout  | 
checkout  | checkout  | goroutine 499 [running]:
checkout  | checkout  | go.opentelemetry.io/otel/sdk/trace.(*recordingSpan).End.deferwrap1()
checkout  | checkout  | 	/go/pkg/mod/go.opentelemetry.io/otel/sdk@v1.38.0/trace/span.go:468 +0x25
checkout  | checkout  | go.opentelemetry.io/otel/sdk/trace.(*recordingSpan).End(0xc000301860, {0x0, 0x0, 0xc000603340?})
checkout  | checkout  | 	/go/pkg/mod/go.opentelemetry.io/otel/sdk@v1.38.0/trace/span.go:517 +0xc12
checkout  | checkout  | panic({0xe2cc80?, 0x1827810?})
checkout  | checkout  | 	/usr/local/go/src/runtime/panic.go:792 +0x132
checkout  | checkout  | main.(*checkout).sendToPostProcessor(0xc0000001c0, {0x1114ed8, 0xc0008904e0}, 0x0?)
checkout  | checkout  | 	/usr/src/app/main.go:630 +0x1e0
checkout  | checkout  | main.(*checkout).PlaceOrder(0xc0000001c0, {0x1114ed8, 0xc0008904e0}, 0xc0001fa230)
checkout  | checkout  | 	/usr/src/app/main.go:388 +0x1d99
checkout  | checkout  | github.com/open-telemetry/opentelemetry-demo/src/checkout/genproto/oteldemo._CheckoutService_PlaceOrder_Handler({0xf688e0, 0xc0000001c0}, {0x1114ed8, 0xc0008904e0}, 0xc0001f8600, 0x0)
checkout  | checkout  | 	/usr/src/app/genproto/oteldemo/demo_grpc.pb.go:1229 +0x1a6
checkout  | checkout  | google.golang.org/grpc.(*Server).processUnaryRPC(0xc000210008, {0x1114ed8, 0xc0008903c0}, 0xc0000fe680, 0xc000387410, 0x182b5e0, 0x0)
checkout  | checkout  | 	/go/pkg/mod/google.golang.org/grpc@v1.77.0/server.go:1428 +0x11e7
checkout  | checkout  | google.golang.org/grpc.(*Server).handleStream(0xc000210008, {0x11159f8, 0xc0001f4820}, 0xc0000fe680)
checkout  | checkout  | 	/go/pkg/mod/google.golang.org/grpc@v1.77.0/server.go:1832 +0xdc6
checkout  | checkout  | google.golang.org/grpc.(*Server).serveStreams.func2.1()
checkout  | checkout  | 	/go/pkg/mod/google.golang.org/grpc@v1.77.0/server.go:1063 +0x7f
checkout  | checkout  | created by google.golang.org/grpc.(*Server).serveStreams.func2 in goroutine 532
checkout  | checkout  | 	/go/pkg/mod/google.golang.org/grpc@v1.77.0/server.go:1074 +0x11d
checkout  | checkout  | panic: runtime error: invalid memory address or nil pointer dereference [recovered]
checkout  | checkout  | 	panic: runtime error: invalid memory address or nil pointer dereference
checkout  | checkout  | [signal SIGSEGV: segmentation violation code=0x1 addr=0x58 pc=0xcc52c0]
checkout  | checkout  | 
checkout  | checkout  | goroutine 290 [running]:
checkout  | checkout  | go.opentelemetry.io/otel/sdk/trace.(*recordingSpan).End.deferwrap1()
checkout  | checkout  | 	/go/pkg/mod/go.opentelemetry.io/otel/sdk@v1.38.0/trace/span.go:468 +0x25
checkout  | checkout  | go.opentelemetry.io/otel/sdk/trace.(*recordingSpan).End(0xc00042c780, {0x0, 0x0, 0xc000560fb8?})
checkout  | checkout  | 	/go/pkg/mod/go.opentelemetry.io/otel/sdk@v1.38.0/trace/span.go:517 +0xc12
checkout  | checkout  | panic({0xe2cc80?, 0x1827810?})
checkout  | checkout  | 	/usr/local/go/src/runtime/panic.go:792 +0x132
checkout  | checkout  | main.(*checkout).sendToPostProcessor(0xc0000001c0, {0x1114ed8, 0xc000608600}, 0x0?)
checkout  | checkout  | 	/usr/src/app/main.go:630 +0x1e0
checkout  | checkout  | main.(*checkout).PlaceOrder(0xc0000001c0, {0x1114ed8, 0xc000608600}, 0xc00017c000)
checkout  | checkout  | 	/usr/src/app/main.go:388 +0x1d99
checkout  | checkout  | github.com/open-telemetry/opentelemetry-demo/src/checkout/genproto/oteldemo._CheckoutService_PlaceOrder_Handler({0xf688e0, 0xc0000001c0}, {0x1114ed8, 0xc000608600}, 0xc000238380, 0x0)
checkout  | checkout  | 	/usr/src/app/genproto/oteldemo/demo_grpc.pb.go:1229 +0x1a6
checkout  | checkout  | google.golang.org/grpc.(*Server).processUnaryRPC(0xc0000fe908, {0x1114ed8, 0xc0004061b0}, 0xc000300340, 0xc000710b40, 0x182b5e0, 0x0)
checkout  | checkout  | 	/go/pkg/mod/google.golang.org/grpc@v1.77.0/server.go:1428 +0x11e7
checkout  | checkout  | google.golang.org/grpc.(*Server).handleStream(0xc0000fe908, {0x11159f8, 0xc0003004e0}, 0xc000300340)
checkout  | checkout  | 	/go/pkg/mod/google.golang.org/grpc@v1.77.0/server.go:1832 +0xdc6
checkout  | checkout  | google.golang.org/grpc.(*Server).serveStreams.func2.1()
checkout  | checkout  | 	/go/pkg/mod/google.golang.org/grpc@v1.77.0/server.go:1063 +0x7f
checkout  | checkout  | created by google.golang.org/grpc.(*Server).serveStreams.func2 in goroutine 169
checkout  | checkout  | 	/go/pkg/mod/google.golang.org/grpc@v1.77.0/server.go:1074 +0x11d
checkout  | checkout  | panic: runtime error: invalid memory address or nil pointer dereference [recovered]
checkout  | checkout  | 	panic: runtime error: invalid memory address or nil pointer dereference
checkout  | checkout  | [signal SIGSEGV: segmentation violation code=0x1 addr=0x58 pc=0xcc52c0]
checkout  | checkout  | 
checkout  | checkout  | goroutine 179 [running]:
checkout  | checkout  | go.opentelemetry.io/otel/sdk/trace.(*recordingSpan).End.deferwrap1()
checkout  | checkout  | 	/go/pkg/mod/go.opentelemetry.io/otel/sdk@v1.38.0/trace/span.go:468 +0x25
checkout  | checkout  | go.opentelemetry.io/otel/sdk/trace.(*recordingSpan).End(0xc0002932c0, {0x0, 0x0, 0xc0007dafb8?})
checkout  | checkout  | 	/go/pkg/mod/go.opentelemetry.io/otel/sdk@v1.38.0/trace/span.go:517 +0xc12
checkout  | checkout  | panic({0xe2cc80?, 0x1827810?})
checkout  | checkout  | 	/usr/local/go/src/runtime/panic.go:792 +0x132
checkout  | checkout  | main.(*checkout).sendToPostProcessor(0xc00039a000, {0x1114ed8, 0xc0003904b0}, 0xc000270690?)
checkout  | checkout  | 	/usr/src/app/main.go:630 +0x1e0
checkout  | checkout  | main.(*checkout).PlaceOrder(0xc00039a000, {0x1114ed8, 0xc0003904b0}, 0xc0002a7500)
checkout  | checkout  | 	/usr/src/app/main.go:388 +0x1d99
checkout  | checkout  | github.com/open-telemetry/opentelemetry-demo/src/checkout/genproto/oteldemo._CheckoutService_PlaceOrder_Handler({0xf688e0, 0xc00039a000}, {0x1114ed8, 0xc0003904b0}, 0xc0002a4300, 0x0)
checkout  | checkout  | 	/usr/src/app/genproto/oteldemo/demo_grpc.pb.go:1229 +0x1a6
checkout  | checkout  | google.golang.org/grpc.(*Server).processUnaryRPC(0xc000158908, {0x1114ed8, 0xc0005143f0}, 0xc0004644e0, 0xc0003913b0, 0x182b5e0, 0x0)
checkout  | checkout  | 	/go/pkg/mod/google.golang.org/grpc@v1.77.0/server.go:1428 +0x11e7
checkout  | checkout  | google.golang.org/grpc.(*Server).handleStream(0xc000158908, {0x11159f8, 0xc00014ed00}, 0xc0004644e0)
checkout  | checkout  | 	/go/pkg/mod/google.golang.org/grpc@v1.77.0/server.go:1832 +0xdc6
checkout  | checkout  | google.golang.org/grpc.(*Server).serveStreams.func2.1()
checkout  | checkout  | 	/go/pkg/mod/google.golang.org/grpc@v1.77.0/server.go:1063 +0x7f
checkout  | checkout  | created by google.golang.org/grpc.(*Server).serveStreams.func2 in goroutine 48
checkout  | checkout  | 	/go/pkg/mod/google.golang.org/grpc@v1.77.0/server.go:1074 +0x11d
currency  | currency  | [Error] File: /opentelemetry-cpp/exporters/otlp/src/otlp_grpc_log_record_exporter.cc:184 [OTLP LOG GRPC Exporter] Export() failed: failed to connect to all addresses; last error: UNKNOWN: ipv4:172.22.0.18:4317: Failed to connect to remote host: Connection refused
email  | email  | I, [2026-02-07T05:01:59.335309 #1]  INFO -- : Instrumentation: OpenTelemetry::Instrumentation::Sinatra was successfully installed with the following options {install_rack: true}
email  | email  | Puma starting in single mode...
email  | email  | * Puma version: 7.1.0 ("Neon Witch")
email  | email  | * Ruby version: ruby 3.4.7 (2025-10-08 revision 7a5688e2a2) +PRISM [x86_64-linux-musl]
email  | email  | *  Min threads: 0
email  | email  | *  Max threads: 5
email  | email  | *  Environment: production
email  | email  | *          PID: 1
email  | email  | * Listening on http://0.0.0.0:6060
email  | email  | == Sinatra (v4.2.1) has taken the stage on 6060 for production with backup from Puma
email  | email  | Use Ctrl-C to stop
flagd  | flagd  | 2026-02-07T05:02:03.064Z	error	file/filepath_sync.go:142	error from os.Open(./etc/flagd/demo.flagd.json): open ./etc/flagd/demo.flagd.json: permission denied	{"component": "sync", "sync": "fileinfo"}
flagd  | flagd  | github.com/open-feature/flagd/core/pkg/sync/file.(*Sync).Sync
flagd  | flagd  | 	/src/core/pkg/sync/file/filepath_sync.go:142
flagd  | flagd  | github.com/open-feature/flagd/flagd/pkg/runtime.(*Runtime).Start.func2
flagd  | flagd  | 	/src/flagd/pkg/runtime/runtime.go:83
flagd  | flagd  | golang.org/x/sync/errgroup.(*Group).add.func1
flagd  | flagd  | 	/go/pkg/mod/golang.org/x/sync@v0.15.0/errgroup/errgroup.go:128
flagd-ui  | flagd-ui  | 05:02:03.614 [info] Read new state from file
flagd-ui  | flagd-ui  | 05:02:03.619 [info] Running FlagdUiWeb.Endpoint with Bandit 1.8.0 at :::4000 (http)
flagd-ui  | flagd-ui  | 05:02:03.620 [info] Access FlagdUiWeb.Endpoint at https://localhost
fraud-detection  | fraud-detection  | SLF4J(W): No SLF4J providers were found.
fraud-detection  | fraud-detection  | SLF4J(W): Defaulting to no-operation (NOP) logger implementation
fraud-detection  | fraud-detection  | SLF4J(W): See https://www.slf4j.org/codes.html#noProviders for further details.
frontend  | frontend  |    ▲ Next.js 16.0.3
frontend  | frontend  |    - Local:         http://3ebbccd05a53:8080
frontend  | frontend  |    - Network:       http://3ebbccd05a53:8080
frontend  | frontend  | 
frontend  | frontend  |  ✓ Starting...
frontend  | frontend  |  ✓ Ready in 145ms
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:436] initializing epoch 0 (base id=0, hot restart version=11.104)
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:438] statically linked extensions:
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.quic.server_preferred_address: quic.server_preferred_address.datasource, quic.server_preferred_address.fixed
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   network.connection.client: default, envoy_internal
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.quic.connection_id_generator: envoy.quic.connection_id_generator.quic_lb, envoy.quic.deterministic_connection_id_generator
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.stats_sinks: envoy.dog_statsd, envoy.graphite_statsd, envoy.metrics_service, envoy.open_telemetry_stat_sink, envoy.stat_sinks.dog_statsd, envoy.stat_sinks.graphite_statsd, envoy.stat_sinks.hystrix, envoy.stat_sinks.metrics_service, envoy.stat_sinks.open_telemetry, envoy.stat_sinks.statsd, envoy.stat_sinks.wasm, envoy.statsd
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.udp_packet_writer: envoy.udp_packet_writer.default, envoy.udp_packet_writer.gso
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.router.cluster_specifier_plugin: envoy.router.cluster_specifier_plugin.lua
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.rds_factory: envoy.rds_factory.default
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.filters.udp.session: envoy.filters.udp.session.dynamic_forward_proxy, envoy.filters.udp.session.http_capsule
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.rbac.matchers: envoy.rbac.matchers.upstream_ip_port
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.thrift_proxy.filters: envoy.filters.thrift.header_to_metadata, envoy.filters.thrift.payload_to_metadata, envoy.filters.thrift.rate_limit, envoy.filters.thrift.router
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.http.cache: envoy.extensions.http.cache.file_system_http_cache, envoy.extensions.http.cache.simple
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.dubbo_proxy.protocols: dubbo
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.rbac.principals: envoy.rbac.principals.mtls_authenticated
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.config_subscription: envoy.config_subscription.ads, envoy.config_subscription.ads_collection, envoy.config_subscription.aggregated_grpc_collection, envoy.config_subscription.delta_grpc, envoy.config_subscription.delta_grpc_collection, envoy.config_subscription.filesystem, envoy.config_subscription.filesystem_collection, envoy.config_subscription.grpc, envoy.config_subscription.rest
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.upstream_options: envoy.extensions.upstreams.http.v3.HttpProtocolOptions, envoy.extensions.upstreams.tcp.v3.TcpProtocolOptions, envoy.upstreams.http.http_protocol_options, envoy.upstreams.tcp.tcp_protocol_options
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.matching.network.input: envoy.matching.inputs.application_protocol, envoy.matching.inputs.destination_ip, envoy.matching.inputs.destination_port, envoy.matching.inputs.direct_source_ip, envoy.matching.inputs.dns_san, envoy.matching.inputs.filter_state, envoy.matching.inputs.server_name, envoy.matching.inputs.source_ip, envoy.matching.inputs.source_port, envoy.matching.inputs.source_type, envoy.matching.inputs.subject, envoy.matching.inputs.transport_protocol, envoy.matching.inputs.uri_san
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.tracers: envoy.tracers.datadog, envoy.tracers.fluentd, envoy.tracers.opentelemetry, envoy.tracers.skywalking, envoy.tracers.xray, envoy.tracers.zipkin, envoy.zipkin
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.matching.generic_proxy_request_input.input: envoy.matching.generic_proxy.input.host, envoy.matching.generic_proxy.input.method, envoy.matching.generic_proxy.input.path, envoy.matching.generic_proxy.input.property, envoy.matching.generic_proxy.input.request, envoy.matching.generic_proxy.input.service
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.http.stateful_session: envoy.http.stateful_session.cookie, envoy.http.stateful_session.header
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   quic.http_server_connection: quic.http_server_connection.default
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.http.ext_proc.response_processors: envoy.extensions.http.ext_proc.save_processing_response
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.thrift_proxy.transports: auto, framed, header, unframed
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.health_check.event_sinks: envoy.health_check.event_sink.file
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.matching.action: envoy.matching.actions.format_string, filter-chain-name
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.clusters: envoy.cluster.dns, envoy.cluster.eds, envoy.cluster.logical_dns, envoy.cluster.original_dst, envoy.cluster.static, envoy.cluster.strict_dns, envoy.clusters.aggregate, envoy.clusters.dynamic_forward_proxy, envoy.clusters.redis
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.thrift_proxy.protocols: auto, binary, binary/non-strict, compact, twitter
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.wasm.runtime: envoy.wasm.runtime.null, envoy.wasm.runtime.v8
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.upstream.local_address_selector: envoy.upstream.local_address_selector.default_local_address_selector
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.config.validators: envoy.config.validators.minimum_clusters, envoy.config.validators.minimum_clusters_validator
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.load_balancing_policies: envoy.load_balancing_policies.client_side_weighted_round_robin, envoy.load_balancing_policies.cluster_provided, envoy.load_balancing_policies.least_request, envoy.load_balancing_policies.maglev, envoy.load_balancing_policies.random, envoy.load_balancing_policies.ring_hash, envoy.load_balancing_policies.round_robin, envoy.load_balancing_policies.subset
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.matching.http.input: envoy.matching.inputs.cel_data_input, envoy.matching.inputs.destination_ip, envoy.matching.inputs.destination_port, envoy.matching.inputs.direct_source_ip, envoy.matching.inputs.dns_san, envoy.matching.inputs.dynamic_metadata, envoy.matching.inputs.filter_state, envoy.matching.inputs.request_headers, envoy.matching.inputs.request_trailers, envoy.matching.inputs.response_headers, envoy.matching.inputs.response_trailers, envoy.matching.inputs.server_name, envoy.matching.inputs.source_ip, envoy.matching.inputs.source_port, envoy.matching.inputs.source_type, envoy.matching.inputs.status_code_class_input, envoy.matching.inputs.status_code_input, envoy.matching.inputs.subject, envoy.matching.inputs.uri_san, query_params
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.http.stateful_header_formatters: envoy.http.stateful_header_formatters.preserve_case, preserve_case
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.matching.http.custom_matchers: envoy.matching.custom_matchers.trie_matcher
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.health_checkers: envoy.health_checkers.grpc, envoy.health_checkers.http, envoy.health_checkers.redis, envoy.health_checkers.tcp, envoy.health_checkers.thrift
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.matching.input_matchers: envoy.matching.input_matchers.generic_request_matcher, envoy.matching.matchers.cel_matcher, envoy.matching.matchers.consistent_hashing, envoy.matching.matchers.ip, envoy.matching.matchers.metadata_matcher, envoy.matching.matchers.runtime_fraction
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.dubbo_proxy.filters: envoy.filters.dubbo.router
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.tracers.opentelemetry.resource_detectors: envoy.tracers.opentelemetry.resource_detectors.dynatrace, envoy.tracers.opentelemetry.resource_detectors.environment, envoy.tracers.opentelemetry.resource_detectors.static_config
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.filters.listener: envoy.filters.listener.http_inspector, envoy.filters.listener.local_ratelimit, envoy.filters.listener.original_dst, envoy.filters.listener.original_src, envoy.filters.listener.proxy_protocol, envoy.filters.listener.tls_inspector, envoy.listener.http_inspector, envoy.listener.original_dst, envoy.listener.original_src, envoy.listener.proxy_protocol, envoy.listener.tls_inspector
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.tracers.opentelemetry.samplers: envoy.tracers.opentelemetry.samplers.always_on, envoy.tracers.opentelemetry.samplers.cel, envoy.tracers.opentelemetry.samplers.dynatrace
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.transport_sockets.downstream: envoy.transport_sockets.alts, envoy.transport_sockets.quic, envoy.transport_sockets.raw_buffer, envoy.transport_sockets.starttls, envoy.transport_sockets.tap, envoy.transport_sockets.tcp_stats, envoy.transport_sockets.tls, raw_buffer, starttls, tls
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.retry_host_predicates: envoy.retry_host_predicates.omit_canary_hosts, envoy.retry_host_predicates.omit_host_metadata, envoy.retry_host_predicates.previous_hosts
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.resource_monitors: envoy.resource_monitors.cpu_utilization, envoy.resource_monitors.fixed_heap, envoy.resource_monitors.injected_resource
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.http.early_header_mutation: envoy.http.early_header_mutation.header_mutation
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.dubbo_proxy.serializers: dubbo.hessian2
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.tls.cert_validator: envoy.tls.cert_validator.default, envoy.tls.cert_validator.spiffe
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.http.original_ip_detection: envoy.http.original_ip_detection.custom_header, envoy.http.original_ip_detection.xff
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.compression.decompressor: envoy.compression.brotli.decompressor, envoy.compression.gzip.decompressor, envoy.compression.zstd.decompressor
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.geoip_providers: envoy.geoip_providers.maxmind
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.upstreams: envoy.filters.connection_pools.tcp.generic
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.formatter: envoy.formatter.cel, envoy.formatter.metadata, envoy.formatter.req_without_query
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.generic_proxy.codecs: envoy.generic_proxy.codecs.dubbo, envoy.generic_proxy.codecs.http1
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.request_id: envoy.request_id.uuid
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.connection_handler: envoy.connection_handler.default
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.filters.http.upstream: envoy.buffer, envoy.ext_proc, envoy.extensions.filters.http.dynamic_modules, envoy.filters.http.admission_control, envoy.filters.http.aws_lambda, envoy.filters.http.aws_request_signing, envoy.filters.http.buffer, envoy.filters.http.composite, envoy.filters.http.credential_injector, envoy.filters.http.ext_proc, envoy.filters.http.header_mutation, envoy.filters.http.lua, envoy.filters.http.match_delegate, envoy.filters.http.upstream_codec, envoy.filters.http.wasm
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.resolvers: envoy.ip
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.quic.server.crypto_stream: envoy.quic.crypto_stream.server.quiche
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.quic.proof_source: envoy.quic.proof_source.filter_chain
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.retry_priorities: envoy.retry_priorities.previous_priorities
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.route.early_data_policy: envoy.route.early_data_policy.default
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.path.match: envoy.path.match.uri_template.uri_template_matcher
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.ssl.server_context_factory: envoy.ssl.server_context_factory.default
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.listener_manager_impl: envoy.listener_manager_impl.default, envoy.listener_manager_impl.validation
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.matching.network.custom_matchers: envoy.matching.custom_matchers.trie_matcher
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.access_loggers: envoy.access_loggers.file, envoy.access_loggers.fluentd, envoy.access_loggers.http_grpc, envoy.access_loggers.open_telemetry, envoy.access_loggers.stderr, envoy.access_loggers.stdout, envoy.access_loggers.tcp_grpc, envoy.access_loggers.wasm, envoy.file_access_log, envoy.fluentd_access_log, envoy.http_grpc_access_log, envoy.open_telemetry_access_log, envoy.stderr_access_log, envoy.stdout_access_log, envoy.tcp_grpc_access_log, envoy.wasm_access_log
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   filter_state.object: envoy.filters.listener.original_dst.local_ip, envoy.filters.listener.original_dst.remote_ip, envoy.network.application_protocols, envoy.network.transport_socket.original_dst_address, envoy.network.upstream_server_name, envoy.network.upstream_subject_alt_names, envoy.ratelimit.hits_addend, envoy.string, envoy.tcp_proxy.cluster, envoy.tcp_proxy.disable_tunneling, envoy.tcp_proxy.per_connection_idle_timeout_ms, envoy.upstream.dynamic_host, envoy.upstream.dynamic_port
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.bootstrap: envoy.bootstrap.internal_listener, envoy.bootstrap.wasm, envoy.extensions.network.socket_interface.default_socket_interface
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.generic_proxy.filters: envoy.filters.generic.router
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.filters.http: envoy.bandwidth_limit, envoy.buffer, envoy.cors, envoy.csrf, envoy.ext_authz, envoy.ext_proc, envoy.extensions.filters.http.dynamic_modules, envoy.fault, envoy.filters.http.adaptive_concurrency, envoy.filters.http.admission_control, envoy.filters.http.alternate_protocols_cache, envoy.filters.http.api_key_auth, envoy.filters.http.aws_lambda, envoy.filters.http.aws_request_signing, envoy.filters.http.bandwidth_limit, envoy.filters.http.basic_auth, envoy.filters.http.buffer, envoy.filters.http.cache, envoy.filters.http.cdn_loop, envoy.filters.http.composite, envoy.filters.http.compressor, envoy.filters.http.connect_grpc_bridge, envoy.filters.http.cors, envoy.filters.http.credential_injector, envoy.filters.http.csrf, envoy.filters.http.custom_response, envoy.filters.http.decompressor, envoy.filters.http.dynamic_forward_proxy, envoy.filters.http.ext_authz, envoy.filters.http.ext_proc, envoy.filters.http.fault, envoy.filters.http.file_system_buffer, envoy.filters.http.gcp_authn, envoy.filters.http.geoip, envoy.filters.http.grpc_field_extraction, envoy.filters.http.grpc_http1_bridge, envoy.filters.http.grpc_http1_reverse_bridge, envoy.filters.http.grpc_json_reverse_transcoder, envoy.filters.http.grpc_json_transcoder, envoy.filters.http.grpc_stats, envoy.filters.http.grpc_web, envoy.filters.http.header_mutation, envoy.filters.http.header_to_metadata, envoy.filters.http.health_check, envoy.filters.http.ip_tagging, envoy.filters.http.json_to_metadata, envoy.filters.http.jwt_authn, envoy.filters.http.local_ratelimit, envoy.filters.http.lua, envoy.filters.http.match_delegate, envoy.filters.http.oauth2, envoy.filters.http.on_demand, envoy.filters.http.original_src, envoy.filters.http.proto_message_extraction, envoy.filters.http.rate_limit_quota, envoy.filters.http.ratelimit, envoy.filters.http.rbac, envoy.filters.http.router, envoy.filters.http.set_filter_state, envoy.filters.http.set_metadata, envoy.filters.http.stateful_session, envoy.filters.http.tap, envoy.filters.http.thrift_to_metadata, envoy.filters.http.wasm, envoy.geoip, envoy.grpc_http1_bridge, envoy.grpc_json_transcoder, envoy.grpc_web, envoy.health_check, envoy.ip_tagging, envoy.local_rate_limit, envoy.lua, envoy.rate_limit, envoy.router
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.filters.network: envoy.echo, envoy.ext_authz, envoy.filters.network.connection_limit, envoy.filters.network.direct_response, envoy.filters.network.dubbo_proxy, envoy.filters.network.echo, envoy.filters.network.ext_authz, envoy.filters.network.ext_proc, envoy.filters.network.generic_proxy, envoy.filters.network.http_connection_manager, envoy.filters.network.local_ratelimit, envoy.filters.network.mongo_proxy, envoy.filters.network.ratelimit, envoy.filters.network.rbac, envoy.filters.network.redis_proxy, envoy.filters.network.set_filter_state, envoy.filters.network.sni_cluster, envoy.filters.network.sni_dynamic_forward_proxy, envoy.filters.network.tcp_proxy, envoy.filters.network.thrift_proxy, envoy.filters.network.wasm, envoy.filters.network.zookeeper_proxy, envoy.http_connection_manager, envoy.mongo_proxy, envoy.ratelimit, envoy.redis_proxy, envoy.tcp_proxy
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.grpc_credentials: envoy.grpc_credentials.aws_iam, envoy.grpc_credentials.default, envoy.grpc_credentials.file_based_metadata
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.internal_redirect_predicates: envoy.internal_redirect_predicates.allow_listed_routes, envoy.internal_redirect_predicates.previous_routes, envoy.internal_redirect_predicates.safe_cross_scheme
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.string_matcher: envoy.string_matcher.lua
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.rate_limit_descriptors: envoy.rate_limit_descriptors.expr
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.common.key_value: envoy.key_value.file_based
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.network.dns_resolver: envoy.network.dns_resolver.cares, envoy.network.dns_resolver.getaddrinfo
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.http.header_validators: envoy.http.header_validators.envoy_default
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.http.injected_credentials: envoy.http.injected_credentials.generic, envoy.http.injected_credentials.oauth2
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.quic.connection_debug_visitor: envoy.quic.connection_debug_visitor.basic, envoy.quic.connection_debug_visitor.quic_stats
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.path.rewrite: envoy.path.rewrite.uri_template.uri_template_rewriter
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.http.custom_response: envoy.extensions.http.custom_response.local_response_policy, envoy.extensions.http.custom_response.redirect_policy
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.built_in_formatters: envoy.built_in_formatters.cel, envoy.built_in_formatters.http.default, envoy.built_in_formatters.metadata, envoy.built_in_formatters.stream_info.default
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.config_mux: envoy.config_mux.delta_grpc_mux_factory, envoy.config_mux.grpc_mux_factory, envoy.config_mux.new_grpc_mux_factory, envoy.config_mux.sotw_grpc_mux_factory
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.compression.compressor: envoy.compression.brotli.compressor, envoy.compression.gzip.compressor, envoy.compression.zstd.compressor
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.access_loggers.extension_filters: envoy.access_loggers.extension_filters.cel
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.srds_factory: envoy.srds_factory.default
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.matching.common_inputs: envoy.matching.common_inputs.environment_variable
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.filters.udp_listener: envoy.filters.udp.dns_filter, envoy.filters.udp_listener.udp_proxy
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.transport_sockets.upstream: envoy.transport_sockets.alts, envoy.transport_sockets.http_11_proxy, envoy.transport_sockets.internal_upstream, envoy.transport_sockets.quic, envoy.transport_sockets.raw_buffer, envoy.transport_sockets.starttls, envoy.transport_sockets.tap, envoy.transport_sockets.tcp_stats, envoy.transport_sockets.tls, envoy.transport_sockets.upstream_proxy_protocol, raw_buffer, starttls, tls
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.guarddog_actions: envoy.watchdog.abort_action, envoy.watchdog.profile_action
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.route_config_update_requester: envoy.route_config_update_requester.default
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.229][8][info][main] [source/server/server.cc:440]   envoy.regex_engines: envoy.regex_engines.google_re2
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.250][8][info][main] [source/server/server.cc:500] HTTP header map info:
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.250][8][info][main] [source/server/server.cc:503]   request header map: 656 bytes: :authority,:method,:path,:protocol,:scheme,accept,accept-encoding,access-control-request-headers,access-control-request-method,access-control-request-private-network,authentication,authorization,cache-control,cdn-loop,connection,content-encoding,content-length,content-type,expect,grpc-accept-encoding,grpc-timeout,if-match,if-modified-since,if-none-match,if-range,if-unmodified-since,keep-alive,origin,pragma,proxy-connection,proxy-status,referer,te,transfer-encoding,upgrade,user-agent,via,x-client-trace-id,x-envoy-attempt-count,x-envoy-decorator-operation,x-envoy-downstream-service-cluster,x-envoy-downstream-service-node,x-envoy-expected-rq-timeout-ms,x-envoy-external-address,x-envoy-force-trace,x-envoy-hedge-on-per-try-timeout,x-envoy-internal,x-envoy-ip-tags,x-envoy-is-timeout-retry,x-envoy-max-retries,x-envoy-original-path,x-envoy-original-url,x-envoy-retriable-header-names,x-envoy-retriable-status-codes,x-envoy-retry-grpc-on,x-envoy-retry-on,x-envoy-upstream-alt-stat-name,x-envoy-upstream-rq-per-try-timeout-ms,x-envoy-upstream-rq-timeout-alt-response,x-envoy-upstream-rq-timeout-ms,x-envoy-upstream-stream-duration-ms,x-forwarded-client-cert,x-forwarded-for,x-forwarded-host,x-forwarded-port,x-forwarded-proto,x-request-id
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.250][8][info][main] [source/server/server.cc:503]   request trailer map: 120 bytes: 
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.250][8][info][main] [source/server/server.cc:503]   response header map: 432 bytes: :status,access-control-allow-credentials,access-control-allow-headers,access-control-allow-methods,access-control-allow-origin,access-control-allow-private-network,access-control-expose-headers,access-control-max-age,age,cache-control,connection,content-encoding,content-length,content-type,date,etag,expires,grpc-message,grpc-status,keep-alive,last-modified,location,proxy-connection,proxy-status,server,transfer-encoding,upgrade,vary,via,x-envoy-attempt-count,x-envoy-decorator-operation,x-envoy-degraded,x-envoy-immediate-health-check-fail,x-envoy-ratelimited,x-envoy-upstream-canary,x-envoy-upstream-healthchecked-cluster,x-envoy-upstream-service-time,x-request-id
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.250][8][info][main] [source/server/server.cc:503]   response trailer map: 144 bytes: grpc-message,grpc-status
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.301][8][info][main] [source/server/server.cc:901] runtime: layers:
frontend-proxy  | frontend-proxy  |   - name: static_layer_0
frontend-proxy  | frontend-proxy  |     static_layer:
frontend-proxy  | frontend-proxy  |       envoy:
frontend-proxy  | frontend-proxy  |         resource_limits:
frontend-proxy  | frontend-proxy  |           listener:
frontend-proxy  | frontend-proxy  |             example_listener_name:
frontend-proxy  | frontend-proxy  |               connection_limit: 10000
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.301][8][info][main] [source/server/server.cc:734] Starting admin HTTP server at 0.0.0.0:10000
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.301][8][info][admin] [source/server/admin/admin.cc:65] admin address: 0.0.0.0:10000
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.301][8][info][config] [source/server/configuration_impl.cc:173] loading tracing configuration
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.301][8][info][config] [source/server/configuration_impl.cc:124] loading 0 static secret(s)
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.301][8][info][config] [source/server/configuration_impl.cc:130] loading 9 cluster(s)
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.307][8][info][config] [source/server/configuration_impl.cc:140] loading 1 listener(s)
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.324][8][info][tracing] [source/common/tracing/tracer_manager_impl.cc:42] instantiating a new tracer: envoy.tracers.opentelemetry
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.327][8][info][config] [source/server/configuration_impl.cc:156] loading stats configuration
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.329][8][warning][main] [source/server/server.cc:970] There is no configured limit to the number of allowed active downstream connections. Configure a limit in `envoy.resource_monitors.global_downstream_max_connections` resource monitor.
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.329][8][info][main] [source/server/server.cc:1012] starting main dispatch loop
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.345][8][info][runtime] [source/common/runtime/runtime_impl.cc:555] RTDS has finished initialization
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.345][8][info][upstream] [source/common/upstream/cluster_manager_impl.cc:264] cm init: all clusters initialized
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.345][8][info][main] [source/server/server.cc:992] all clusters initialized. initializing init manager
frontend-proxy  | frontend-proxy  | [2026-02-07 05:01:59.345][8][info][config] [source/common/listener_manager/listener_manager_impl.cc:948] all dependencies initialized. starting workers
grafana  | grafana  | logger=installer.fs t=2026-02-07T05:02:03.291727704Z level=info msg="Downloaded and extracted grafana-lokiexplore-app v1.0.35 zip successfully to /var/lib/grafana/plugins/grafana-lokiexplore-app"
grafana  | grafana  | logger=plugins.registration t=2026-02-07T05:02:03.318301445Z level=info msg="Plugin registered" pluginId=grafana-lokiexplore-app
grafana  | grafana  | logger=plugin.backgroundinstaller t=2026-02-07T05:02:03.318334444Z level=info msg="Plugin successfully installed" pluginId=grafana-lokiexplore-app version= duration=1.590256168s
grafana  | grafana  | logger=ngalert.state.manager rule_uid=des78nlna99tsf org_id=1 t=2026-02-07T05:02:10.009476859Z level=error msg="Error in expanding template" error="failed to expand template '{{- $labels := .Labels -}}{{- $values := .Values -}}{{- $value := .Value -}}The 95th percentile response time for operation {{ $labels.service_namespace\n    }}/{{ $labels.service_name }} \"{{ $labels.http_request_method }} {{\n    $labels.http_route }}\" has been\n    above xxx seconds for 2 minutes on {{ $labels.service_instance_id}}. Current\n    value: {{ .Value | humanizeDuration }}.': error executing template __alert_CartAddItemHighLatency: template: __alert_CartAddItemHighLatency:5:23: executing \"__alert_CartAddItemHighLatency\" at <humanizeDuration>: error calling humanizeDuration: strconv.ParseFloat: parsing \"\": invalid syntax"
grafana  | grafana  | logger=ngalert.state.manager rule_uid=des78nlna99tsf org_id=1 t=2026-02-07T05:02:10.009751771Z level=info msg="Detected stale state entry" cacheID=ce0a04f7440f6a6b state=Alerting reason=
grafana  | grafana  | logger=ngalert.sender.router rule_uid=des78nlna99tsf org_id=1 t=2026-02-07T05:02:10.012998307Z level=info msg="Sending alerts to local notifier" count=2
image-provider  | image-provider  | load_module modules/ngx_otel_module.so;
image-provider  | image-provider  | 
image-provider  | image-provider  | pid /tmp/nginx.pid;
image-provider  | image-provider  | 
image-provider  | image-provider  | events {
image-provider  | image-provider  |     worker_connections 1024;
image-provider  | image-provider  | }
image-provider  | image-provider  | 
image-provider  | image-provider  | http {
image-provider  | image-provider  |     otel_exporter {
image-provider  | image-provider  |         endpoint otel-collector:4317;
image-provider  | image-provider  |     }
image-provider  | image-provider  |     otel_trace on;
image-provider  | image-provider  |     otel_trace_context propagate;
image-provider  | image-provider  |     otel_service_name image-provider;
image-provider  | image-provider  |     otel_span_name image-provider;
image-provider  | image-provider  | 
image-provider  | image-provider  | 
image-provider  | image-provider  |     include mime.types;
image-provider  | image-provider  |     sendfile on;
image-provider  | image-provider  |     server {
image-provider  | image-provider  |         listen 8081;
image-provider  | image-provider  |         listen [::]:8081;
image-provider  | image-provider  | 
image-provider  | image-provider  |         resolver 127.0.0.11;
image-provider  | image-provider  |         autoindex off;
image-provider  | image-provider  | 
image-provider  | image-provider  |         server_name _;
image-provider  | image-provider  |         server_tokens off;
image-provider  | image-provider  | 
image-provider  | image-provider  |         root /static;
image-provider  | image-provider  |         gzip_static on;
image-provider  | image-provider  | 
image-provider  | image-provider  |         location /status {
image-provider  | image-provider  |             stub_status on;
image-provider  | image-provider  |             access_log  on;           
image-provider  | image-provider  |             allow all;
image-provider  | image-provider  |         }
image-provider  | image-provider  |     }
image-provider  | image-provider  | }
jaeger  | jaeger  | 2026/02/07 05:01:58 application version: git-commit=a204929483c734da6c54eaa75f3f7c322b3e886b, git-version=v2.11.0, build-date=2025-10-03T02:41:46Z
jaeger  | jaeger  | 2026-02-07T05:01:58.326Z	info	service@v0.132.0/service.go:187	Setting up own telemetry...	{"resource": {"service.instance.id": "8e6c20d0-2fc4-4bab-8e8f-6c1e800d9735", "service.name": "jaeger-query", "service.version": "v2.11.0"}}
jaeger  | jaeger  | 2026-02-07T05:01:58.327Z	info	builders/builders.go:26	Development component. May change in the future.	{"resource": {"service.instance.id": "8e6c20d0-2fc4-4bab-8e8f-6c1e800d9735", "service.name": "jaeger-query", "service.version": "v2.11.0"}, "otelcol.component.id": "jaeger_storage_exporter", "otelcol.component.kind": "exporter", "otelcol.signal": "traces"}
jaeger  | jaeger  | 2026-02-07T05:01:58.327Z	info	builders/extension.go:50	Development component. May change in the future.	{"resource": {"service.instance.id": "8e6c20d0-2fc4-4bab-8e8f-6c1e800d9735", "service.name": "jaeger-query", "service.version": "v2.11.0"}, "otelcol.component.id": "healthcheckv2", "otelcol.component.kind": "extension"}
jaeger  | jaeger  | 2026-02-07T05:01:58.328Z	info	service@v0.132.0/service.go:249	Starting jaeger...	{"resource": {"service.instance.id": "8e6c20d0-2fc4-4bab-8e8f-6c1e800d9735", "service.name": "jaeger-query", "service.version": "v2.11.0"}, "Version": "v2.11.0", "NumCPU": 32}
jaeger  | jaeger  | 2026-02-07T05:01:58.328Z	info	extensions/extensions.go:41	Starting extensions...	{"resource": {"service.instance.id": "8e6c20d0-2fc4-4bab-8e8f-6c1e800d9735", "service.name": "jaeger-query", "service.version": "v2.11.0"}}
jaeger  | jaeger  | 2026-02-07T05:01:58.328Z	info	extensions/extensions.go:45	Extension is starting...	{"resource": {"service.instance.id": "8e6c20d0-2fc4-4bab-8e8f-6c1e800d9735", "service.name": "jaeger-query", "service.version": "v2.11.0"}, "otelcol.component.id": "healthcheckv2", "otelcol.component.kind": "extension"}
jaeger  | jaeger  | 2026-02-07T05:01:58.328Z	info	extensions/extensions.go:62	Extension started.	{"resource": {"service.instance.id": "8e6c20d0-2fc4-4bab-8e8f-6c1e800d9735", "service.name": "jaeger-query", "service.version": "v2.11.0"}, "otelcol.component.id": "healthcheckv2", "otelcol.component.kind": "extension"}
jaeger  | jaeger  | 2026-02-07T05:01:58.329Z	info	extensions/extensions.go:45	Extension is starting...	{"resource": {"service.instance.id": "8e6c20d0-2fc4-4bab-8e8f-6c1e800d9735", "service.name": "jaeger-query", "service.version": "v2.11.0"}, "otelcol.component.id": "jaeger_storage", "otelcol.component.kind": "extension"}
jaeger  | jaeger  | 2026-02-07T05:01:58.329Z	info	jaegerstorage/extension.go:159	Initializing storage 'memory_backend'	{"resource": {"service.instance.id": "8e6c20d0-2fc4-4bab-8e8f-6c1e800d9735", "service.name": "jaeger-query", "service.version": "v2.11.0"}, "otelcol.component.id": "jaeger_storage", "otelcol.component.kind": "extension"}
jaeger  | jaeger  | 2026-02-07T05:01:58.329Z	info	jaegerstorage/extension.go:209	Initializing metrics storage 'metrics_backend'	{"resource": {"service.instance.id": "8e6c20d0-2fc4-4bab-8e8f-6c1e800d9735", "service.name": "jaeger-query", "service.version": "v2.11.0"}, "otelcol.component.id": "jaeger_storage", "otelcol.component.kind": "extension"}
jaeger  | jaeger  | 2026-02-07T05:01:58.329Z	info	extensions/extensions.go:62	Extension started.	{"resource": {"service.instance.id": "8e6c20d0-2fc4-4bab-8e8f-6c1e800d9735", "service.name": "jaeger-query", "service.version": "v2.11.0"}, "otelcol.component.id": "jaeger_storage", "otelcol.component.kind": "extension"}
jaeger  | jaeger  | 2026-02-07T05:01:58.329Z	info	extensions/extensions.go:45	Extension is starting...	{"resource": {"service.instance.id": "8e6c20d0-2fc4-4bab-8e8f-6c1e800d9735", "service.name": "jaeger-query", "service.version": "v2.11.0"}, "otelcol.component.id": "jaeger_query", "otelcol.component.kind": "extension"}
jaeger  | jaeger  | 2026-02-07T05:01:58.329Z	info	jaegerquery/server.go:150	Archive storage not configured	{"resource": {"service.instance.id": "8e6c20d0-2fc4-4bab-8e8f-6c1e800d9735", "service.name": "jaeger-query", "service.version": "v2.11.0"}, "otelcol.component.id": "jaeger_query", "otelcol.component.kind": "extension"}
jaeger  | jaeger  | 2026-02-07T05:01:58.329Z	info	metricstore/reader.go:125	Prometheus reader initialized	{"resource": {"service.instance.id": "8e6c20d0-2fc4-4bab-8e8f-6c1e800d9735", "service.name": "jaeger-query", "service.version": "v2.11.0"}, "otelcol.component.id": "jaeger_storage", "otelcol.component.kind": "extension", "addr": "http://prometheus:9090"}
jaeger  | jaeger  | 2026-02-07T05:01:58.329Z	info	app/static_handler.go:92	Using UI configuration	{"resource": {"service.instance.id": "8e6c20d0-2fc4-4bab-8e8f-6c1e800d9735", "service.name": "jaeger-query", "service.version": "v2.11.0"}, "otelcol.component.id": "jaeger_query", "otelcol.component.kind": "extension", "path": ""}
jaeger  | jaeger  | 2026-02-07T05:01:58.329Z	info	app/server.go:258	Query server started	{"resource": {"service.instance.id": "8e6c20d0-2fc4-4bab-8e8f-6c1e800d9735", "service.name": "jaeger-query", "service.version": "v2.11.0"}, "otelcol.component.id": "jaeger_query", "otelcol.component.kind": "extension", "http_addr": "[::]:16686", "grpc_addr": "[::]:16685"}
jaeger  | jaeger  | 2026-02-07T05:01:58.329Z	info	extensions/extensions.go:62	Extension started.	{"resource": {"service.instance.id": "8e6c20d0-2fc4-4bab-8e8f-6c1e800d9735", "service.name": "jaeger-query", "service.version": "v2.11.0"}, "otelcol.component.id": "jaeger_query", "otelcol.component.kind": "extension"}
jaeger  | jaeger  | 2026-02-07T05:01:58.329Z	info	app/server.go:286	Starting HTTP server	{"resource": {"service.instance.id": "8e6c20d0-2fc4-4bab-8e8f-6c1e800d9735", "service.name": "jaeger-query", "service.version": "v2.11.0"}, "otelcol.component.id": "jaeger_query", "otelcol.component.kind": "extension", "port": 16686, "addr": ":16686"}
jaeger  | jaeger  | 2026-02-07T05:01:58.329Z	info	app/server.go:300	Starting GRPC server	{"resource": {"service.instance.id": "8e6c20d0-2fc4-4bab-8e8f-6c1e800d9735", "service.name": "jaeger-query", "service.version": "v2.11.0"}, "otelcol.component.id": "jaeger_query", "otelcol.component.kind": "extension", "port": 16685, "addr": ":16685"}
jaeger  | jaeger  | 2026-02-07T05:01:58.332Z	info	otlpreceiver@v0.132.0/otlp.go:117	Starting GRPC server	{"resource": {"service.instance.id": "8e6c20d0-2fc4-4bab-8e8f-6c1e800d9735", "service.name": "jaeger-query", "service.version": "v2.11.0"}, "otelcol.component.id": "otlp", "otelcol.component.kind": "receiver", "endpoint": "jaeger:4317"}
jaeger  | jaeger  | 2026-02-07T05:01:58.333Z	info	service@v0.132.0/service.go:272	Everything is ready. Begin running and processing data.	{"resource": {"service.instance.id": "8e6c20d0-2fc4-4bab-8e8f-6c1e800d9735", "service.name": "jaeger-query", "service.version": "v2.11.0"}}
kafka  | kafka  | OpenJDK 64-Bit Server VM warning: Sharing is only supported for boot loader classes because bootstrap classpath has been appended [otel.javaagent 2026-02-07 05:01:59:582 +0000] [main] INFO io.opentelemetry.javaagent.tooling.VersionLogger - opentelemetry-javaagent - version: 2.21.0 [otel.javaagent 2026-02-07 05:02:02:302 +0000] [main] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.yaml.RuleParser - kafka-broker: found 20 metric rules Log directory /tmp/kafka-logs is already formatted. Use --ignore-formatted to ignore this directory and format the others.
kafka  | kafka  | [0.004s][warning][cds] Unable to read generic CDS file map header from shared archive
kafka  | kafka  | [otel.javaagent 2026-02-07 05:02:05:330 +0000] [main] INFO io.opentelemetry.javaagent.tooling.VersionLogger - opentelemetry-javaagent - version: 2.21.0
kafka  | kafka  | [otel.javaagent 2026-02-07 05:02:06:803 +0000] [main] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.yaml.RuleParser - kafka-broker: found 20 metric rules
kafka  | kafka  | [2026-02-07 05:02:07,170] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
kafka  | kafka  | [2026-02-07 05:02:07,372] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
kafka  | kafka  | [2026-02-07 05:02:07,543] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
kafka  | kafka  | [2026-02-07 05:02:07,549] INFO [ControllerServer id=1] Starting controller (kafka.server.ControllerServer)
kafka  | kafka  | [2026-02-07 05:02:07,881] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
kafka  | kafka  | [2026-02-07 05:02:07,918] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(CONTROLLER) (kafka.network.SocketServer)
kafka  | kafka  | [2026-02-07 05:02:07,924] INFO authorizerStart completed for endpoint CONTROLLER. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
kafka  | kafka  | [2026-02-07 05:02:07,925] INFO [SharedServer id=1] Starting SharedServer (kafka.server.SharedServer)
kafka  | kafka  | [2026-02-07 05:02:07,979] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Recovering unflushed segment 3836303. 0/8 recovered for __cluster_metadata-0. (kafka.log.LogLoader)
kafka  | kafka  | [2026-02-07 05:02:07,981] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 3836303 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:07,981] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 3836303 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:07,982] INFO Deleted producer state snapshot /tmp/kafka-logs/__cluster_metadata-0/00000000000003836333.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
kafka  | kafka  | [2026-02-07 05:02:07,982] INFO Deleted producer state snapshot /tmp/kafka-logs/__cluster_metadata-0/00000000000003836363.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
kafka  | kafka  | [2026-02-07 05:02:07,982] INFO Deleted producer state snapshot /tmp/kafka-logs/__cluster_metadata-0/00000000000003836393.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
kafka  | kafka  | [2026-02-07 05:02:07,982] INFO Deleted producer state snapshot /tmp/kafka-logs/__cluster_metadata-0/00000000000003836423.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
kafka  | kafka  | [2026-02-07 05:02:07,982] INFO Deleted producer state snapshot /tmp/kafka-logs/__cluster_metadata-0/00000000000003836453.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
kafka  | kafka  | [2026-02-07 05:02:07,982] INFO Deleted producer state snapshot /tmp/kafka-logs/__cluster_metadata-0/00000000000003836483.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
kafka  | kafka  | [2026-02-07 05:02:07,982] INFO Deleted producer state snapshot /tmp/kafka-logs/__cluster_metadata-0/00000000000003836513.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
kafka  | kafka  | [2026-02-07 05:02:07,982] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=3836303, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000003836303.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
kafka  | kafka  | [2026-02-07 05:02:07,988] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 7ms for snapshot load and 0ms for segment recovery from offset 3836303 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:07,996] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 3836333 with 0 producer ids in 1 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
kafka  | kafka  | [2026-02-07 05:02:07,996] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Recovering unflushed segment 3836333. 1/8 recovered for __cluster_metadata-0. (kafka.log.LogLoader)
kafka  | kafka  | [2026-02-07 05:02:07,997] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 3836333 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:07,997] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 3836333 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:07,997] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=3836333, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000003836333.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
kafka  | kafka  | [2026-02-07 05:02:07,997] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 3836333 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,000] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 3836363 with 0 producer ids in 1 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
kafka  | kafka  | [2026-02-07 05:02:08,000] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Recovering unflushed segment 3836363. 2/8 recovered for __cluster_metadata-0. (kafka.log.LogLoader)
kafka  | kafka  | [2026-02-07 05:02:08,000] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 3836363 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,000] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 3836363 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,000] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=3836363, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000003836363.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
kafka  | kafka  | [2026-02-07 05:02:08,001] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 3836363 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,002] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 3836393 with 0 producer ids in 0 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
kafka  | kafka  | [2026-02-07 05:02:08,002] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Recovering unflushed segment 3836393. 3/8 recovered for __cluster_metadata-0. (kafka.log.LogLoader)
kafka  | kafka  | [2026-02-07 05:02:08,003] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 3836393 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,003] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 3836393 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,003] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=3836393, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000003836393.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
kafka  | kafka  | [2026-02-07 05:02:08,003] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 3836393 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,005] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 3836423 with 0 producer ids in 1 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
kafka  | kafka  | [2026-02-07 05:02:08,005] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Recovering unflushed segment 3836423. 4/8 recovered for __cluster_metadata-0. (kafka.log.LogLoader)
kafka  | kafka  | [2026-02-07 05:02:08,006] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 3836423 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,006] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 3836423 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,006] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=3836423, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000003836423.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
kafka  | kafka  | [2026-02-07 05:02:08,006] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 3836423 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,008] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 3836453 with 0 producer ids in 1 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
kafka  | kafka  | [2026-02-07 05:02:08,008] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Recovering unflushed segment 3836453. 5/8 recovered for __cluster_metadata-0. (kafka.log.LogLoader)
kafka  | kafka  | [2026-02-07 05:02:08,008] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 3836453 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,009] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 3836453 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,009] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=3836453, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000003836453.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
kafka  | kafka  | [2026-02-07 05:02:08,009] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 3836453 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,015] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 3836483 with 0 producer ids in 1 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
kafka  | kafka  | [2026-02-07 05:02:08,015] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Recovering unflushed segment 3836483. 6/8 recovered for __cluster_metadata-0. (kafka.log.LogLoader)
kafka  | kafka  | [2026-02-07 05:02:08,015] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 3836483 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,015] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 3836483 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,015] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=3836483, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000003836483.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
kafka  | kafka  | [2026-02-07 05:02:08,016] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 3836483 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,021] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 3836513 with 0 producer ids in 1 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
kafka  | kafka  | [2026-02-07 05:02:08,021] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Recovering unflushed segment 3836513. 7/8 recovered for __cluster_metadata-0. (kafka.log.LogLoader)
kafka  | kafka  | [2026-02-07 05:02:08,021] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 3836513 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,021] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 3836513 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,021] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=3836513, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000003836513.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
kafka  | kafka  | [2026-02-07 05:02:08,022] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 3836513 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,027] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 3836518 with 0 producer ids in 1 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
kafka  | kafka  | [2026-02-07 05:02:08,034] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 3836518 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,034] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 3836518 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,034] INFO Deleted producer state snapshot /tmp/kafka-logs/__cluster_metadata-0/00000000000003836303.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
kafka  | kafka  | [2026-02-07 05:02:08,034] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=3836518, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000003836518.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
kafka  | kafka  | [2026-02-07 05:02:08,034] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 3836518 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,064] INFO Deleted unneeded snapshot file with path SnapshotPath(path=/tmp/kafka-logs/__cluster_metadata-0/00000000000003836190-0000000009.checkpoint.deleted, snapshotId=OffsetAndEpoch(offset=3836190, epoch=9), partial=false) (kafka.raft.KafkaMetadataLog$)
kafka  | kafka  | [2026-02-07 05:02:08,065] INFO Deleted unneeded snapshot file with path SnapshotPath(path=/tmp/kafka-logs/__cluster_metadata-0/00000000000003836229-0000000009.checkpoint.deleted, snapshotId=OffsetAndEpoch(offset=3836229, epoch=9), partial=false) (kafka.raft.KafkaMetadataLog$)
kafka  | kafka  | [2026-02-07 05:02:08,065] INFO Deleted unneeded snapshot file with path SnapshotPath(path=/tmp/kafka-logs/__cluster_metadata-0/00000000000003836268-0000000009.checkpoint.deleted, snapshotId=OffsetAndEpoch(offset=3836268, epoch=9), partial=false) (kafka.raft.KafkaMetadataLog$)
kafka  | kafka  | [2026-02-07 05:02:08,069] INFO Initialized snapshots with IDs SortedSet(OffsetAndEpoch(offset=3836307, epoch=9), OffsetAndEpoch(offset=3836346, epoch=9), OffsetAndEpoch(offset=3836385, epoch=9), OffsetAndEpoch(offset=3836424, epoch=9), OffsetAndEpoch(offset=3836463, epoch=9), OffsetAndEpoch(offset=3836502, epoch=9)) from /tmp/kafka-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)
kafka  | kafka  | [2026-02-07 05:02:08,091] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
kafka  | kafka  | [2026-02-07 05:02:08,111] INFO [RaftManager id=1] Reading KRaft snapshot and log as part of the initialization (org.apache.kafka.raft.KafkaRaftClient)
kafka  | kafka  | [2026-02-07 05:02:08,114] INFO [RaftManager id=1] Loading snapshot (OffsetAndEpoch(offset=3836502, epoch=9)) since log start offset (3836303) is greater than the internal listener's next offset (-1) (org.apache.kafka.raft.internals.KRaftControlRecordStateMachine)
kafka  | kafka  | [2026-02-07 05:02:08,199] INFO [RaftManager id=1] Starting voters are VoterSet(voters={1=VoterNode(voterKey=ReplicaKey(id=1, directoryId=Optional.empty), listeners=Endpoints(endpoints={ListenerName(CONTROLLER)=kafka/172.22.0.23:9093}), supportedKRaftVersion=SupportedVersionRange[min_version:0, max_version:0])}) (org.apache.kafka.raft.KafkaRaftClient)
kafka  | kafka  | [2026-02-07 05:02:08,200] INFO [RaftManager id=1] Starting request manager with static voters: [kafka:9093 (id: 1 rack: null)] (org.apache.kafka.raft.KafkaRaftClient)
kafka  | kafka  | [2026-02-07 05:02:08,322] INFO [RaftManager id=1] Attempting durable transition to ResignedState(localId=1, epoch=9, voters=[1], electionTimeoutMs=1463, unackedVoters=[], preferredSuccessors=[]) from null (org.apache.kafka.raft.QuorumState)
kafka  | kafka  | [2026-02-07 05:02:08,334] INFO [RaftManager id=1] Completed transition to ResignedState(localId=1, epoch=9, voters=[1], electionTimeoutMs=1463, unackedVoters=[], preferredSuccessors=[]) from null (org.apache.kafka.raft.QuorumState)
kafka  | kafka  | [2026-02-07 05:02:08,337] INFO [RaftManager id=1] Attempting durable transition to CandidateState(localId=1, localDirectoryId=RpgKmoe3nn5c5lsnw5tZxA,epoch=10, retries=1, voteStates={1=org.apache.kafka.raft.CandidateState$VoterState@21d48c40}, highWatermark=Optional.empty, electionTimeoutMs=1523) from ResignedState(localId=1, epoch=9, voters=[1], electionTimeoutMs=1463, unackedVoters=[], preferredSuccessors=[]) (org.apache.kafka.raft.QuorumState)
kafka  | kafka  | [2026-02-07 05:02:08,340] INFO [RaftManager id=1] Completed transition to CandidateState(localId=1, localDirectoryId=RpgKmoe3nn5c5lsnw5tZxA,epoch=10, retries=1, voteStates={1=org.apache.kafka.raft.CandidateState$VoterState@21d48c40}, highWatermark=Optional.empty, electionTimeoutMs=1523) from ResignedState(localId=1, epoch=9, voters=[1], electionTimeoutMs=1463, unackedVoters=[], preferredSuccessors=[]) (org.apache.kafka.raft.QuorumState)
kafka  | kafka  | [2026-02-07 05:02:08,343] INFO [RaftManager id=1] Attempting durable transition to Leader(localReplicaKey=ReplicaKey(id=1, directoryId=Optional[RpgKmoe3nn5c5lsnw5tZxA]), epoch=10, epochStartOffset=3836518, highWatermark=Optional.empty, voterStates={1=ReplicaState(replicaKey=ReplicaKey(id=1, directoryId=Optional.empty), endOffset=Optional.empty, lastFetchTimestamp=-1, lastCaughtUpTimestamp=-1, hasAcknowledgedLeader=true)}) from CandidateState(localId=1, localDirectoryId=RpgKmoe3nn5c5lsnw5tZxA,epoch=10, retries=1, voteStates={1=org.apache.kafka.raft.CandidateState$VoterState@21d48c40}, highWatermark=Optional.empty, electionTimeoutMs=1523) (org.apache.kafka.raft.QuorumState)
kafka  | kafka  | [2026-02-07 05:02:08,345] INFO [RaftManager id=1] Completed transition to Leader(localReplicaKey=ReplicaKey(id=1, directoryId=Optional[RpgKmoe3nn5c5lsnw5tZxA]), epoch=10, epochStartOffset=3836518, highWatermark=Optional.empty, voterStates={1=ReplicaState(replicaKey=ReplicaKey(id=1, directoryId=Optional.empty), endOffset=Optional.empty, lastFetchTimestamp=-1, lastCaughtUpTimestamp=-1, hasAcknowledgedLeader=true)}) from CandidateState(localId=1, localDirectoryId=RpgKmoe3nn5c5lsnw5tZxA,epoch=10, retries=1, voteStates={1=org.apache.kafka.raft.CandidateState$VoterState@21d48c40}, highWatermark=Optional.empty, electionTimeoutMs=1523) (org.apache.kafka.raft.QuorumState)
kafka  | kafka  | [2026-02-07 05:02:08,354] INFO [kafka-1-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
kafka  | kafka  | [2026-02-07 05:02:08,354] INFO [kafka-1-raft-io-thread]: Starting (org.apache.kafka.raft.KafkaRaftClientDriver)
kafka  | kafka  | [2026-02-07 05:02:08,366] INFO [LocalLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Rolled new log segment at offset 3836518 in 3 ms. (kafka.log.LocalLog)
kafka  | kafka  | [2026-02-07 05:02:08,371] INFO [MetadataLoader id=1] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
kafka  | kafka  | [2026-02-07 05:02:08,374] INFO [ControllerServer id=1] Waiting for controller quorum voters future (kafka.server.ControllerServer)
kafka  | kafka  | [2026-02-07 05:02:08,374] INFO [ControllerServer id=1] Finished waiting for controller quorum voters future (kafka.server.ControllerServer)
kafka  | kafka  | [2026-02-07 05:02:08,384] INFO [RaftManager id=1] High watermark set to LogOffsetMetadata(offset=3836519, metadata=Optional[(segmentBaseOffset=3836518,relativePositionInSegment=91)]) for the first time for epoch 10 based on indexOfHw 0 and voters [ReplicaState(replicaKey=ReplicaKey(id=1, directoryId=Optional.empty), endOffset=Optional[LogOffsetMetadata(offset=3836519, metadata=Optional[(segmentBaseOffset=3836518,relativePositionInSegment=91)])], lastFetchTimestamp=-1, lastCaughtUpTimestamp=-1, hasAcknowledgedLeader=true)] (org.apache.kafka.raft.LeaderState)
kafka  | kafka  | [2026-02-07 05:02:08,389] INFO [RaftManager id=1] Registered the listener org.apache.kafka.image.loader.MetadataLoader@182698462 (org.apache.kafka.raft.KafkaRaftClient)
kafka  | kafka  | [2026-02-07 05:02:08,390] INFO [MetadataLoader id=1] handleLoadSnapshot(00000000000003836502-0000000009): incrementing HandleLoadSnapshotCount to 1. (org.apache.kafka.image.loader.MetadataLoader)
kafka  | kafka  | [2026-02-07 05:02:08,405] INFO [RaftManager id=1] Registered the listener org.apache.kafka.controller.QuorumController$QuorumMetaLogListener@1265304858 (org.apache.kafka.raft.KafkaRaftClient)
kafka  | kafka  | [2026-02-07 05:02:08,405] INFO [MetadataLoader id=1] handleLoadSnapshot(00000000000003836502-0000000009): generated a metadata delta between offset -1 and this snapshot in 15130 us. (org.apache.kafka.image.loader.MetadataLoader)
kafka  | kafka  | [2026-02-07 05:02:08,406] INFO [MetadataLoader id=1] maybePublishMetadata(SNAPSHOT): The loader is still catching up because we have loaded up to offset 3836501, but the high water mark is 3836519 (org.apache.kafka.image.loader.MetadataLoader)
kafka  | kafka  | [2026-02-07 05:02:08,407] INFO [MetadataLoader id=1] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have loaded up to offset 3836512, but the high water mark is 3836519 (org.apache.kafka.image.loader.MetadataLoader)
kafka  | kafka  | [2026-02-07 05:02:08,412] INFO [controller-1-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
kafka  | kafka  | [2026-02-07 05:02:08,413] INFO [controller-1-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
kafka  | kafka  | [2026-02-07 05:02:08,413] INFO [MetadataLoader id=1] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have loaded up to offset 3836517, but the high water mark is 3836519 (org.apache.kafka.image.loader.MetadataLoader)
kafka  | kafka  | [2026-02-07 05:02:08,413] INFO [MetadataLoader id=1] maybePublishMetadata(LOG_DELTA): The loader finished catching up to the current high water mark of 3836519 (org.apache.kafka.image.loader.MetadataLoader)
kafka  | kafka  | [2026-02-07 05:02:08,414] INFO [controller-1-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
kafka  | kafka  | [2026-02-07 05:02:08,415] INFO [controller-1-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
kafka  | kafka  | [2026-02-07 05:02:08,416] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 3836518 (org.apache.kafka.image.loader.MetadataLoader)
kafka  | kafka  | [2026-02-07 05:02:08,438] INFO [ExpirationReaper-1-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
kafka  | kafka  | [2026-02-07 05:02:08,449] INFO [ControllerServer id=1] Waiting for the controller metadata publishers to be installed (kafka.server.ControllerServer)
kafka  | kafka  | [2026-02-07 05:02:08,450] INFO [ControllerServer id=1] Finished waiting for the controller metadata publishers to be installed (kafka.server.ControllerServer)
kafka  | kafka  | [2026-02-07 05:02:08,450] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Enabling request processing. (kafka.network.SocketServer)
kafka  | kafka  | [2026-02-07 05:02:08,450] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing KRaftMetadataCachePublisher with a snapshot at offset 3836518 (org.apache.kafka.image.loader.MetadataLoader)
kafka  | kafka  | [2026-02-07 05:02:08,450] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing FeaturesPublisher with a snapshot at offset 3836518 (org.apache.kafka.image.loader.MetadataLoader)
kafka  | kafka  | [2026-02-07 05:02:08,450] INFO [ControllerServer id=1] Loaded new metadata Features(metadataVersion=3.9-IV0, finalizedFeatures={metadata.version=21}, finalizedFeaturesEpoch=3836518). (org.apache.kafka.metadata.publisher.FeaturesPublisher)
kafka  | kafka  | [2026-02-07 05:02:08,450] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ControllerRegistrationsPublisher with a snapshot at offset 3836518 (org.apache.kafka.image.loader.MetadataLoader)
kafka  | kafka  | [2026-02-07 05:02:08,450] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ControllerRegistrationManager with a snapshot at offset 3836518 (org.apache.kafka.image.loader.MetadataLoader)
kafka  | kafka  | [2026-02-07 05:02:08,451] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing DynamicConfigPublisher controller id=1 with a snapshot at offset 3836518 (org.apache.kafka.image.loader.MetadataLoader)
kafka  | kafka  | [2026-02-07 05:02:08,451] INFO [ControllerRegistrationManager id=1 incarnation=E8DW-QnOREap1CLCmgreeQ] Found registration for K9xCItdXQgSLx68KFfXVQw instead of our incarnation. (kafka.server.ControllerRegistrationManager)
kafka  | kafka  | [2026-02-07 05:02:08,451] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing DynamicClientQuotaPublisher controller id=1 with a snapshot at offset 3836518 (org.apache.kafka.image.loader.MetadataLoader)
kafka  | kafka  | [2026-02-07 05:02:08,453] INFO Awaiting socket connections on kafka:9093. (kafka.network.DataPlaneAcceptor)
kafka  | kafka  | [2026-02-07 05:02:08,453] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ScramPublisher controller id=1 with a snapshot at offset 3836518 (org.apache.kafka.image.loader.MetadataLoader)
kafka  | kafka  | [2026-02-07 05:02:08,454] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing DelegationTokenPublisher controller id=1 with a snapshot at offset 3836518 (org.apache.kafka.image.loader.MetadataLoader)
kafka  | kafka  | [2026-02-07 05:02:08,455] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ControllerMetadataMetricsPublisher with a snapshot at offset 3836518 (org.apache.kafka.image.loader.MetadataLoader)
kafka  | kafka  | [2026-02-07 05:02:08,455] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing AclPublisher controller id=1 with a snapshot at offset 3836518 (org.apache.kafka.image.loader.MetadataLoader)
kafka  | kafka  | [2026-02-07 05:02:08,462] INFO [controller-1-to-controller-registration-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
kafka  | kafka  | [2026-02-07 05:02:08,462] INFO [ControllerServer id=1] Waiting for all of the authorizer futures to be completed (kafka.server.ControllerServer)
kafka  | kafka  | [2026-02-07 05:02:08,462] INFO [ControllerServer id=1] Finished waiting for all of the authorizer futures to be completed (kafka.server.ControllerServer)
kafka  | kafka  | [2026-02-07 05:02:08,462] INFO [ControllerServer id=1] Waiting for all of the SocketServer Acceptors to be started (kafka.server.ControllerServer)
kafka  | kafka  | [2026-02-07 05:02:08,462] INFO [ControllerServer id=1] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.ControllerServer)
kafka  | kafka  | [2026-02-07 05:02:08,463] INFO [ControllerRegistrationManager id=1 incarnation=E8DW-QnOREap1CLCmgreeQ] initialized channel manager. (kafka.server.ControllerRegistrationManager)
kafka  | kafka  | [2026-02-07 05:02:08,463] INFO [BrokerServer id=1] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)
kafka  | kafka  | [2026-02-07 05:02:08,463] INFO [controller-1-to-controller-registration-channel-manager]: Recorded new KRaft controller, from now on will use node kafka:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
kafka  | kafka  | [2026-02-07 05:02:08,463] INFO [BrokerServer id=1] Starting broker (kafka.server.BrokerServer)
kafka  | kafka  | [2026-02-07 05:02:08,465] INFO [ControllerRegistrationManager id=1 incarnation=E8DW-QnOREap1CLCmgreeQ] sendControllerRegistration: attempting to send ControllerRegistrationRequestData(controllerId=1, incarnationId=E8DW-QnOREap1CLCmgreeQ, zkMigrationReady=false, listeners=[Listener(name='CONTROLLER', host='kafka', port=9093, securityProtocol=0)], features=[Feature(name='kraft.version', minSupportedVersion=0, maxSupportedVersion=1), Feature(name='metadata.version', minSupportedVersion=1, maxSupportedVersion=21)]) (kafka.server.ControllerRegistrationManager)
kafka  | kafka  | [2026-02-07 05:02:08,467] INFO [broker-1-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
kafka  | kafka  | [2026-02-07 05:02:08,467] INFO [broker-1-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
kafka  | kafka  | [2026-02-07 05:02:08,467] INFO [broker-1-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
kafka  | kafka  | [2026-02-07 05:02:08,468] INFO [broker-1-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
kafka  | kafka  | [2026-02-07 05:02:08,486] INFO [BrokerServer id=1] Waiting for controller quorum voters future (kafka.server.BrokerServer)
kafka  | kafka  | [2026-02-07 05:02:08,486] INFO [BrokerServer id=1] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)
kafka  | kafka  | [2026-02-07 05:02:08,487] INFO [broker-1-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
kafka  | kafka  | [2026-02-07 05:02:08,487] INFO [broker-1-to-controller-forwarding-channel-manager]: Recorded new KRaft controller, from now on will use node kafka:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
kafka  | kafka  | [2026-02-07 05:02:08,501] INFO [client-metrics-reaper]: Starting (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
kafka  | kafka  | [2026-02-07 05:02:08,524] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
kafka  | kafka  | [2026-02-07 05:02:08,526] INFO [SocketServer listenerType=BROKER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
kafka  | kafka  | [2026-02-07 05:02:08,531] INFO [broker-1-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
kafka  | kafka  | [2026-02-07 05:02:08,532] INFO [broker-1-to-controller-alter-partition-channel-manager]: Recorded new KRaft controller, from now on will use node kafka:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
kafka  | kafka  | [2026-02-07 05:02:08,540] INFO [broker-1-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
kafka  | kafka  | [2026-02-07 05:02:08,541] INFO [broker-1-to-controller-directory-assignments-channel-manager]: Recorded new KRaft controller, from now on will use node kafka:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
kafka  | kafka  | [2026-02-07 05:02:08,547] INFO [ControllerRegistrationManager id=1 incarnation=E8DW-QnOREap1CLCmgreeQ] Our registration has been persisted to the metadata log. (kafka.server.ControllerRegistrationManager)
kafka  | kafka  | [2026-02-07 05:02:08,549] INFO [ControllerRegistrationManager id=1 incarnation=E8DW-QnOREap1CLCmgreeQ] RegistrationResponseHandler: controller acknowledged ControllerRegistrationRequest. (kafka.server.ControllerRegistrationManager)
kafka  | kafka  | [2026-02-07 05:02:08,561] INFO [ExpirationReaper-1-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
kafka  | kafka  | [2026-02-07 05:02:08,561] INFO [ExpirationReaper-1-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
kafka  | kafka  | [2026-02-07 05:02:08,575] INFO [ExpirationReaper-1-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
kafka  | kafka  | [2026-02-07 05:02:08,575] INFO [ExpirationReaper-1-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
kafka  | kafka  | [2026-02-07 05:02:08,576] INFO [ExpirationReaper-1-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
kafka  | kafka  | [2026-02-07 05:02:08,601] INFO [ExpirationReaper-1-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
kafka  | kafka  | [2026-02-07 05:02:08,602] INFO [ExpirationReaper-1-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
kafka  | kafka  | [2026-02-07 05:02:08,647] INFO Unable to read the broker epoch in /tmp/kafka-logs. (kafka.log.LogManager)
kafka  | kafka  | [2026-02-07 05:02:08,648] INFO [broker-1-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
kafka  | kafka  | [2026-02-07 05:02:08,648] INFO [broker-1-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
kafka  | kafka  | [2026-02-07 05:02:08,651] INFO [BrokerLifecycleManager id=1] Incarnation xB3-aqzvQeatJdHrvWm8GA of broker 1 in cluster ckjPoprWQzOf0-FuNkGfFQ is now STARTING. (kafka.server.BrokerLifecycleManager)
kafka  | kafka  | [2026-02-07 05:02:08,662] INFO [BrokerLifecycleManager id=1] Unable to register broker 1 because the controller returned error DUPLICATE_BROKER_REGISTRATION (kafka.server.BrokerLifecycleManager)
kafka  | kafka  | [2026-02-07 05:02:08,671] INFO [ExpirationReaper-1-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
kafka  | kafka  | [2026-02-07 05:02:08,691] INFO [BrokerServer id=1] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
kafka  | kafka  | [2026-02-07 05:02:08,691] INFO [BrokerServer id=1] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
kafka  | kafka  | [2026-02-07 05:02:08,691] INFO [BrokerServer id=1] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
kafka  | kafka  | [2026-02-07 05:02:08,692] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing MetadataVersionPublisher(id=1) with a snapshot at offset 3836519 (org.apache.kafka.image.loader.MetadataLoader)
kafka  | kafka  | [2026-02-07 05:02:08,692] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 3836519 (org.apache.kafka.image.loader.MetadataLoader)
kafka  | kafka  | [2026-02-07 05:02:08,692] INFO [BrokerMetadataPublisher id=1] Publishing initial metadata at offset OffsetAndEpoch(offset=3836519, epoch=10) with metadata.version 3.9-IV0. (kafka.server.metadata.BrokerMetadataPublisher)
kafka  | kafka  | [2026-02-07 05:02:08,695] INFO Loading logs from log dirs ArrayBuffer(/tmp/kafka-logs) (kafka.log.LogManager)
kafka  | kafka  | [2026-02-07 05:02:08,702] INFO Recovering 51 logs from /tmp/kafka-logs since no clean shutdown file was found (kafka.log.LogManager)
kafka  | kafka  | [2026-02-07 05:02:08,709] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-13. (kafka.log.LogLoader)
kafka  | kafka  | [2026-02-07 05:02:08,709] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,709] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,709] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,715] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,715] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,715] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,717] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-13, topicId=SQC9JBaeSjed4PliMyUgqQ, topic=__consumer_offsets, partition=13, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 8ms (1/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
kafka  | kafka  | [2026-02-07 05:02:08,718] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-46. (kafka.log.LogLoader)
kafka  | kafka  | [2026-02-07 05:02:08,718] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,718] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,718] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,721] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,721] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,721] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,722] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-46, topicId=SQC9JBaeSjed4PliMyUgqQ, topic=__consumer_offsets, partition=46, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 5ms (2/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
kafka  | kafka  | [2026-02-07 05:02:08,722] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-9. (kafka.log.LogLoader)
kafka  | kafka  | [2026-02-07 05:02:08,723] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,723] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,723] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,725] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,725] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,725] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,726] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-9, topicId=SQC9JBaeSjed4PliMyUgqQ, topic=__consumer_offsets, partition=9, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 4ms (3/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
kafka  | kafka  | [2026-02-07 05:02:08,727] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-42. (kafka.log.LogLoader)
kafka  | kafka  | [2026-02-07 05:02:08,727] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,727] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,727] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,730] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,730] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,730] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,731] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-42, topicId=SQC9JBaeSjed4PliMyUgqQ, topic=__consumer_offsets, partition=42, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 5ms (4/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
kafka  | kafka  | [2026-02-07 05:02:08,732] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-21. (kafka.log.LogLoader)
kafka  | kafka  | [2026-02-07 05:02:08,732] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,732] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,732] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,734] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,734] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,734] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,735] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-21, topicId=SQC9JBaeSjed4PliMyUgqQ, topic=__consumer_offsets, partition=21, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 4ms (5/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
kafka  | kafka  | [2026-02-07 05:02:08,736] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-17. (kafka.log.LogLoader)
kafka  | kafka  | [2026-02-07 05:02:08,736] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,736] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,736] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,738] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,738] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,738] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,738] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-17, topicId=SQC9JBaeSjed4PliMyUgqQ, topic=__consumer_offsets, partition=17, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 3ms (6/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
kafka  | kafka  | [2026-02-07 05:02:08,739] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-30. (kafka.log.LogLoader)
kafka  | kafka  | [2026-02-07 05:02:08,739] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,739] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,739] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,741] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,741] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,741] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,742] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-30, topicId=SQC9JBaeSjed4PliMyUgqQ, topic=__consumer_offsets, partition=30, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 4ms (7/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
kafka  | kafka  | [2026-02-07 05:02:08,742] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-26. (kafka.log.LogLoader)
kafka  | kafka  | [2026-02-07 05:02:08,742] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,742] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,742] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,745] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,745] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,745] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,745] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-26, topicId=SQC9JBaeSjed4PliMyUgqQ, topic=__consumer_offsets, partition=26, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 3ms (8/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
kafka  | kafka  | [2026-02-07 05:02:08,746] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-5. (kafka.log.LogLoader)
kafka  | kafka  | [2026-02-07 05:02:08,746] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,746] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,746] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,749] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,749] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,749] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,749] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-5, topicId=SQC9JBaeSjed4PliMyUgqQ, topic=__consumer_offsets, partition=5, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 4ms (9/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
kafka  | kafka  | [2026-02-07 05:02:08,750] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-38. (kafka.log.LogLoader)
kafka  | kafka  | [2026-02-07 05:02:08,750] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,750] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,750] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,752] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,752] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,752] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,753] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-38, topicId=SQC9JBaeSjed4PliMyUgqQ, topic=__consumer_offsets, partition=38, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 3ms (10/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
kafka  | kafka  | [2026-02-07 05:02:08,754] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-1. (kafka.log.LogLoader)
kafka  | kafka  | [2026-02-07 05:02:08,754] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,754] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,754] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,756] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,756] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,756] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,757] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-1, topicId=SQC9JBaeSjed4PliMyUgqQ, topic=__consumer_offsets, partition=1, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 4ms (11/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
kafka  | kafka  | [2026-02-07 05:02:08,758] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-34. (kafka.log.LogLoader)
kafka  | kafka  | [2026-02-07 05:02:08,758] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,758] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,758] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,760] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,760] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,760] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,761] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-34, topicId=SQC9JBaeSjed4PliMyUgqQ, topic=__consumer_offsets, partition=34, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 4ms (12/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
kafka  | kafka  | [2026-02-07 05:02:08,763] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-16. (kafka.log.LogLoader)
kafka  | kafka  | [2026-02-07 05:02:08,763] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,763] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,763] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,764] INFO [BrokerLifecycleManager id=1] Unable to register broker 1 because the controller returned error DUPLICATE_BROKER_REGISTRATION (kafka.server.BrokerLifecycleManager)
kafka  | kafka  | [2026-02-07 05:02:08,766] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,766] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,766] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,766] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-16, topicId=SQC9JBaeSjed4PliMyUgqQ, topic=__consumer_offsets, partition=16, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 5ms (13/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
kafka  | kafka  | [2026-02-07 05:02:08,767] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-45. (kafka.log.LogLoader)
kafka  | kafka  | [2026-02-07 05:02:08,767] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,767] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,767] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,770] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,770] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,770] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,771] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-45, topicId=SQC9JBaeSjed4PliMyUgqQ, topic=__consumer_offsets, partition=45, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 5ms (14/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
kafka  | kafka  | [2026-02-07 05:02:08,771] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-12. (kafka.log.LogLoader)
kafka  | kafka  | [2026-02-07 05:02:08,772] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,772] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,772] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,774] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,774] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,774] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,775] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-12, topicId=SQC9JBaeSjed4PliMyUgqQ, topic=__consumer_offsets, partition=12, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 4ms (15/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
kafka  | kafka  | [2026-02-07 05:02:08,775] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-41. (kafka.log.LogLoader)
kafka  | kafka  | [2026-02-07 05:02:08,775] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,775] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,775] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,778] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,778] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,778] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,778] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-41, topicId=SQC9JBaeSjed4PliMyUgqQ, topic=__consumer_offsets, partition=41, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 3ms (16/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
kafka  | kafka  | [2026-02-07 05:02:08,779] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-24. (kafka.log.LogLoader)
kafka  | kafka  | [2026-02-07 05:02:08,779] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,779] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,779] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,781] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,781] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,781] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,782] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-24, topicId=SQC9JBaeSjed4PliMyUgqQ, topic=__consumer_offsets, partition=24, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 4ms (17/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
kafka  | kafka  | [2026-02-07 05:02:08,782] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-20. (kafka.log.LogLoader)
kafka  | kafka  | [2026-02-07 05:02:08,783] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,783] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,783] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,784] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,785] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,785] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,785] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-20, topicId=SQC9JBaeSjed4PliMyUgqQ, topic=__consumer_offsets, partition=20, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 3ms (18/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
kafka  | kafka  | [2026-02-07 05:02:08,786] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-49. (kafka.log.LogLoader)
kafka  | kafka  | [2026-02-07 05:02:08,786] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,786] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,786] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,788] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,788] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,788] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,788] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-49, topicId=SQC9JBaeSjed4PliMyUgqQ, topic=__consumer_offsets, partition=49, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 3ms (19/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
kafka  | kafka  | [2026-02-07 05:02:08,789] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-0. (kafka.log.LogLoader)
kafka  | kafka  | [2026-02-07 05:02:08,789] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,789] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,789] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,790] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,790] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,790] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,791] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-0, topicId=SQC9JBaeSjed4PliMyUgqQ, topic=__consumer_offsets, partition=0, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 3ms (20/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
kafka  | kafka  | [2026-02-07 05:02:08,791] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-29. (kafka.log.LogLoader)
kafka  | kafka  | [2026-02-07 05:02:08,791] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,791] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,791] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,793] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,793] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,793] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,794] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-29, topicId=SQC9JBaeSjed4PliMyUgqQ, topic=__consumer_offsets, partition=29, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 3ms (21/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
kafka  | kafka  | [2026-02-07 05:02:08,794] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-25. (kafka.log.LogLoader)
kafka  | kafka  | [2026-02-07 05:02:08,795] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,795] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,795] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,797] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,797] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,797] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,797] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-25, topicId=SQC9JBaeSjed4PliMyUgqQ, topic=__consumer_offsets, partition=25, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 3ms (22/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
kafka  | kafka  | [2026-02-07 05:02:08,798] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-8. (kafka.log.LogLoader)
kafka  | kafka  | [2026-02-07 05:02:08,798] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,798] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,798] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,800] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,800] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,800] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,800] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-8, topicId=SQC9JBaeSjed4PliMyUgqQ, topic=__consumer_offsets, partition=8, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 3ms (23/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
kafka  | kafka  | [2026-02-07 05:02:08,801] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-37. (kafka.log.LogLoader)
kafka  | kafka  | [2026-02-07 05:02:08,801] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,801] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,801] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,803] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,803] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,803] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,803] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-37, topicId=SQC9JBaeSjed4PliMyUgqQ, topic=__consumer_offsets, partition=37, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 3ms (24/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
kafka  | kafka  | [2026-02-07 05:02:08,804] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-4. (kafka.log.LogLoader)
kafka  | kafka  | [2026-02-07 05:02:08,804] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,804] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,804] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,806] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,806] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,806] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,807] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-4, topicId=SQC9JBaeSjed4PliMyUgqQ, topic=__consumer_offsets, partition=4, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 3ms (25/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
kafka  | kafka  | [2026-02-07 05:02:08,807] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-33. (kafka.log.LogLoader)
kafka  | kafka  | [2026-02-07 05:02:08,807] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,807] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,808] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,809] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,810] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,810] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,810] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-33, topicId=SQC9JBaeSjed4PliMyUgqQ, topic=__consumer_offsets, partition=33, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 3ms (26/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
kafka  | kafka  | [2026-02-07 05:02:08,811] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-15. (kafka.log.LogLoader)
kafka  | kafka  | [2026-02-07 05:02:08,811] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,811] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,811] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,813] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,813] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,813] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,814] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-15, topicId=SQC9JBaeSjed4PliMyUgqQ, topic=__consumer_offsets, partition=15, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 4ms (27/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
kafka  | kafka  | [2026-02-07 05:02:08,814] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-48. (kafka.log.LogLoader)
kafka  | kafka  | [2026-02-07 05:02:08,814] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,814] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,814] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,816] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,816] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,816] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,817] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-48, topicId=SQC9JBaeSjed4PliMyUgqQ, topic=__consumer_offsets, partition=48, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 3ms (28/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
kafka  | kafka  | [2026-02-07 05:02:08,817] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-11. (kafka.log.LogLoader)
kafka  | kafka  | [2026-02-07 05:02:08,817] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,817] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,817] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,819] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,819] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,819] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,820] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-11, topicId=SQC9JBaeSjed4PliMyUgqQ, topic=__consumer_offsets, partition=11, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 3ms (29/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
kafka  | kafka  | [2026-02-07 05:02:08,820] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Recovering unflushed segment 327234. 0/1 recovered for __consumer_offsets-44. (kafka.log.LogLoader)
kafka  | kafka  | [2026-02-07 05:02:08,821] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 327234 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,821] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 327234 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,821] INFO [ProducerStateManager partition=__consumer_offsets-44] Loading producer state from snapshot file 'SnapshotFile(offset=327234, file=/tmp/kafka-logs/__consumer_offsets-44/00000000000000327234.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
kafka  | kafka  | [2026-02-07 05:02:08,821] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 327234 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,946] INFO [ProducerStateManager partition=__consumer_offsets-44] Wrote producer snapshot at offset 375294 with 0 producer ids in 1 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
kafka  | kafka  | [2026-02-07 05:02:08,951] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 375294 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,951] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 375294 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,951] INFO [ProducerStateManager partition=__consumer_offsets-44] Loading producer state from snapshot file 'SnapshotFile(offset=375294, file=/tmp/kafka-logs/__consumer_offsets-44/00000000000000375294.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
kafka  | kafka  | [2026-02-07 05:02:08,951] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 375294 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,952] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-44, topicId=SQC9JBaeSjed4PliMyUgqQ, topic=__consumer_offsets, partition=44, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=375294) with 2 segments, local-log-start-offset 0 and log-end-offset 375294 in 132ms (30/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
kafka  | kafka  | [2026-02-07 05:02:08,953] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-23. (kafka.log.LogLoader)
kafka  | kafka  | [2026-02-07 05:02:08,954] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,954] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,954] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,956] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,956] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,956] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,957] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-23, topicId=SQC9JBaeSjed4PliMyUgqQ, topic=__consumer_offsets, partition=23, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 5ms (31/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
kafka  | kafka  | [2026-02-07 05:02:08,957] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-19. (kafka.log.LogLoader)
kafka  | kafka  | [2026-02-07 05:02:08,958] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,958] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,958] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,960] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,960] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,960] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,960] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-19, topicId=SQC9JBaeSjed4PliMyUgqQ, topic=__consumer_offsets, partition=19, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 3ms (32/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
kafka  | kafka  | [2026-02-07 05:02:08,961] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-32. (kafka.log.LogLoader)
kafka  | kafka  | [2026-02-07 05:02:08,961] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,961] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,961] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,964] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,964] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,964] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,964] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-32, topicId=SQC9JBaeSjed4PliMyUgqQ, topic=__consumer_offsets, partition=32, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 3ms (33/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
kafka  | kafka  | [2026-02-07 05:02:08,965] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-28. (kafka.log.LogLoader)
kafka  | kafka  | [2026-02-07 05:02:08,965] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,965] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,965] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,965] INFO [BrokerLifecycleManager id=1] Unable to register broker 1 because the controller returned error DUPLICATE_BROKER_REGISTRATION (kafka.server.BrokerLifecycleManager)
kafka  | kafka  | [2026-02-07 05:02:08,967] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,967] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,967] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,968] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-28, topicId=SQC9JBaeSjed4PliMyUgqQ, topic=__consumer_offsets, partition=28, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 4ms (34/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
kafka  | kafka  | [2026-02-07 05:02:08,969] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-7. (kafka.log.LogLoader)
kafka  | kafka  | [2026-02-07 05:02:08,969] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,969] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,969] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,971] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,971] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,971] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,972] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-7, topicId=SQC9JBaeSjed4PliMyUgqQ, topic=__consumer_offsets, partition=7, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 4ms (35/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
kafka  | kafka  | [2026-02-07 05:02:08,972] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-40. (kafka.log.LogLoader)
kafka  | kafka  | [2026-02-07 05:02:08,972] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,973] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,973] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,975] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,976] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,976] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,976] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-40, topicId=SQC9JBaeSjed4PliMyUgqQ, topic=__consumer_offsets, partition=40, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 4ms (36/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
kafka  | kafka  | [2026-02-07 05:02:08,978] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-3. (kafka.log.LogLoader)
kafka  | kafka  | [2026-02-07 05:02:08,978] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,978] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,978] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,981] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,981] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,981] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,982] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-3, topicId=SQC9JBaeSjed4PliMyUgqQ, topic=__consumer_offsets, partition=3, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 5ms (37/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
kafka  | kafka  | [2026-02-07 05:02:08,983] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-36. (kafka.log.LogLoader)
kafka  | kafka  | [2026-02-07 05:02:08,983] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,983] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,983] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,986] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,986] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,986] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,987] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-36, topicId=SQC9JBaeSjed4PliMyUgqQ, topic=__consumer_offsets, partition=36, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 5ms (38/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
kafka  | kafka  | [2026-02-07 05:02:08,988] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-47. (kafka.log.LogLoader)
kafka  | kafka  | [2026-02-07 05:02:08,988] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,988] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,988] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,990] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,990] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,990] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,991] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-47, topicId=SQC9JBaeSjed4PliMyUgqQ, topic=__consumer_offsets, partition=47, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 4ms (39/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
kafka  | kafka  | [2026-02-07 05:02:08,991] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-14. (kafka.log.LogLoader)
kafka  | kafka  | [2026-02-07 05:02:08,992] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,992] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,992] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,994] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,994] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,994] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,994] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-14, topicId=SQC9JBaeSjed4PliMyUgqQ, topic=__consumer_offsets, partition=14, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 3ms (40/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
kafka  | kafka  | [2026-02-07 05:02:08,996] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Recovering unflushed segment 12652. 0/1 recovered for __consumer_offsets-43. (kafka.log.LogLoader)
kafka  | kafka  | [2026-02-07 05:02:08,996] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 12652 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,996] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 12652 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:08,996] INFO [ProducerStateManager partition=__consumer_offsets-43] Loading producer state from snapshot file 'SnapshotFile(offset=12652, file=/tmp/kafka-logs/__consumer_offsets-43/00000000000000012652.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
kafka  | kafka  | [2026-02-07 05:02:08,996] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 12652 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,000] INFO [ProducerStateManager partition=__consumer_offsets-43] Wrote producer snapshot at offset 12653 with 0 producer ids in 0 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
kafka  | kafka  | [2026-02-07 05:02:09,001] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 12653 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,001] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 12653 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,001] INFO [ProducerStateManager partition=__consumer_offsets-43] Loading producer state from snapshot file 'SnapshotFile(offset=12653, file=/tmp/kafka-logs/__consumer_offsets-43/00000000000000012653.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
kafka  | kafka  | [2026-02-07 05:02:09,001] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 12653 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,001] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-43, topicId=SQC9JBaeSjed4PliMyUgqQ, topic=__consumer_offsets, partition=43, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=12653) with 3 segments, local-log-start-offset 0 and log-end-offset 12653 in 6ms (41/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
kafka  | kafka  | [2026-02-07 05:02:09,002] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-10. (kafka.log.LogLoader)
kafka  | kafka  | [2026-02-07 05:02:09,002] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,002] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,002] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,005] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,005] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,005] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,005] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-10, topicId=SQC9JBaeSjed4PliMyUgqQ, topic=__consumer_offsets, partition=10, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 3ms (42/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
kafka  | kafka  | [2026-02-07 05:02:09,006] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-22. (kafka.log.LogLoader)
kafka  | kafka  | [2026-02-07 05:02:09,006] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,006] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,006] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,008] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,008] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,008] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,008] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-22, topicId=SQC9JBaeSjed4PliMyUgqQ, topic=__consumer_offsets, partition=22, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 4ms (43/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
kafka  | kafka  | [2026-02-07 05:02:09,009] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-18. (kafka.log.LogLoader)
kafka  | kafka  | [2026-02-07 05:02:09,009] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,009] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,009] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,013] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,013] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,013] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,014] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-18, topicId=SQC9JBaeSjed4PliMyUgqQ, topic=__consumer_offsets, partition=18, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 5ms (44/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
kafka  | kafka  | [2026-02-07 05:02:09,016] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-31. (kafka.log.LogLoader)
kafka  | kafka  | [2026-02-07 05:02:09,016] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,016] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,016] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,019] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,019] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,019] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,020] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-31, topicId=SQC9JBaeSjed4PliMyUgqQ, topic=__consumer_offsets, partition=31, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 6ms (45/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
kafka  | kafka  | [2026-02-07 05:02:09,021] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-27. (kafka.log.LogLoader)
kafka  | kafka  | [2026-02-07 05:02:09,021] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,021] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,021] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,025] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,025] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,025] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,025] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-27, topicId=SQC9JBaeSjed4PliMyUgqQ, topic=__consumer_offsets, partition=27, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 5ms (46/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
kafka  | kafka  | [2026-02-07 05:02:09,026] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-39. (kafka.log.LogLoader)
kafka  | kafka  | [2026-02-07 05:02:09,027] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,027] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,027] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,030] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,030] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,030] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,031] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-39, topicId=SQC9JBaeSjed4PliMyUgqQ, topic=__consumer_offsets, partition=39, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 5ms (47/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
kafka  | kafka  | [2026-02-07 05:02:09,032] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-6. (kafka.log.LogLoader)
kafka  | kafka  | [2026-02-07 05:02:09,032] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,032] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,032] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,035] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,035] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,035] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,035] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-6, topicId=SQC9JBaeSjed4PliMyUgqQ, topic=__consumer_offsets, partition=6, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 4ms (48/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
kafka  | kafka  | [2026-02-07 05:02:09,036] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-35. (kafka.log.LogLoader)
kafka  | kafka  | [2026-02-07 05:02:09,037] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,037] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,037] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,039] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,039] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,039] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,039] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-35, topicId=SQC9JBaeSjed4PliMyUgqQ, topic=__consumer_offsets, partition=35, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 3ms (49/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
kafka  | kafka  | [2026-02-07 05:02:09,040] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-2. (kafka.log.LogLoader)
kafka  | kafka  | [2026-02-07 05:02:09,040] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,040] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,040] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,042] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,042] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,042] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,043] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-2, topicId=SQC9JBaeSjed4PliMyUgqQ, topic=__consumer_offsets, partition=2, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 4ms (50/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
kafka  | kafka  | [2026-02-07 05:02:09,044] INFO [LogLoader partition=orders-0, dir=/tmp/kafka-logs] Recovering unflushed segment 12947. 0/1 recovered for orders-0. (kafka.log.LogLoader)
kafka  | kafka  | [2026-02-07 05:02:09,044] INFO [LogLoader partition=orders-0, dir=/tmp/kafka-logs] Loading producer state till offset 12947 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,044] INFO [LogLoader partition=orders-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 12947 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,046] INFO [ProducerStateManager partition=orders-0] Wrote producer snapshot at offset 12947 with 0 producer ids in 1 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
kafka  | kafka  | [2026-02-07 05:02:09,046] INFO [LogLoader partition=orders-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 2ms for segment recovery from offset 12947 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,050] INFO [LogLoader partition=orders-0, dir=/tmp/kafka-logs] Loading producer state till offset 12947 with message format version 2 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,050] INFO [LogLoader partition=orders-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 12947 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,050] INFO Deleted producer state snapshot /tmp/kafka-logs/orders-0/00000000000000012947.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
kafka  | kafka  | [2026-02-07 05:02:09,050] INFO [LogLoader partition=orders-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 12947 (kafka.log.UnifiedLog$)
kafka  | kafka  | [2026-02-07 05:02:09,051] INFO Completed load of Log(dir=/tmp/kafka-logs/orders-0, topicId=lBgkzPyoRlC9BIOAkeaWlQ, topic=orders, partition=0, highWatermark=12947, lastStableOffset=12947, logStartOffset=12947, logEndOffset=12947) with 1 segments, local-log-start-offset 12947 and log-end-offset 12947 in 8ms (51/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
kafka  | kafka  | [2026-02-07 05:02:09,053] INFO Loaded 51 logs in 357ms (unclean log dirs = ArrayBuffer(/tmp/kafka-logs)) (kafka.log.LogManager)
kafka  | kafka  | [2026-02-07 05:02:09,053] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
kafka  | kafka  | [2026-02-07 05:02:09,055] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
kafka  | kafka  | [2026-02-07 05:02:09,129] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)
kafka  | kafka  | [2026-02-07 05:02:09,136] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
kafka  | kafka  | [2026-02-07 05:02:09,137] INFO [AddPartitionsToTxnSenderThread-1]: Starting (kafka.server.AddPartitionsToTxnManager)
kafka  | kafka  | [2026-02-07 05:02:09,137] INFO [GroupCoordinator 1]: Starting up. (kafka.coordinator.group.GroupCoordinator)
kafka  | kafka  | [2026-02-07 05:02:09,140] INFO [GroupCoordinator 1]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
kafka  | kafka  | [2026-02-07 05:02:09,140] INFO [TransactionCoordinator id=1] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
kafka  | kafka  | [2026-02-07 05:02:09,144] INFO [TxnMarkerSenderThread-1]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
kafka  | kafka  | [2026-02-07 05:02:09,144] INFO [TransactionCoordinator id=1] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
kafka  | kafka  | [2026-02-07 05:02:09,149] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(__consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-17, __consumer_offsets-30, __consumer_offsets-26, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-16, __consumer_offsets-45, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-0, __consumer_offsets-29, __consumer_offsets-25, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, __consumer_offsets-28, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-47, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-27, __consumer_offsets-39, orders-0, __consumer_offsets-6, __consumer_offsets-35, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
kafka  | kafka  | [2026-02-07 05:02:09,165] INFO [Partition __consumer_offsets-13 broker=1] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
kafka  | kafka  | [2026-02-07 05:02:09,172] INFO [Partition __consumer_offsets-46 broker=1] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
kafka  | kafka  | [2026-02-07 05:02:09,174] INFO [Partition __consumer_offsets-9 broker=1] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
kafka  | kafka  | [2026-02-07 05:02:09,177] INFO [Partition __consumer_offsets-42 broker=1] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
kafka  | kafka  | [2026-02-07 05:02:09,180] INFO [Partition __consumer_offsets-21 broker=1] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
kafka  | kafka  | [2026-02-07 05:02:09,182] INFO [Partition __consumer_offsets-17 broker=1] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
kafka  | kafka  | [2026-02-07 05:02:09,184] INFO [Partition __consumer_offsets-30 broker=1] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
kafka  | kafka  | [2026-02-07 05:02:09,186] INFO [Partition __consumer_offsets-26 broker=1] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
kafka  | kafka  | [2026-02-07 05:02:09,188] INFO [Partition __consumer_offsets-5 broker=1] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
kafka  | kafka  | [2026-02-07 05:02:09,190] INFO [Partition __consumer_offsets-38 broker=1] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
kafka  | kafka  | [2026-02-07 05:02:09,193] INFO [Partition __consumer_offsets-1 broker=1] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
kafka  | kafka  | [2026-02-07 05:02:09,196] INFO [Partition __consumer_offsets-34 broker=1] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
kafka  | kafka  | [2026-02-07 05:02:09,199] INFO [Partition __consumer_offsets-16 broker=1] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
kafka  | kafka  | [2026-02-07 05:02:09,203] INFO [Partition __consumer_offsets-45 broker=1] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
kafka  | kafka  | [2026-02-07 05:02:09,206] INFO [Partition __consumer_offsets-12 broker=1] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
kafka  | kafka  | [2026-02-07 05:02:09,208] INFO [Partition __consumer_offsets-41 broker=1] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
kafka  | kafka  | [2026-02-07 05:02:09,211] INFO [Partition __consumer_offsets-24 broker=1] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
kafka  | kafka  | [2026-02-07 05:02:09,213] INFO [Partition __consumer_offsets-20 broker=1] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
kafka  | kafka  | [2026-02-07 05:02:09,214] INFO [Partition __consumer_offsets-49 broker=1] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
kafka  | kafka  | [2026-02-07 05:02:09,216] INFO [Partition __consumer_offsets-0 broker=1] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
kafka  | kafka  | [2026-02-07 05:02:09,218] INFO [Partition __consumer_offsets-29 broker=1] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)
kafka  | kafka  | [2026-02-07 05:02:09,220] INFO [Partition __consumer_offsets-25 broker=1] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
kafka  | kafka  | [2026-02-07 05:02:09,221] INFO [Partition __consumer_offsets-8 broker=1] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
kafka  | kafka  | [2026-02-07 05:02:09,223] INFO [Partition __consumer_offsets-37 broker=1] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
kafka  | kafka  | [2026-02-07 05:02:09,225] INFO [Partition __consumer_offsets-4 broker=1] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
kafka  | kafka  | [2026-02-07 05:02:09,226] INFO [Partition __consumer_offsets-33 broker=1] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
kafka  | kafka  | [2026-02-07 05:02:09,228] INFO [Partition __consumer_offsets-15 broker=1] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
kafka  | kafka  | [2026-02-07 05:02:09,230] INFO [Partition __consumer_offsets-48 broker=1] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
kafka  | kafka  | [2026-02-07 05:02:09,231] INFO [Partition __consumer_offsets-11 broker=1] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
kafka  | kafka  | [2026-02-07 05:02:09,233] INFO [Partition __consumer_offsets-44 broker=1] Log loaded for partition __consumer_offsets-44 with initial high watermark 375294 (kafka.cluster.Partition)
kafka  | kafka  | [2026-02-07 05:02:09,233] INFO [Partition __consumer_offsets-23 broker=1] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
kafka  | kafka  | [2026-02-07 05:02:09,235] INFO [Partition __consumer_offsets-19 broker=1] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
kafka  | kafka  | [2026-02-07 05:02:09,236] INFO [Partition __consumer_offsets-32 broker=1] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
kafka  | kafka  | [2026-02-07 05:02:09,238] INFO [Partition __consumer_offsets-28 broker=1] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
kafka  | kafka  | [2026-02-07 05:02:09,239] INFO [Partition __consumer_offsets-7 broker=1] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
kafka  | kafka  | [2026-02-07 05:02:09,241] INFO [Partition __consumer_offsets-40 broker=1] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
kafka  | kafka  | [2026-02-07 05:02:09,243] INFO [Partition __consumer_offsets-3 broker=1] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
kafka  | kafka  | [2026-02-07 05:02:09,244] INFO [Partition __consumer_offsets-36 broker=1] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
kafka  | kafka  | [2026-02-07 05:02:09,246] INFO [Partition __consumer_offsets-47 broker=1] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
kafka  | kafka  | [2026-02-07 05:02:09,248] INFO [Partition __consumer_offsets-14 broker=1] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
kafka  | kafka  | [2026-02-07 05:02:09,249] INFO [Partition __consumer_offsets-43 broker=1] Log loaded for partition __consumer_offsets-43 with initial high watermark 12653 (kafka.cluster.Partition)
kafka  | kafka  | [2026-02-07 05:02:09,250] INFO [Partition __consumer_offsets-10 broker=1] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
kafka  | kafka  | [2026-02-07 05:02:09,251] INFO [Partition __consumer_offsets-22 broker=1] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
kafka  | kafka  | [2026-02-07 05:02:09,253] INFO [Partition __consumer_offsets-18 broker=1] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
kafka  | kafka  | [2026-02-07 05:02:09,255] INFO [Partition __consumer_offsets-31 broker=1] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
kafka  | kafka  | [2026-02-07 05:02:09,256] INFO [Partition __consumer_offsets-27 broker=1] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
kafka  | kafka  | [2026-02-07 05:02:09,258] INFO [Partition __consumer_offsets-39 broker=1] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
kafka  | kafka  | [2026-02-07 05:02:09,260] INFO [Partition orders-0 broker=1] Log loaded for partition orders-0 with initial high watermark 12947 (kafka.cluster.Partition)
kafka  | kafka  | [2026-02-07 05:02:09,261] INFO [Partition __consumer_offsets-6 broker=1] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
kafka  | kafka  | [2026-02-07 05:02:09,263] INFO [Partition __consumer_offsets-35 broker=1] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
kafka  | kafka  | [2026-02-07 05:02:09,265] INFO [Partition __consumer_offsets-2 broker=1] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
kafka  | kafka  | [2026-02-07 05:02:09,271] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 13 in epoch 14 (kafka.coordinator.group.GroupCoordinator)
kafka  | kafka  | [2026-02-07 05:02:09,272] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-13 for epoch 14 (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 46 in epoch 14 (kafka.coordinator.group.GroupCoordinator)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 14 (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 9 in epoch 14 (kafka.coordinator.group.GroupCoordinator)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-9 for epoch 14 (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 42 in epoch 14 (kafka.coordinator.group.GroupCoordinator)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 14 (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 21 in epoch 14 (kafka.coordinator.group.GroupCoordinator)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-21 for epoch 14 (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 17 in epoch 14 (kafka.coordinator.group.GroupCoordinator)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-17 for epoch 14 (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 30 in epoch 14 (kafka.coordinator.group.GroupCoordinator)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-30 for epoch 14 (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 26 in epoch 14 (kafka.coordinator.group.GroupCoordinator)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-26 for epoch 14 (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 5 in epoch 14 (kafka.coordinator.group.GroupCoordinator)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-5 for epoch 14 (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 38 in epoch 14 (kafka.coordinator.group.GroupCoordinator)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-38 for epoch 14 (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 1 in epoch 14 (kafka.coordinator.group.GroupCoordinator)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 14 (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 34 in epoch 14 (kafka.coordinator.group.GroupCoordinator)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-34 for epoch 14 (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 16 in epoch 14 (kafka.coordinator.group.GroupCoordinator)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-16 for epoch 14 (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 45 in epoch 14 (kafka.coordinator.group.GroupCoordinator)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-45 for epoch 14 (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 12 in epoch 14 (kafka.coordinator.group.GroupCoordinator)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-12 for epoch 14 (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 41 in epoch 14 (kafka.coordinator.group.GroupCoordinator)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-41 for epoch 14 (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 24 in epoch 14 (kafka.coordinator.group.GroupCoordinator)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-24 for epoch 14 (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 20 in epoch 14 (kafka.coordinator.group.GroupCoordinator)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-20 for epoch 14 (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 49 in epoch 14 (kafka.coordinator.group.GroupCoordinator)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-49 for epoch 14 (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 0 in epoch 14 (kafka.coordinator.group.GroupCoordinator)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 14 (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 29 in epoch 14 (kafka.coordinator.group.GroupCoordinator)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-29 for epoch 14 (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 25 in epoch 14 (kafka.coordinator.group.GroupCoordinator)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-25 for epoch 14 (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 8 in epoch 14 (kafka.coordinator.group.GroupCoordinator)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 14 (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 37 in epoch 14 (kafka.coordinator.group.GroupCoordinator)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-37 for epoch 14 (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 4 in epoch 14 (kafka.coordinator.group.GroupCoordinator)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 14 (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 33 in epoch 14 (kafka.coordinator.group.GroupCoordinator)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-33 for epoch 14 (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 15 in epoch 14 (kafka.coordinator.group.GroupCoordinator)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-15 for epoch 14 (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 48 in epoch 14 (kafka.coordinator.group.GroupCoordinator)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 14 (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 11 in epoch 14 (kafka.coordinator.group.GroupCoordinator)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-11 for epoch 14 (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 44 in epoch 14 (kafka.coordinator.group.GroupCoordinator)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-44 for epoch 14 (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 23 in epoch 14 (kafka.coordinator.group.GroupCoordinator)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-23 for epoch 14 (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 19 in epoch 14 (kafka.coordinator.group.GroupCoordinator)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-19 for epoch 14 (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 32 in epoch 14 (kafka.coordinator.group.GroupCoordinator)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-32 for epoch 14 (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 28 in epoch 14 (kafka.coordinator.group.GroupCoordinator)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 14 (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 7 in epoch 14 (kafka.coordinator.group.GroupCoordinator)
kafka  | kafka  | [2026-02-07 05:02:09,275] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-7 for epoch 14 (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,276] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 40 in epoch 14 (kafka.coordinator.group.GroupCoordinator)
kafka  | kafka  | [2026-02-07 05:02:09,276] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-40 for epoch 14 (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,276] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 3 in epoch 14 (kafka.coordinator.group.GroupCoordinator)
kafka  | kafka  | [2026-02-07 05:02:09,276] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 14 (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,276] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 36 in epoch 14 (kafka.coordinator.group.GroupCoordinator)
kafka  | kafka  | [2026-02-07 05:02:09,276] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-36 for epoch 14 (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,276] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 47 in epoch 14 (kafka.coordinator.group.GroupCoordinator)
kafka  | kafka  | [2026-02-07 05:02:09,276] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-47 for epoch 14 (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,276] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 14 in epoch 14 (kafka.coordinator.group.GroupCoordinator)
kafka  | kafka  | [2026-02-07 05:02:09,276] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 14 (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,276] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 43 in epoch 14 (kafka.coordinator.group.GroupCoordinator)
kafka  | kafka  | [2026-02-07 05:02:09,276] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-43 for epoch 14 (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,276] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 10 in epoch 14 (kafka.coordinator.group.GroupCoordinator)
kafka  | kafka  | [2026-02-07 05:02:09,276] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-10 for epoch 14 (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,276] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 22 in epoch 14 (kafka.coordinator.group.GroupCoordinator)
kafka  | kafka  | [2026-02-07 05:02:09,276] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-22 for epoch 14 (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,276] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 18 in epoch 14 (kafka.coordinator.group.GroupCoordinator)
kafka  | kafka  | [2026-02-07 05:02:09,276] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-18 for epoch 14 (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,276] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 31 in epoch 14 (kafka.coordinator.group.GroupCoordinator)
kafka  | kafka  | [2026-02-07 05:02:09,276] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-31 for epoch 14 (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,276] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 27 in epoch 14 (kafka.coordinator.group.GroupCoordinator)
kafka  | kafka  | [2026-02-07 05:02:09,276] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-27 for epoch 14 (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,276] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 39 in epoch 14 (kafka.coordinator.group.GroupCoordinator)
kafka  | kafka  | [2026-02-07 05:02:09,276] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-39 for epoch 14 (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,276] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 6 in epoch 14 (kafka.coordinator.group.GroupCoordinator)
kafka  | kafka  | [2026-02-07 05:02:09,276] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-6 for epoch 14 (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,276] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 35 in epoch 14 (kafka.coordinator.group.GroupCoordinator)
kafka  | kafka  | [2026-02-07 05:02:09,276] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 14 (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,276] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 2 in epoch 14 (kafka.coordinator.group.GroupCoordinator)
kafka  | kafka  | [2026-02-07 05:02:09,276] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 14 (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,280] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-13 in 7 milliseconds for epoch 14, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,281] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-46 in 6 milliseconds for epoch 14, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,281] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-9 in 6 milliseconds for epoch 14, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,281] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-42 in 6 milliseconds for epoch 14, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,281] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-21 in 6 milliseconds for epoch 14, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,281] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-17 in 6 milliseconds for epoch 14, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,281] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-30 in 6 milliseconds for epoch 14, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,281] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-26 in 6 milliseconds for epoch 14, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,281] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-5 in 6 milliseconds for epoch 14, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,282] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-38 in 7 milliseconds for epoch 14, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,282] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-1 in 7 milliseconds for epoch 14, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,282] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-34 in 7 milliseconds for epoch 14, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,282] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-16 in 7 milliseconds for epoch 14, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,282] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-45 in 7 milliseconds for epoch 14, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,282] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-12 in 7 milliseconds for epoch 14, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,282] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-41 in 7 milliseconds for epoch 14, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,282] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-24 in 7 milliseconds for epoch 14, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,282] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-20 in 7 milliseconds for epoch 14, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,282] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-49 in 7 milliseconds for epoch 14, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,283] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 8 milliseconds for epoch 14, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,283] INFO [DynamicConfigPublisher broker id=1] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600 (kafka.server.metadata.DynamicConfigPublisher)
kafka  | kafka  | [2026-02-07 05:02:09,283] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-29 in 8 milliseconds for epoch 14, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,283] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-25 in 8 milliseconds for epoch 14, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,283] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-8 in 8 milliseconds for epoch 14, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,283] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-37 in 8 milliseconds for epoch 14, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,283] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-4 in 8 milliseconds for epoch 14, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,283] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-33 in 8 milliseconds for epoch 14, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,283] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-15 in 8 milliseconds for epoch 14, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,284] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-48 in 9 milliseconds for epoch 14, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,284] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-11 in 9 milliseconds for epoch 14, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,291] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing BrokerRegistrationTracker(id=1) with a snapshot at offset 3836519 (org.apache.kafka.image.loader.MetadataLoader)
kafka  | kafka  | [2026-02-07 05:02:09,298] INFO Loaded member MemberMetadata(memberId=consumer-fraud-detection-1-1bd090dd-f3df-477d-8395-a30d4511b003, groupInstanceId=None, clientId=consumer-fraud-detection-1, clientHost=/172.22.0.15, sessionTimeoutMs=45000, rebalanceTimeoutMs=300000, supportedProtocols=List(range)) in group fraud-detection with generation 21. (kafka.coordinator.group.GroupMetadata$)
kafka  | kafka  | [2026-02-07 05:02:09,306] INFO Loaded member MemberMetadata(memberId=consumer-fraud-detection-1-556005eb-0385-4c93-88a7-50c43f311a21, groupInstanceId=None, clientId=consumer-fraud-detection-1, clientHost=/172.22.0.20, sessionTimeoutMs=45000, rebalanceTimeoutMs=300000, supportedProtocols=List(range)) in group fraud-detection with generation 22. (kafka.coordinator.group.GroupMetadata$)
kafka  | kafka  | [2026-02-07 05:02:09,359] INFO [BrokerLifecycleManager id=1] Unable to register broker 1 because the controller returned error DUPLICATE_BROKER_REGISTRATION (kafka.server.BrokerLifecycleManager)
kafka  | kafka  | [2026-02-07 05:02:09,367] INFO Loaded member MemberMetadata(memberId=consumer-fraud-detection-1-3c36eb72-45fc-4993-bb09-338699276b66, groupInstanceId=None, clientId=consumer-fraud-detection-1, clientHost=/172.22.0.20, sessionTimeoutMs=45000, rebalanceTimeoutMs=300000, supportedProtocols=List(range)) in group fraud-detection with generation 23. (kafka.coordinator.group.GroupMetadata$)
kafka  | kafka  | [2026-02-07 05:02:09,377] INFO [GroupCoordinator 1]: Loading group metadata for fraud-detection with generation 23 (kafka.coordinator.group.GroupCoordinator)
kafka  | kafka  | [2026-02-07 05:02:09,380] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-44 in 105 milliseconds for epoch 14, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,380] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-23 in 105 milliseconds for epoch 14, of which 105 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,380] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-19 in 105 milliseconds for epoch 14, of which 105 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,380] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-32 in 105 milliseconds for epoch 14, of which 105 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,380] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-28 in 105 milliseconds for epoch 14, of which 105 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,380] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-7 in 104 milliseconds for epoch 14, of which 104 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,380] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-40 in 104 milliseconds for epoch 14, of which 104 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,380] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-3 in 104 milliseconds for epoch 14, of which 104 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,380] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-36 in 104 milliseconds for epoch 14, of which 104 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,380] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-47 in 104 milliseconds for epoch 14, of which 104 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,380] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-14 in 104 milliseconds for epoch 14, of which 104 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,381] INFO Loaded member MemberMetadata(memberId=rdkafka-0b0ce831-f2ad-4ee7-bf8a-2cc0adea77bb, groupInstanceId=None, clientId=rdkafka, clientHost=/172.22.0.29, sessionTimeoutMs=45000, rebalanceTimeoutMs=300000, supportedProtocols=List(range)) in group accounting with generation 6. (kafka.coordinator.group.GroupMetadata$)
kafka  | kafka  | [2026-02-07 05:02:09,381] INFO Loaded member MemberMetadata(memberId=rdkafka-ba5a3221-d390-40a8-bb43-edf44b356a96, groupInstanceId=None, clientId=rdkafka, clientHost=/172.22.0.14, sessionTimeoutMs=45000, rebalanceTimeoutMs=300000, supportedProtocols=List(range)) in group accounting with generation 7. (kafka.coordinator.group.GroupMetadata$)
kafka  | kafka  | [2026-02-07 05:02:09,382] INFO Loaded member MemberMetadata(memberId=rdkafka-a8d1e8eb-94b7-4276-8cd8-eceb218ffa0a, groupInstanceId=None, clientId=rdkafka, clientHost=/172.22.0.28, sessionTimeoutMs=45000, rebalanceTimeoutMs=300000, supportedProtocols=List(range)) in group accounting with generation 8. (kafka.coordinator.group.GroupMetadata$)
kafka  | kafka  | [2026-02-07 05:02:09,382] INFO [GroupCoordinator 1]: Loading group metadata for accounting with generation 8 (kafka.coordinator.group.GroupCoordinator)
kafka  | kafka  | [2026-02-07 05:02:09,382] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-43 in 106 milliseconds for epoch 14, of which 104 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,382] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-10 in 106 milliseconds for epoch 14, of which 106 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,382] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-22 in 106 milliseconds for epoch 14, of which 106 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,382] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-18 in 106 milliseconds for epoch 14, of which 106 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,382] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-31 in 106 milliseconds for epoch 14, of which 106 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,382] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-27 in 106 milliseconds for epoch 14, of which 106 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,382] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-39 in 106 milliseconds for epoch 14, of which 106 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,382] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-6 in 106 milliseconds for epoch 14, of which 106 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,382] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-35 in 106 milliseconds for epoch 14, of which 106 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:09,382] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-2 in 106 milliseconds for epoch 14, of which 106 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka  | kafka  | [2026-02-07 05:02:10,152] INFO [BrokerLifecycleManager id=1] Unable to register broker 1 because the controller returned error DUPLICATE_BROKER_REGISTRATION (kafka.server.BrokerLifecycleManager)
kafka  | kafka  | [2026-02-07 05:02:11,785] INFO [BrokerLifecycleManager id=1] Unable to register broker 1 because the controller returned error DUPLICATE_BROKER_REGISTRATION (kafka.server.BrokerLifecycleManager)
kafka  | kafka  | [2026-02-07 05:02:15,009] INFO [BrokerLifecycleManager id=1] Unable to register broker 1 because the controller returned error DUPLICATE_BROKER_REGISTRATION (kafka.server.BrokerLifecycleManager)
kafka  | kafka  | [2026-02-07 05:02:21,555] INFO [BrokerLifecycleManager id=1] Successfully registered broker 1 with broker epoch 3836546 (kafka.server.BrokerLifecycleManager)
kafka  | kafka  | [2026-02-07 05:02:21,561] INFO [BrokerLifecycleManager id=1] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)
kafka  | kafka  | [2026-02-07 05:02:21,561] INFO [BrokerServer id=1] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
kafka  | kafka  | [2026-02-07 05:02:21,561] INFO [BrokerServer id=1] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
kafka  | kafka  | [2026-02-07 05:02:21,561] INFO [BrokerServer id=1] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
kafka  | kafka  | [2026-02-07 05:02:21,561] INFO KafkaConfig values: 
kafka  | kafka  | 	advertised.listeners = PLAINTEXT://kafka:9092
kafka  | kafka  | 	alter.config.policy.class.name = null
kafka  | kafka  | 	alter.log.dirs.replication.quota.window.num = 11
kafka  | kafka  | 	alter.log.dirs.replication.quota.window.size.seconds = 1
kafka  | kafka  | 	authorizer.class.name = 
kafka  | kafka  | 	auto.create.topics.enable = true
kafka  | kafka  | 	auto.include.jmx.reporter = true
kafka  | kafka  | 	auto.leader.rebalance.enable = true
kafka  | kafka  | 	background.threads = 10
kafka  | kafka  | 	broker.heartbeat.interval.ms = 2000
kafka  | kafka  | 	broker.id = 1
kafka  | kafka  | 	broker.id.generation.enable = true
kafka  | kafka  | 	broker.rack = null
kafka  | kafka  | 	broker.session.timeout.ms = 9000
kafka  | kafka  | 	client.quota.callback.class = null
kafka  | kafka  | 	compression.gzip.level = -1
kafka  | kafka  | 	compression.lz4.level = 9
kafka  | kafka  | 	compression.type = producer
kafka  | kafka  | 	compression.zstd.level = 3
kafka  | kafka  | 	connection.failed.authentication.delay.ms = 100
kafka  | kafka  | 	connections.max.idle.ms = 600000
kafka  | kafka  | 	connections.max.reauth.ms = 0
kafka  | kafka  | 	control.plane.listener.name = null
kafka  | kafka  | 	controlled.shutdown.enable = true
kafka  | kafka  | 	controlled.shutdown.max.retries = 3
kafka  | kafka  | 	controlled.shutdown.retry.backoff.ms = 5000
kafka  | kafka  | 	controller.listener.names = CONTROLLER
kafka  | kafka  | 	controller.quorum.append.linger.ms = 25
kafka  | kafka  | 	controller.quorum.bootstrap.servers = []
kafka  | kafka  | 	controller.quorum.election.backoff.max.ms = 1000
kafka  | kafka  | 	controller.quorum.election.timeout.ms = 1000
kafka  | kafka  | 	controller.quorum.fetch.timeout.ms = 2000
kafka  | kafka  | 	controller.quorum.request.timeout.ms = 2000
kafka  | kafka  | 	controller.quorum.retry.backoff.ms = 20
kafka  | kafka  | 	controller.quorum.voters = [1@kafka:9093]
kafka  | kafka  | 	controller.quota.window.num = 11
kafka  | kafka  | 	controller.quota.window.size.seconds = 1
kafka  | kafka  | 	controller.socket.timeout.ms = 30000
kafka  | kafka  | 	create.topic.policy.class.name = null
kafka  | kafka  | 	default.replication.factor = 1
kafka  | kafka  | 	delegation.token.expiry.check.interval.ms = 3600000
kafka  | kafka  | 	delegation.token.expiry.time.ms = 86400000
kafka  | kafka  | 	delegation.token.master.key = null
kafka  | kafka  | 	delegation.token.max.lifetime.ms = 604800000
kafka  | kafka  | 	delegation.token.secret.key = null
kafka  | kafka  | 	delete.records.purgatory.purge.interval.requests = 1
kafka  | kafka  | 	delete.topic.enable = true
kafka  | kafka  | 	early.start.listeners = null
kafka  | kafka  | 	eligible.leader.replicas.enable = false
kafka  | kafka  | 	fetch.max.bytes = 57671680
kafka  | kafka  | 	fetch.purgatory.purge.interval.requests = 1000
kafka  | kafka  | 	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
kafka  | kafka  | 	group.consumer.heartbeat.interval.ms = 5000
kafka  | kafka  | 	group.consumer.max.heartbeat.interval.ms = 15000
kafka  | kafka  | 	group.consumer.max.session.timeout.ms = 60000
kafka  | kafka  | 	group.consumer.max.size = 2147483647
kafka  | kafka  | 	group.consumer.migration.policy = disabled
kafka  | kafka  | 	group.consumer.min.heartbeat.interval.ms = 5000
kafka  | kafka  | 	group.consumer.min.session.timeout.ms = 45000
kafka  | kafka  | 	group.consumer.session.timeout.ms = 45000
kafka  | kafka  | 	group.coordinator.append.linger.ms = 10
kafka  | kafka  | 	group.coordinator.new.enable = false
kafka  | kafka  | 	group.coordinator.rebalance.protocols = [classic]
kafka  | kafka  | 	group.coordinator.threads = 1
kafka  | kafka  | 	group.initial.rebalance.delay.ms = 0
kafka  | kafka  | 	group.max.session.timeout.ms = 1800000
kafka  | kafka  | 	group.max.size = 2147483647
kafka  | kafka  | 	group.min.session.timeout.ms = 6000
kafka  | kafka  | 	group.share.delivery.count.limit = 5
kafka  | kafka  | 	group.share.enable = false
kafka  | kafka  | 	group.share.heartbeat.interval.ms = 5000
kafka  | kafka  | 	group.share.max.groups = 10
kafka  | kafka  | 	group.share.max.heartbeat.interval.ms = 15000
kafka  | kafka  | 	group.share.max.record.lock.duration.ms = 60000
kafka  | kafka  | 	group.share.max.session.timeout.ms = 60000
kafka  | kafka  | 	group.share.max.size = 200
kafka  | kafka  | 	group.share.min.heartbeat.interval.ms = 5000
kafka  | kafka  | 	group.share.min.record.lock.duration.ms = 15000
kafka  | kafka  | 	group.share.min.session.timeout.ms = 45000
kafka  | kafka  | 	group.share.partition.max.record.locks = 200
kafka  | kafka  | 	group.share.record.lock.duration.ms = 30000
kafka  | kafka  | 	group.share.session.timeout.ms = 45000
kafka  | kafka  | 	initial.broker.registration.timeout.ms = 60000
kafka  | kafka  | 	inter.broker.listener.name = null
kafka  | kafka  | 	inter.broker.protocol.version = 3.9-IV0
kafka  | kafka  | 	kafka.metrics.polling.interval.secs = 10
kafka  | kafka  | 	kafka.metrics.reporters = []
kafka  | kafka  | 	leader.imbalance.check.interval.seconds = 300
kafka  | kafka  | 	leader.imbalance.per.broker.percentage = 10
kafka  | kafka  | 	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
kafka  | kafka  | 	listeners = PLAINTEXT://kafka:9092,CONTROLLER://kafka:9093
kafka  | kafka  | 	log.cleaner.backoff.ms = 15000
kafka  | kafka  | 	log.cleaner.dedupe.buffer.size = 134217728
kafka  | kafka  | 	log.cleaner.delete.retention.ms = 86400000
kafka  | kafka  | 	log.cleaner.enable = true
kafka  | kafka  | 	log.cleaner.io.buffer.load.factor = 0.9
kafka  | kafka  | 	log.cleaner.io.buffer.size = 524288
kafka  | kafka  | 	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
kafka  | kafka  | 	log.cleaner.max.compaction.lag.ms = 9223372036854775807
kafka  | kafka  | 	log.cleaner.min.cleanable.ratio = 0.5
kafka  | kafka  | 	log.cleaner.min.compaction.lag.ms = 0
kafka  | kafka  | 	log.cleaner.threads = 1
kafka  | kafka  | 	log.cleanup.policy = [delete]
kafka  | kafka  | 	log.dir = /tmp/kafka-logs
kafka  | kafka  | 	log.dir.failure.timeout.ms = 30000
kafka  | kafka  | 	log.dirs = null
kafka  | kafka  | 	log.flush.interval.messages = 9223372036854775807
kafka  | kafka  | 	log.flush.interval.ms = null
kafka  | kafka  | 	log.flush.offset.checkpoint.interval.ms = 60000
kafka  | kafka  | 	log.flush.scheduler.interval.ms = 9223372036854775807
kafka  | kafka  | 	log.flush.start.offset.checkpoint.interval.ms = 60000
kafka  | kafka  | 	log.index.interval.bytes = 4096
kafka  | kafka  | 	log.index.size.max.bytes = 10485760
kafka  | kafka  | 	log.initial.task.delay.ms = 30000
kafka  | kafka  | 	log.local.retention.bytes = -2
kafka  | kafka  | 	log.local.retention.ms = -2
kafka  | kafka  | 	log.message.downconversion.enable = true
kafka  | kafka  | 	log.message.format.version = 3.0-IV1
kafka  | kafka  | 	log.message.timestamp.after.max.ms = 9223372036854775807
kafka  | kafka  | 	log.message.timestamp.before.max.ms = 9223372036854775807
kafka  | kafka  | 	log.message.timestamp.difference.max.ms = 9223372036854775807
kafka  | kafka  | 	log.message.timestamp.type = CreateTime
kafka  | kafka  | 	log.preallocate = false
kafka  | kafka  | 	log.retention.bytes = -1
kafka  | kafka  | 	log.retention.check.interval.ms = 300000
kafka  | kafka  | 	log.retention.hours = 168
kafka  | kafka  | 	log.retention.minutes = null
kafka  | kafka  | 	log.retention.ms = null
kafka  | kafka  | 	log.roll.hours = 168
kafka  | kafka  | 	log.roll.jitter.hours = 0
kafka  | kafka  | 	log.roll.jitter.ms = null
kafka  | kafka  | 	log.roll.ms = null
kafka  | kafka  | 	log.segment.bytes = 1073741824
kafka  | kafka  | 	log.segment.delete.delay.ms = 60000
kafka  | kafka  | 	max.connection.creation.rate = 2147483647
kafka  | kafka  | 	max.connections = 2147483647
kafka  | kafka  | 	max.connections.per.ip = 2147483647
kafka  | kafka  | 	max.connections.per.ip.overrides = 
kafka  | kafka  | 	max.incremental.fetch.session.cache.slots = 1000
kafka  | kafka  | 	max.request.partition.size.limit = 2000
kafka  | kafka  | 	message.max.bytes = 1048588
kafka  | kafka  | 	metadata.log.dir = null
kafka  | kafka  | 	metadata.log.max.record.bytes.between.snapshots = 2800
kafka  | kafka  | 	metadata.log.max.snapshot.interval.ms = 3600000
kafka  | kafka  | 	metadata.log.segment.bytes = 1073741824
kafka  | kafka  | 	metadata.log.segment.min.bytes = 8388608
kafka  | kafka  | 	metadata.log.segment.ms = 15000
kafka  | kafka  | 	metadata.max.idle.interval.ms = 500
kafka  | kafka  | 	metadata.max.retention.bytes = 104857600
kafka  | kafka  | 	metadata.max.retention.ms = 60000
kafka  | kafka  | 	metric.reporters = []
kafka  | kafka  | 	metrics.num.samples = 2
kafka  | kafka  | 	metrics.recording.level = INFO
kafka  | kafka  | 	metrics.sample.window.ms = 30000
kafka  | kafka  | 	min.insync.replicas = 1
kafka  | kafka  | 	node.id = 1
kafka  | kafka  | 	num.io.threads = 8
kafka  | kafka  | 	num.network.threads = 3
kafka  | kafka  | 	num.partitions = 1
kafka  | kafka  | 	num.recovery.threads.per.data.dir = 1
kafka  | kafka  | 	num.replica.alter.log.dirs.threads = null
kafka  | kafka  | 	num.replica.fetchers = 1
kafka  | kafka  | 	offset.metadata.max.bytes = 4096
kafka  | kafka  | 	offsets.commit.required.acks = -1
kafka  | kafka  | 	offsets.commit.timeout.ms = 5000
kafka  | kafka  | 	offsets.load.buffer.size = 5242880
kafka  | kafka  | 	offsets.retention.check.interval.ms = 600000
kafka  | kafka  | 	offsets.retention.minutes = 10080
kafka  | kafka  | 	offsets.topic.compression.codec = 0
kafka  | kafka  | 	offsets.topic.num.partitions = 50
kafka  | kafka  | 	offsets.topic.replication.factor = 1
kafka  | kafka  | 	offsets.topic.segment.bytes = 104857600
kafka  | kafka  | 	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
kafka  | kafka  | 	password.encoder.iterations = 4096
kafka  | kafka  | 	password.encoder.key.length = 128
kafka  | kafka  | 	password.encoder.keyfactory.algorithm = null
kafka  | kafka  | 	password.encoder.old.secret = null
kafka  | kafka  | 	password.encoder.secret = null
kafka  | kafka  | 	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
kafka  | kafka  | 	process.roles = [controller, broker]
kafka  | kafka  | 	producer.id.expiration.check.interval.ms = 600000
kafka  | kafka  | 	producer.id.expiration.ms = 86400000
kafka  | kafka  | 	producer.purgatory.purge.interval.requests = 1000
kafka  | kafka  | 	queued.max.request.bytes = -1
kafka  | kafka  | 	queued.max.requests = 500
kafka  | kafka  | 	quota.window.num = 11
kafka  | kafka  | 	quota.window.size.seconds = 1
kafka  | kafka  | 	remote.fetch.max.wait.ms = 500
kafka  | kafka  | 	remote.log.index.file.cache.total.size.bytes = 1073741824
kafka  | kafka  | 	remote.log.manager.copier.thread.pool.size = -1
kafka  | kafka  | 	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
kafka  | kafka  | 	remote.log.manager.copy.quota.window.num = 11
kafka  | kafka  | 	remote.log.manager.copy.quota.window.size.seconds = 1
kafka  | kafka  | 	remote.log.manager.expiration.thread.pool.size = -1
kafka  | kafka  | 	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
kafka  | kafka  | 	remote.log.manager.fetch.quota.window.num = 11
kafka  | kafka  | 	remote.log.manager.fetch.quota.window.size.seconds = 1
kafka  | kafka  | 	remote.log.manager.task.interval.ms = 30000
kafka  | kafka  | 	remote.log.manager.task.retry.backoff.max.ms = 30000
kafka  | kafka  | 	remote.log.manager.task.retry.backoff.ms = 500
kafka  | kafka  | 	remote.log.manager.task.retry.jitter = 0.2
kafka  | kafka  | 	remote.log.manager.thread.pool.size = 10
kafka  | kafka  | 	remote.log.metadata.custom.metadata.max.bytes = 128
kafka  | kafka  | 	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
kafka  | kafka  | 	remote.log.metadata.manager.class.path = null
kafka  | kafka  | 	remote.log.metadata.manager.impl.prefix = rlmm.config.
kafka  | kafka  | 	remote.log.metadata.manager.listener.name = null
kafka  | kafka  | 	remote.log.reader.max.pending.tasks = 100
kafka  | kafka  | 	remote.log.reader.threads = 10
kafka  | kafka  | 	remote.log.storage.manager.class.name = null
kafka  | kafka  | 	remote.log.storage.manager.class.path = null
kafka  | kafka  | 	remote.log.storage.manager.impl.prefix = rsm.config.
kafka  | kafka  | 	remote.log.storage.system.enable = false
kafka  | kafka  | 	replica.fetch.backoff.ms = 1000
kafka  | kafka  | 	replica.fetch.max.bytes = 1048576
kafka  | kafka  | 	replica.fetch.min.bytes = 1
kafka  | kafka  | 	replica.fetch.response.max.bytes = 10485760
kafka  | kafka  | 	replica.fetch.wait.max.ms = 500
kafka  | kafka  | 	replica.high.watermark.checkpoint.interval.ms = 5000
kafka  | kafka  | 	replica.lag.time.max.ms = 30000
kafka  | kafka  | 	replica.selector.class = null
kafka  | kafka  | 	replica.socket.receive.buffer.bytes = 65536
kafka  | kafka  | 	replica.socket.timeout.ms = 30000
kafka  | kafka  | 	replication.quota.window.num = 11
kafka  | kafka  | 	replication.quota.window.size.seconds = 1
kafka  | kafka  | 	request.timeout.ms = 30000
kafka  | kafka  | 	reserved.broker.max.id = 1000
kafka  | kafka  | 	sasl.client.callback.handler.class = null
kafka  | kafka  | 	sasl.enabled.mechanisms = [GSSAPI]
kafka  | kafka  | 	sasl.jaas.config = null
kafka  | kafka  | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
kafka  | kafka  | 	sasl.kerberos.min.time.before.relogin = 60000
kafka  | kafka  | 	sasl.kerberos.principal.to.local.rules = [DEFAULT]
kafka  | kafka  | 	sasl.kerberos.service.name = null
kafka  | kafka  | 	sasl.kerberos.ticket.renew.jitter = 0.05
kafka  | kafka  | 	sasl.kerberos.ticket.renew.window.factor = 0.8
kafka  | kafka  | 	sasl.login.callback.handler.class = null
kafka  | kafka  | 	sasl.login.class = null
kafka  | kafka  | 	sasl.login.connect.timeout.ms = null
kafka  | kafka  | 	sasl.login.read.timeout.ms = null
kafka  | kafka  | 	sasl.login.refresh.buffer.seconds = 300
kafka  | kafka  | 	sasl.login.refresh.min.period.seconds = 60
kafka  | kafka  | 	sasl.login.refresh.window.factor = 0.8
kafka  | kafka  | 	sasl.login.refresh.window.jitter = 0.05
kafka  | kafka  | 	sasl.login.retry.backoff.max.ms = 10000
kafka  | kafka  | 	sasl.login.retry.backoff.ms = 100
kafka  | kafka  | 	sasl.mechanism.controller.protocol = GSSAPI
kafka  | kafka  | 	sasl.mechanism.inter.broker.protocol = GSSAPI
kafka  | kafka  | 	sasl.oauthbearer.clock.skew.seconds = 30
kafka  | kafka  | 	sasl.oauthbearer.expected.audience = null
kafka  | kafka  | 	sasl.oauthbearer.expected.issuer = null
kafka  | kafka  | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
kafka  | kafka  | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
kafka  | kafka  | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
kafka  | kafka  | 	sasl.oauthbearer.jwks.endpoint.url = null
kafka  | kafka  | 	sasl.oauthbearer.scope.claim.name = scope
kafka  | kafka  | 	sasl.oauthbearer.sub.claim.name = sub
kafka  | kafka  | 	sasl.oauthbearer.token.endpoint.url = null
kafka  | kafka  | 	sasl.server.callback.handler.class = null
kafka  | kafka  | 	sasl.server.max.receive.size = 524288
kafka  | kafka  | 	security.inter.broker.protocol = PLAINTEXT
kafka  | kafka  | 	security.providers = null
kafka  | kafka  | 	server.max.startup.time.ms = 9223372036854775807
kafka  | kafka  | 	socket.connection.setup.timeout.max.ms = 30000
kafka  | kafka  | 	socket.connection.setup.timeout.ms = 10000
kafka  | kafka  | 	socket.listen.backlog.size = 50
kafka  | kafka  | 	socket.receive.buffer.bytes = 102400
kafka  | kafka  | 	socket.request.max.bytes = 104857600
kafka  | kafka  | 	socket.send.buffer.bytes = 102400
kafka  | kafka  | 	ssl.allow.dn.changes = false
kafka  | kafka  | 	ssl.allow.san.changes = false
kafka  | kafka  | 	ssl.cipher.suites = []
kafka  | kafka  | 	ssl.client.auth = none
kafka  | kafka  | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
kafka  | kafka  | 	ssl.endpoint.identification.algorithm = https
kafka  | kafka  | 	ssl.engine.factory.class = null
kafka  | kafka  | 	ssl.key.password = null
kafka  | kafka  | 	ssl.keymanager.algorithm = SunX509
kafka  | kafka  | 	ssl.keystore.certificate.chain = null
kafka  | kafka  | 	ssl.keystore.key = null
kafka  | kafka  | 	ssl.keystore.location = null
kafka  | kafka  | 	ssl.keystore.password = null
kafka  | kafka  | 	ssl.keystore.type = JKS
kafka  | kafka  | 	ssl.principal.mapping.rules = DEFAULT
kafka  | kafka  | 	ssl.protocol = TLSv1.3
kafka  | kafka  | 	ssl.provider = null
kafka  | kafka  | 	ssl.secure.random.implementation = null
kafka  | kafka  | 	ssl.trustmanager.algorithm = PKIX
kafka  | kafka  | 	ssl.truststore.certificates = null
kafka  | kafka  | 	ssl.truststore.location = null
kafka  | kafka  | 	ssl.truststore.password = null
kafka  | kafka  | 	ssl.truststore.type = JKS
kafka  | kafka  | 	telemetry.max.bytes = 1048576
kafka  | kafka  | 	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
kafka  | kafka  | 	transaction.max.timeout.ms = 900000
kafka  | kafka  | 	transaction.partition.verification.enable = true
kafka  | kafka  | 	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
kafka  | kafka  | 	transaction.state.log.load.buffer.size = 5242880
kafka  | kafka  | 	transaction.state.log.min.isr = 2
kafka  | kafka  | 	transaction.state.log.num.partitions = 50
kafka  | kafka  | 	transaction.state.log.replication.factor = 1
kafka  | kafka  | 	transaction.state.log.segment.bytes = 104857600
kafka  | kafka  | 	transactional.id.expiration.ms = 604800000
kafka  | kafka  | 	unclean.leader.election.enable = false
kafka  | kafka  | 	unclean.leader.election.interval.ms = 300000
kafka  | kafka  | 	unstable.api.versions.enable = false
kafka  | kafka  | 	unstable.feature.versions.enable = false
kafka  | kafka  | 	zookeeper.clientCnxnSocket = null
kafka  | kafka  | 	zookeeper.connect = null
kafka  | kafka  | 	zookeeper.connection.timeout.ms = null
kafka  | kafka  | 	zookeeper.max.in.flight.requests = 10
kafka  | kafka  | 	zookeeper.metadata.migration.enable = false
kafka  | kafka  | 	zookeeper.metadata.migration.min.batch.size = 200
kafka  | kafka  | 	zookeeper.session.timeout.ms = 18000
kafka  | kafka  | 	zookeeper.set.acl = false
kafka  | kafka  | 	zookeeper.ssl.cipher.suites = null
kafka  | kafka  | 	zookeeper.ssl.client.enable = false
kafka  | kafka  | 	zookeeper.ssl.crl.enable = false
kafka  | kafka  | 	zookeeper.ssl.enabled.protocols = null
kafka  | kafka  | 	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
kafka  | kafka  | 	zookeeper.ssl.keystore.location = null
kafka  | kafka  | 	zookeeper.ssl.keystore.password = null
kafka  | kafka  | 	zookeeper.ssl.keystore.type = null
kafka  | kafka  | 	zookeeper.ssl.ocsp.enable = false
kafka  | kafka  | 	zookeeper.ssl.protocol = TLSv1.2
kafka  | kafka  | 	zookeeper.ssl.truststore.location = null
kafka  | kafka  | 	zookeeper.ssl.truststore.password = null
kafka  | kafka  | 	zookeeper.ssl.truststore.type = null
kafka  | kafka  |  (kafka.server.KafkaConfig)
kafka  | kafka  | [2026-02-07 05:02:21,563] INFO [BrokerServer id=1] Waiting for the broker to be unfenced (kafka.server.BrokerServer)
kafka  | kafka  | [2026-02-07 05:02:21,596] INFO [BrokerLifecycleManager id=1] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)
kafka  | kafka  | [2026-02-07 05:02:21,597] INFO [BrokerServer id=1] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)
kafka  | kafka  | [2026-02-07 05:02:21,597] INFO authorizerStart completed for endpoint PLAINTEXT. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
kafka  | kafka  | [2026-02-07 05:02:21,597] INFO [SocketServer listenerType=BROKER, nodeId=1] Enabling request processing. (kafka.network.SocketServer)
kafka  | kafka  | [2026-02-07 05:02:21,597] INFO Awaiting socket connections on kafka:9092. (kafka.network.DataPlaneAcceptor)
kafka  | kafka  | [2026-02-07 05:02:21,598] INFO [BrokerServer id=1] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
kafka  | kafka  | [2026-02-07 05:02:21,598] INFO [BrokerServer id=1] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
kafka  | kafka  | [2026-02-07 05:02:21,598] INFO [BrokerServer id=1] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
kafka  | kafka  | [2026-02-07 05:02:21,598] INFO [BrokerServer id=1] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
kafka  | kafka  | [2026-02-07 05:02:21,598] INFO [BrokerServer id=1] Transition from STARTING to STARTED (kafka.server.BrokerServer)
kafka  | kafka  | [2026-02-07 05:02:21,598] INFO Kafka version: 3.9.1 (org.apache.kafka.common.utils.AppInfoParser)
kafka  | kafka  | [2026-02-07 05:02:21,598] INFO Kafka commitId: f745dfdcee2b9851 (org.apache.kafka.common.utils.AppInfoParser)
kafka  | kafka  | [2026-02-07 05:02:21,598] INFO Kafka startTimeMs: 1770440541598 (org.apache.kafka.common.utils.AppInfoParser)
kafka  | kafka  | [2026-02-07 05:02:21,599] INFO [KafkaRaftServer nodeId=1] Kafka Server started (kafka.server.KafkaRaftServer)
kafka  | kafka  | [2026-02-07 05:02:21,692] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group accounting in Stable state. Created a new member id rdkafka-30cafd20-9e22-41a6-b60e-b2a801533073 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
kafka  | kafka  | [2026-02-07 05:02:21,696] INFO [GroupCoordinator 1]: Preparing to rebalance group accounting in state PreparingRebalance with old generation 8 (__consumer_offsets-43) (reason: Adding new member rdkafka-30cafd20-9e22-41a6-b60e-b2a801533073 with group instance id None; client reason: not provided) (kafka.coordinator.group.GroupCoordinator)
kafka  | kafka  | [2026-02-07 05:02:22,634] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group fraud-detection in Stable state. Created a new member id consumer-fraud-detection-1-80aae0fb-8651-4d7b-8491-1050d338f578 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
kafka  | kafka  | [2026-02-07 05:02:22,636] INFO [GroupCoordinator 1]: Preparing to rebalance group fraud-detection in state PreparingRebalance with old generation 23 (__consumer_offsets-44) (reason: Adding new member consumer-fraud-detection-1-80aae0fb-8651-4d7b-8491-1050d338f578 with group instance id None; client reason: need to re-join with the given member-id: consumer-fraud-detection-1-80aae0fb-8651-4d7b-8491-1050d338f578) (kafka.coordinator.group.GroupCoordinator)
kafka  | kafka  | [2026-02-07 05:02:23,468] INFO [LocalLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Rolled new log segment at offset 3836551 in 2 ms. (kafka.log.LocalLog)
kafka  | kafka  | [2026-02-07 05:02:23,468] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 3836551 with 0 producer ids in 0 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
kafka  | kafka  | [2026-02-07 05:02:24,968] INFO [SnapshotGenerator id=1] Creating new KRaft snapshot file snapshot 00000000000003836555-0000000010 because we have replayed at least 2800 bytes. (org.apache.kafka.image.publisher.SnapshotGenerator)
kafka  | kafka  | [2026-02-07 05:02:25,001] INFO [SnapshotEmitter id=1] Successfully wrote snapshot 00000000000003836555-0000000010 (org.apache.kafka.image.publisher.SnapshotEmitter)
llm  | llm  | [2026-02-07 05:02:08,362] INFO in app: Received a chat completion request: '[{'role': 'system', 'content': "You are a helpful assistant that answers related to a specific product. Use tools as needed to fetch the product reviews and product information. Keep the response brief with no more than 1-2 sentences. If you don't know the answer, just say you don't know."}, {'role': 'user', 'content': 'Answer the following question about product ID:LS4PSXUNUM: Can you summarize the product reviews?'}]'
llm  | llm  | [2026-02-07 05:02:08,362] INFO in app: last_message is: 'Answer the following question about product ID:LS4PSXUNUM: Can you summarize the product reviews?'
llm  | llm  | [2026-02-07 05:02:08,362] INFO in app: Processing a tool call with args: '{"product_id": "LS4PSXUNUM"}'
llm  | llm  | [2026-02-07 05:02:08,362] INFO in app: The model is: astronomy-llm
llm  | llm  | 172.22.0.12 - - [07/Feb/2026 05:02:08] "POST /v1/chat/completions HTTP/1.1" 200 -
llm  | llm  | [2026-02-07 05:02:08,394] INFO in app: Received a chat completion request: '[{'role': 'system', 'content': "You are a helpful assistant that answers related to a specific product. Use tools as needed to fetch the product reviews and product information. Keep the response brief with no more than 1-2 sentences. If you don't know the answer, just say you don't know."}, {'role': 'user', 'content': 'Answer the following question about product ID:LS4PSXUNUM: Can you summarize the product reviews?'}, {'content': 'requesting a tool call', 'role': 'assistant', 'tool_calls': [{'id': 'call', 'function': {'arguments': '{"product_id": "LS4PSXUNUM"}', 'name': 'fetch_product_reviews'}, 'type': 'function'}]}, {'tool_call_id': 'call', 'role': 'tool', 'name': 'fetch_product_reviews', 'content': '[["night_walker", "The red light is perfect for preserving night vision during astronomy sessions. The hand warmer is an unexpected bonus. Very practical device.", 5.0], ["star_party_goer", "This flashlight is indispensable for star parties. The red mode is gentle on the eyes, and the power bank feature is super handy. Love it!", 4.5], ["camper_chris", "Rugged and versatile, this flashlight is great for camping and night walks. The hand warmer function is a game-changer on cold nights. Highly recommend.", 4.5], ["emergency_kit", "A fantastic multi-tool for my emergency kit. The red light is useful, and the power bank means I can charge my phone. Great design.", 4.0], ["astro_accessory", "Every astronomer needs one of these. The red light is essential, and the hand warmer and power bank make it incredibly useful. A top-tier accessory.", 5.0]]'}, {'role': 'user', 'content': 'Based on the tool results, answer the original question about product ID:LS4PSXUNUM. Keep the response brief with no more than 1-2 sentences.'}]'
llm  | llm  | [2026-02-07 05:02:08,394] INFO in app: last_message is: 'Based on the tool results, answer the original question about product ID:LS4PSXUNUM. Keep the response brief with no more than 1-2 sentences.'
llm  | llm  | [2026-02-07 05:02:08,399] INFO in app: llmInaccurateResponse feature flag: False
llm  | llm  | [2026-02-07 05:02:08,399] INFO in app: product_review_summary is: This 3-in-1 device is highly valued by users for its essential red flashlight mode, which preserves night vision during astronomy sessions and star parties. Its integrated hand warmer and portable power bank features make it an incredibly versatile, rugged, and practical multi-tool for various outdoor activities and emergencies.
llm  | llm  | [2026-02-07 05:02:08,399] INFO in app: Processing a response: 'This 3-in-1 device is highly valued by users for its essential red flashlight mode, which preserves night vision during astronomy sessions and star parties. Its integrated hand warmer and portable power bank features make it an incredibly versatile, rugged, and practical multi-tool for various outdoor activities and emergencies.'
llm  | llm  | 172.22.0.12 - - [07/Feb/2026 05:02:08] "POST /v1/chat/completions HTTP/1.1" 200 -
opensearch  | opensearch  | WARNING: A restricted method in java.lang.System has been called
opensearch  | opensearch  | WARNING: java.lang.System::load has been called by com.sun.jna.Native in an unnamed module (file:/usr/share/opensearch/lib/jna-5.16.0.jar)
opensearch  | opensearch  | WARNING: Use --enable-native-access=ALL-UNNAMED to avoid a warning for callers in this module
opensearch  | opensearch  | WARNING: Restricted methods will be blocked in a future release unless native access is enabled
opensearch  | opensearch  | 
opensearch  | opensearch  | Disabling OpenSearch Security Plugin
opensearch  | opensearch  | OpenSearch Performance Analyzer Plugin does not exist, disable by default
opensearch  | opensearch  | WARNING: Using incubator modules: jdk.incubator.vector
opensearch  | opensearch  | WARNING: Unknown module: org.apache.arrow.memory.core specified to --add-opens
opensearch  | opensearch  | WARNING: A terminally deprecated method in sun.misc.Unsafe has been called
opensearch  | opensearch  | WARNING: sun.misc.Unsafe::objectFieldOffset has been called by net.bytebuddy.dynamic.loading.ClassInjector$UsingUnsafe$Dispatcher$CreationAction
opensearch  | opensearch  | WARNING: Please consider reporting this to the maintainers of class net.bytebuddy.dynamic.loading.ClassInjector$UsingUnsafe$Dispatcher$CreationAction
opensearch  | opensearch  | WARNING: sun.misc.Unsafe::objectFieldOffset will be removed in a future release
opensearch  | opensearch  | WARNING: A restricted method in java.lang.System has been called
opensearch  | opensearch  | WARNING: java.lang.System::load has been called by com.sun.jna.Native in an unnamed module (file:/usr/share/opensearch/lib/jna-5.16.0.jar)
opensearch  | opensearch  | WARNING: Use --enable-native-access=ALL-UNNAMED to avoid a warning for callers in this module
opensearch  | opensearch  | WARNING: Restricted methods will be blocked in a future release unless native access is enabled
opensearch  | opensearch  | 
opensearch  | opensearch  | [2026-02-07T05:02:07,193][WARN ][stderr                   ] [demo-node] Feb 07, 2026 5:02:07 AM org.opensearch.javaagent.bootstrap.AgentPolicy setPolicy
opensearch  | opensearch  | [2026-02-07T05:02:07,195][WARN ][stderr                   ] [demo-node] INFO: Policy attached successfully: org.opensearch.bootstrap.OpenSearchPolicy@3f9270ed
opensearch  | opensearch  | [2026-02-07T05:02:07,205][INFO ][o.o.n.Node               ] [demo-node] version[3.2.0], pid[1], build[tar/6adc0bf476e1624190564d7fbe4aba00ccf49ad8/2025-08-12T03:55:01.226522683Z], OS[Linux/6.16.8-200.fc42.x86_64/amd64], JVM[Eclipse Adoptium/OpenJDK 64-Bit Server VM/24.0.2/24.0.2+12]
opensearch  | opensearch  | [2026-02-07T05:02:07,205][INFO ][o.o.n.Node               ] [demo-node] JVM home [/usr/share/opensearch/jdk], using bundled JDK/JRE [true]
opensearch  | opensearch  | [2026-02-07T05:02:07,205][INFO ][o.o.n.Node               ] [demo-node] JVM arguments [-Xshare:auto, -Dopensearch.networkaddress.cache.ttl=60, -Dopensearch.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Djava.locale.providers=SPI,CLDR, -Xms1g, -Xmx1g, -XX:+UseG1GC, -XX:G1ReservePercent=25, -XX:InitiatingHeapOccupancyPercent=30, -Djava.io.tmpdir=/tmp/opensearch-8792245792435927115, -XX:+HeapDumpOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, --add-modules=jdk.incubator.vector, -javaagent:agent/opensearch-agent.jar, --add-opens=java.base/java.nio=org.apache.arrow.memory.core,ALL-UNNAMED, -Dclk.tck=100, -Djdk.attach.allowAttachSelf=true, -Djava.security.policy=/usr/share/opensearch/config/opensearch-performance-analyzer/opensearch_security.policy, --add-opens=jdk.attach/sun.tools.attach=ALL-UNNAMED, -Dopensearch.cgroups.hierarchy.override=/, -Xms400m, -Xmx400m, -XX:MaxDirectMemorySize=209715200, -Dopensearch.path.home=/usr/share/opensearch, -Dopensearch.path.conf=/usr/share/opensearch/config, -Dopensearch.distribution.type=tar, -Dopensearch.bundled_jdk=true]
opensearch  | opensearch  | [2026-02-07T05:02:07,329][WARN ][stderr                   ] [demo-node] Feb 07, 2026 5:02:07 AM org.apache.lucene.internal.vectorization.PanamaVectorizationProvider <init>
opensearch  | opensearch  | [2026-02-07T05:02:07,330][WARN ][stderr                   ] [demo-node] INFO: Java vector incubator API enabled; uses preferredBitSize=256; FMA enabled
opensearch  | opensearch  | [2026-02-07T05:02:07,942][INFO ][o.o.s.s.t.SSLConfig      ] [demo-node] SSL dual mode is disabled
opensearch  | opensearch  | [2026-02-07T05:02:07,944][WARN ][o.o.s.OpenSearchSecurityPlugin] [demo-node] OpenSearch Security plugin installed but disabled. This can expose your configuration (including passwords) to the public.
opensearch  | opensearch  | [2026-02-07T05:02:07,996][INFO ][o.o.i.r.ReindexModulePlugin] [demo-node] ReindexPlugin reloadSPI called
opensearch  | opensearch  | [2026-02-07T05:02:07,997][INFO ][o.o.i.r.ReindexModulePlugin] [demo-node] Unable to find any implementation for RemoteReindexExtension
opensearch  | opensearch  | [2026-02-07T05:02:08,003][INFO ][o.o.j.JobSchedulerPlugin ] [demo-node] Loaded scheduler extension: opendistro-index-management, index: .opendistro-ism-config
opensearch  | opensearch  | [2026-02-07T05:02:08,005][INFO ][o.o.j.JobSchedulerPlugin ] [demo-node] Loaded scheduler extension: async-query-scheduler, index: .async-query-scheduler
opensearch  | opensearch  | [2026-02-07T05:02:08,013][INFO ][o.o.p.PluginsService     ] [demo-node] loaded module [aggs-matrix-stats]
opensearch  | opensearch  | [2026-02-07T05:02:08,014][INFO ][o.o.p.PluginsService     ] [demo-node] loaded module [analysis-common]
opensearch  | opensearch  | [2026-02-07T05:02:08,014][INFO ][o.o.p.PluginsService     ] [demo-node] loaded module [cache-common]
opensearch  | opensearch  | [2026-02-07T05:02:08,014][INFO ][o.o.p.PluginsService     ] [demo-node] loaded module [geo]
opensearch  | opensearch  | [2026-02-07T05:02:08,014][INFO ][o.o.p.PluginsService     ] [demo-node] loaded module [ingest-common]
opensearch  | opensearch  | [2026-02-07T05:02:08,014][INFO ][o.o.p.PluginsService     ] [demo-node] loaded module [ingest-geoip]
opensearch  | opensearch  | [2026-02-07T05:02:08,014][INFO ][o.o.p.PluginsService     ] [demo-node] loaded module [ingest-user-agent]
opensearch  | opensearch  | [2026-02-07T05:02:08,014][INFO ][o.o.p.PluginsService     ] [demo-node] loaded module [lang-expression]
opensearch  | opensearch  | [2026-02-07T05:02:08,014][INFO ][o.o.p.PluginsService     ] [demo-node] loaded module [lang-mustache]
opensearch  | opensearch  | [2026-02-07T05:02:08,014][INFO ][o.o.p.PluginsService     ] [demo-node] loaded module [lang-painless]
opensearch  | opensearch  | [2026-02-07T05:02:08,014][INFO ][o.o.p.PluginsService     ] [demo-node] loaded module [mapper-extras]
opensearch  | opensearch  | [2026-02-07T05:02:08,014][INFO ][o.o.p.PluginsService     ] [demo-node] loaded module [opensearch-dashboards]
opensearch  | opensearch  | [2026-02-07T05:02:08,014][INFO ][o.o.p.PluginsService     ] [demo-node] loaded module [parent-join]
opensearch  | opensearch  | [2026-02-07T05:02:08,014][INFO ][o.o.p.PluginsService     ] [demo-node] loaded module [percolator]
opensearch  | opensearch  | [2026-02-07T05:02:08,014][INFO ][o.o.p.PluginsService     ] [demo-node] loaded module [rank-eval]
opensearch  | opensearch  | [2026-02-07T05:02:08,014][INFO ][o.o.p.PluginsService     ] [demo-node] loaded module [reindex]
opensearch  | opensearch  | [2026-02-07T05:02:08,015][INFO ][o.o.p.PluginsService     ] [demo-node] loaded module [repository-url]
opensearch  | opensearch  | [2026-02-07T05:02:08,015][INFO ][o.o.p.PluginsService     ] [demo-node] loaded module [rule-framework]
opensearch  | opensearch  | [2026-02-07T05:02:08,015][INFO ][o.o.p.PluginsService     ] [demo-node] loaded module [search-pipeline-common]
opensearch  | opensearch  | [2026-02-07T05:02:08,015][INFO ][o.o.p.PluginsService     ] [demo-node] loaded module [systemd]
opensearch  | opensearch  | [2026-02-07T05:02:08,015][INFO ][o.o.p.PluginsService     ] [demo-node] loaded module [transport-grpc]
opensearch  | opensearch  | [2026-02-07T05:02:08,015][INFO ][o.o.p.PluginsService     ] [demo-node] loaded module [transport-netty4]
opensearch  | opensearch  | [2026-02-07T05:02:08,015][INFO ][o.o.p.PluginsService     ] [demo-node] loaded plugin [opensearch-index-management]
opensearch  | opensearch  | [2026-02-07T05:02:08,015][INFO ][o.o.p.PluginsService     ] [demo-node] loaded plugin [opensearch-job-scheduler]
opensearch  | opensearch  | [2026-02-07T05:02:08,015][INFO ][o.o.p.PluginsService     ] [demo-node] loaded plugin [opensearch-security]
opensearch  | opensearch  | [2026-02-07T05:02:08,015][INFO ][o.o.p.PluginsService     ] [demo-node] loaded plugin [opensearch-sql]
opensearch  | opensearch  | [2026-02-07T05:02:08,096][INFO ][o.o.e.NodeEnvironment    ] [demo-node] using [1] data paths, mounts [[/ (overlay)]], net usable_space [1.2tb], net total_space [1.8tb], types [overlay]
opensearch  | opensearch  | [2026-02-07T05:02:08,096][INFO ][o.o.e.NodeEnvironment    ] [demo-node] heap size [400mb], compressed ordinary object pointers [true]
opensearch  | opensearch  | [2026-02-07T05:02:08,214][INFO ][o.o.n.Node               ] [demo-node] node name [demo-node], node ID [Kd5AhdqOQ2WwXiCk4k-KmA], cluster name [demo-cluster], roles [ingest, remote_cluster_client, data, cluster_manager]
opensearch  | opensearch  | [2026-02-07T05:02:08,237][INFO ][o.o.e.ExtensionsManager  ] [demo-node] ExtensionsManager initialized
opensearch  | opensearch  | [2026-02-07T05:02:09,555][WARN ][stderr                   ] [demo-node] SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
opensearch  | opensearch  | [2026-02-07T05:02:09,556][WARN ][stderr                   ] [demo-node] SLF4J: Defaulting to no-operation (NOP) logger implementation
opensearch  | opensearch  | [2026-02-07T05:02:09,556][WARN ][stderr                   ] [demo-node] SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
opensearch  | opensearch  | [2026-02-07T05:02:11,193][INFO ][o.o.t.g.p.r.s.q.QueryBuilderProtoConverterRegistry] [demo-node] Registered 4 built-in query converters
opensearch  | opensearch  | [2026-02-07T05:02:11,194][INFO ][o.o.t.g.p.r.s.q.QueryBuilderProtoConverterRegistry] [demo-node] Loaded 0 external query converters (0 failed)
opensearch  | opensearch  | [2026-02-07T05:02:11,249][WARN ][o.o.s.p.SQLPlugin        ] [demo-node] Master key is a required config for using create and update datasource APIs. Please set plugins.query.datasources.encryption.masterkey config in opensearch.yml in all the cluster nodes. More details can be found here: https://github.com/opensearch-project/sql/blob/main/docs/user/ppl/admin/datasources.rst#master-key-config-for-encrypting-credential-information
opensearch  | opensearch  | [2026-02-07T05:02:11,445][INFO ][o.o.s.e.f.PPLFuncImpTable] [demo-node] Cannot create type checker for function: ARRAY. Will skip its type checking
opensearch  | opensearch  | [2026-02-07T05:02:11,445][INFO ][o.o.s.e.f.PPLFuncImpTable] [demo-node] Cannot create type checker for function: FORALL. Will skip its type checking
opensearch  | opensearch  | [2026-02-07T05:02:11,445][INFO ][o.o.s.e.f.PPLFuncImpTable] [demo-node] Cannot create type checker for function: EXISTS. Will skip its type checking
opensearch  | opensearch  | [2026-02-07T05:02:11,445][INFO ][o.o.s.e.f.PPLFuncImpTable] [demo-node] Cannot create type checker for function: FILTER. Will skip its type checking
opensearch  | opensearch  | [2026-02-07T05:02:11,445][INFO ][o.o.s.e.f.PPLFuncImpTable] [demo-node] Cannot create type checker for function: TRANSFORM. Will skip its type checking
opensearch  | opensearch  | [2026-02-07T05:02:11,445][INFO ][o.o.s.e.f.PPLFuncImpTable] [demo-node] Cannot create type checker for function: REDUCE. Will skip its type checking
opensearch  | opensearch  | [2026-02-07T05:02:11,445][INFO ][o.o.s.e.f.PPLFuncImpTable] [demo-node] Cannot create type checker for function: JSON. Will skip its type checking
opensearch  | opensearch  | [2026-02-07T05:02:11,445][INFO ][o.o.s.e.f.PPLFuncImpTable] [demo-node] Cannot create type checker for function: JSON_EXTRACT. Will skip its type checking
opensearch  | opensearch  | [2026-02-07T05:02:11,445][INFO ][o.o.s.e.f.PPLFuncImpTable] [demo-node] Cannot create type checker for function: JSON_KEYS. Will skip its type checking
opensearch  | opensearch  | [2026-02-07T05:02:11,446][INFO ][o.o.s.e.f.PPLFuncImpTable] [demo-node] Cannot create type checker for function: JSON_SET. Will skip its type checking
opensearch  | opensearch  | [2026-02-07T05:02:11,446][INFO ][o.o.s.e.f.PPLFuncImpTable] [demo-node] Cannot create type checker for function: JSON_DELETE. Will skip its type checking
opensearch  | opensearch  | [2026-02-07T05:02:11,446][INFO ][o.o.s.e.f.PPLFuncImpTable] [demo-node] Cannot create type checker for function: JSON_APPEND. Will skip its type checking
opensearch  | opensearch  | [2026-02-07T05:02:11,446][INFO ][o.o.s.e.f.PPLFuncImpTable] [demo-node] Cannot create type checker for function: JSON_EXTEND. Will skip its type checking
opensearch  | opensearch  | [2026-02-07T05:02:11,900][INFO ][o.o.t.NettyAllocator     ] [demo-node] creating NettyAllocator with the following configs: [name=unpooled, suggested_max_allocation_size=256kb, factors={opensearch.unsafe.use_unpooled_allocator=null, g1gc_enabled=true, g1gc_region_size=1mb, heap_size=400mb}]
opensearch  | opensearch  | [2026-02-07T05:02:11,974][INFO ][o.o.d.DiscoveryModule    ] [demo-node] using discovery type [single-node] and seed hosts providers [settings]
opensearch  | opensearch  | [2026-02-07T05:02:12,303][WARN ][o.o.g.DanglingIndicesState] [demo-node] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
opensearch  | opensearch  | [2026-02-07T05:02:12,571][INFO ][o.o.n.Node               ] [demo-node] initialized
opensearch  | opensearch  | [2026-02-07T05:02:12,571][INFO ][o.o.n.Node               ] [demo-node] starting ...
opensearch  | opensearch  | [2026-02-07T05:02:12,688][INFO ][o.o.t.TransportService   ] [demo-node] publish_address {172.22.0.27:9300}, bound_addresses {[::]:9300}
opensearch  | opensearch  | [2026-02-07T05:02:12,689][INFO ][o.o.t.TransportService   ] [demo-node] Remote clusters initialized successfully.
opensearch  | opensearch  | [2026-02-07T05:02:12,905][INFO ][o.o.c.c.Coordinator      ] [demo-node] cluster UUID [G_lsAJt3TaeCDfVdbABpFQ]
opensearch  | opensearch  | [2026-02-07T05:02:12,960][INFO ][o.o.c.s.ClusterManagerService] [demo-node] Tasks batched with key: org.opensearch.cluster.coordination.JoinHelper, count:3 and sample tasks: elected-as-cluster-manager ([1] nodes joined)[{demo-node}{Kd5AhdqOQ2WwXiCk4k-KmA}{7jUbClzfQcS8G-okwmRx_g}{172.22.0.27}{172.22.0.27:9300}{dimr}{shard_indexing_pressure_enabled=true} elect leader, _BECOME_CLUSTER_MANAGER_TASK_, _FINISH_ELECTION_], term: 2, version: 174, delta: cluster-manager node changed {previous [], current [{demo-node}{Kd5AhdqOQ2WwXiCk4k-KmA}{7jUbClzfQcS8G-okwmRx_g}{172.22.0.27}{172.22.0.27:9300}{dimr}{shard_indexing_pressure_enabled=true}]}
opensearch  | opensearch  | [2026-02-07T05:02:12,993][INFO ][o.o.c.s.ClusterApplierService] [demo-node] cluster-manager node changed {previous [], current [{demo-node}{Kd5AhdqOQ2WwXiCk4k-KmA}{7jUbClzfQcS8G-okwmRx_g}{172.22.0.27}{172.22.0.27:9300}{dimr}{shard_indexing_pressure_enabled=true}]}, term: 2, version: 174, reason: Publication{term=2, version=174}
opensearch  | opensearch  | [2026-02-07T05:02:13,001][INFO ][o.o.i.i.ManagedIndexCoordinator] [demo-node] Cache cluster manager node onClusterManager time: 1770440533001
opensearch  | opensearch  | [2026-02-07T05:02:13,005][INFO ][o.o.d.PeerFinder         ] [demo-node] setting findPeersInterval to [1s] as node commission status = [true] for local node [{demo-node}{Kd5AhdqOQ2WwXiCk4k-KmA}{7jUbClzfQcS8G-okwmRx_g}{172.22.0.27}{172.22.0.27:9300}{dimr}{shard_indexing_pressure_enabled=true}]
opensearch  | opensearch  | [2026-02-07T05:02:13,014][INFO ][o.o.h.AbstractHttpServerTransport] [demo-node] publish_address {172.22.0.27:9200}, bound_addresses {[::]:9200}
opensearch  | opensearch  | [2026-02-07T05:02:13,015][INFO ][o.o.n.Node               ] [demo-node] started
opensearch  | opensearch  | [2026-02-07T05:02:13,015][INFO ][o.o.s.OpenSearchSecurityPlugin] [demo-node] 0 OpenSearch Security modules loaded so far: []
opensearch  | opensearch  | [2026-02-07T05:02:13,074][INFO ][o.o.g.GatewayService     ] [demo-node] recovered [6] indices into cluster_state
opensearch  | opensearch  | [2026-02-07T05:02:13,148][INFO ][o.o.p.PluginsService     ] [demo-node] PluginService:onIndexModule index:[otel-logs-2026-02-06/ztYeVNLtSyC1YBCcrcv-8w]
opensearch  | opensearch  | [2026-02-07T05:02:13,268][INFO ][o.o.p.PluginsService     ] [demo-node] PluginService:onIndexModule index:[otel-logs-2026-02-05/3g9be4LZQyalERiCT5aJww]
opensearch  | opensearch  | [2026-02-07T05:02:13,281][INFO ][o.o.p.PluginsService     ] [demo-node] PluginService:onIndexModule index:[otel-traces-2026-02-06/EdlJsQ3sSQyXaBH2ZF3Jcw]
opensearch  | opensearch  | [2026-02-07T05:02:13,294][INFO ][o.o.p.PluginsService     ] [demo-node] PluginService:onIndexModule index:[otel-traces-2026-02-05/N6FogVMwR0eCSnBtf7ETsQ]
opensearch  | opensearch  | [2026-02-07T05:02:13,684][INFO ][o.o.c.s.ClusterManagerTaskThrottler] [demo-node] Starting cluster manager throttling as all nodes are higher than or equal to 2.5.0
opensearch  | opensearch  | [2026-02-07T05:02:13,689][INFO ][o.o.p.PluginsService     ] [demo-node] PluginService:onIndexModule index:[otel-traces-2026-02-04/Ji0RwV0_Q1mky7QJMlivvw]
opensearch  | opensearch  | [2026-02-07T05:02:13,742][INFO ][o.o.p.PluginsService     ] [demo-node] PluginService:onIndexModule index:[otel-traces-2026-02-07/A5DJnCDgQ4mpNuf2yiHlTA]
opensearch  | opensearch  | [2026-02-07T05:02:13,750][INFO ][o.o.c.m.MetadataCreateIndexService] [demo-node] [otel-traces-2026-02-07] creating index, cause [auto(bulk api)], templates [], shards [1]/[1]
opensearch  | opensearch  | [2026-02-07T05:02:13,789][INFO ][o.o.p.PluginsService     ] [demo-node] PluginService:onIndexModule index:[otel-logs-2026-02-04/A9Roo34zTXyhtFGzJGy9aA]
opensearch  | opensearch  | [2026-02-07T05:02:13,799][INFO ][o.o.p.PluginsService     ] [demo-node] PluginService:onIndexModule index:[otel-logs-2026-02-07/psH-t4nuTJ-1rKYuEIH4bw]
opensearch  | opensearch  | [2026-02-07T05:02:13,801][INFO ][o.o.c.m.MetadataCreateIndexService] [demo-node] [otel-logs-2026-02-07] creating index, cause [auto(bulk api)], templates [], shards [1]/[1]
opensearch  | opensearch  | [2026-02-07T05:02:14,063][INFO ][o.o.p.PluginsService     ] [demo-node] PluginService:onIndexModule index:[otel-traces-2026-02-07/A5DJnCDgQ4mpNuf2yiHlTA]
opensearch  | opensearch  | [2026-02-07T05:02:14,070][INFO ][o.o.p.PluginsService     ] [demo-node] PluginService:onIndexModule index:[otel-logs-2026-02-07/psH-t4nuTJ-1rKYuEIH4bw]
opensearch  | opensearch  | [2026-02-07T05:02:14,162][INFO ][o.o.p.PluginsService     ] [demo-node] PluginService:onIndexModule index:[otel-traces-2026-02-07/A5DJnCDgQ4mpNuf2yiHlTA]
opensearch  | opensearch  | [2026-02-07T05:02:14,168][INFO ][o.o.c.m.MetadataMappingService] [demo-node] [otel-traces-2026-02-07/A5DJnCDgQ4mpNuf2yiHlTA] create_mapping
opensearch  | opensearch  | [2026-02-07T05:02:14,169][INFO ][o.o.p.PluginsService     ] [demo-node] PluginService:onIndexModule index:[otel-logs-2026-02-07/psH-t4nuTJ-1rKYuEIH4bw]
opensearch  | opensearch  | [2026-02-07T05:02:14,180][INFO ][o.o.c.m.MetadataMappingService] [demo-node] [otel-logs-2026-02-07/psH-t4nuTJ-1rKYuEIH4bw] create_mapping
opensearch  | opensearch  | [2026-02-07T05:02:14,204][INFO ][o.o.p.PluginsService     ] [demo-node] PluginService:onIndexModule index:[otel-logs-2026-02-07/psH-t4nuTJ-1rKYuEIH4bw]
opensearch  | opensearch  | [2026-02-07T05:02:14,208][INFO ][o.o.c.m.MetadataMappingService] [demo-node] [otel-logs-2026-02-07/psH-t4nuTJ-1rKYuEIH4bw] update_mapping [_doc]
opensearch  | opensearch  | [2026-02-07T05:02:14,242][INFO ][o.o.p.PluginsService     ] [demo-node] PluginService:onIndexModule index:[otel-traces-2026-02-07/A5DJnCDgQ4mpNuf2yiHlTA]
opensearch  | opensearch  | [2026-02-07T05:02:14,246][INFO ][o.o.c.m.MetadataMappingService] [demo-node] [otel-traces-2026-02-07/A5DJnCDgQ4mpNuf2yiHlTA] update_mapping [_doc]
opensearch  | opensearch  | [2026-02-07T05:02:14,250][INFO ][o.o.c.m.MetadataMappingService] [demo-node] [otel-traces-2026-02-07/A5DJnCDgQ4mpNuf2yiHlTA] update_mapping [_doc]
opensearch  | opensearch  | [2026-02-07T05:02:14,275][INFO ][o.o.p.PluginsService     ] [demo-node] PluginService:onIndexModule index:[otel-logs-2026-02-07/psH-t4nuTJ-1rKYuEIH4bw]
opensearch  | opensearch  | [2026-02-07T05:02:14,280][INFO ][o.o.p.PluginsService     ] [demo-node] PluginService:onIndexModule index:[otel-traces-2026-02-07/A5DJnCDgQ4mpNuf2yiHlTA]
opensearch  | opensearch  | [2026-02-07T05:02:14,289][INFO ][o.o.p.PluginsService     ] [demo-node] PluginService:onIndexModule index:[otel-traces-2026-02-07/A5DJnCDgQ4mpNuf2yiHlTA]
opensearch  | opensearch  | [2026-02-07T05:02:14,304][INFO ][o.o.p.PluginsService     ] [demo-node] PluginService:onIndexModule index:[otel-traces-2026-02-07/A5DJnCDgQ4mpNuf2yiHlTA]
opensearch  | opensearch  | [2026-02-07T05:02:14,310][INFO ][o.o.c.m.MetadataMappingService] [demo-node] [otel-traces-2026-02-07/A5DJnCDgQ4mpNuf2yiHlTA] update_mapping [_doc]
opensearch  | opensearch  | [2026-02-07T05:02:14,334][INFO ][o.o.p.PluginsService     ] [demo-node] PluginService:onIndexModule index:[otel-traces-2026-02-07/A5DJnCDgQ4mpNuf2yiHlTA]
opensearch  | opensearch  | [2026-02-07T05:02:14,341][INFO ][o.o.c.m.MetadataMappingService] [demo-node] [otel-traces-2026-02-07/A5DJnCDgQ4mpNuf2yiHlTA] update_mapping [_doc]
opensearch  | opensearch  | [2026-02-07T05:02:14,369][INFO ][o.o.p.PluginsService     ] [demo-node] PluginService:onIndexModule index:[otel-logs-2026-02-07/psH-t4nuTJ-1rKYuEIH4bw]
opensearch  | opensearch  | [2026-02-07T05:02:14,376][INFO ][o.o.c.m.MetadataMappingService] [demo-node] [otel-logs-2026-02-07/psH-t4nuTJ-1rKYuEIH4bw] update_mapping [_doc]
opensearch  | opensearch  | [2026-02-07T05:02:14,395][INFO ][o.o.p.PluginsService     ] [demo-node] PluginService:onIndexModule index:[otel-traces-2026-02-07/A5DJnCDgQ4mpNuf2yiHlTA]
opensearch  | opensearch  | [2026-02-07T05:02:14,401][INFO ][o.o.c.m.MetadataMappingService] [demo-node] [otel-traces-2026-02-07/A5DJnCDgQ4mpNuf2yiHlTA] update_mapping [_doc]
opensearch  | opensearch  | [2026-02-07T05:02:14,423][INFO ][o.o.p.PluginsService     ] [demo-node] PluginService:onIndexModule index:[otel-logs-2026-02-07/psH-t4nuTJ-1rKYuEIH4bw]
opensearch  | opensearch  | [2026-02-07T05:02:14,433][INFO ][o.o.p.PluginsService     ] [demo-node] PluginService:onIndexModule index:[otel-traces-2026-02-07/A5DJnCDgQ4mpNuf2yiHlTA]
opensearch  | opensearch  | [2026-02-07T05:02:15,400][INFO ][o.o.p.PluginsService     ] [demo-node] PluginService:onIndexModule index:[otel-traces-2026-02-07/A5DJnCDgQ4mpNuf2yiHlTA]
opensearch  | opensearch  | [2026-02-07T05:02:15,407][INFO ][o.o.c.m.MetadataMappingService] [demo-node] [otel-traces-2026-02-07/A5DJnCDgQ4mpNuf2yiHlTA] update_mapping [_doc]
opensearch  | opensearch  | [2026-02-07T05:02:15,431][INFO ][o.o.p.PluginsService     ] [demo-node] PluginService:onIndexModule index:[otel-logs-2026-02-07/psH-t4nuTJ-1rKYuEIH4bw]
opensearch  | opensearch  | [2026-02-07T05:02:15,438][INFO ][o.o.c.m.MetadataMappingService] [demo-node] [otel-logs-2026-02-07/psH-t4nuTJ-1rKYuEIH4bw] update_mapping [_doc]
opensearch  | opensearch  | [2026-02-07T05:02:15,467][INFO ][o.o.p.PluginsService     ] [demo-node] PluginService:onIndexModule index:[otel-logs-2026-02-07/psH-t4nuTJ-1rKYuEIH4bw]
opensearch  | opensearch  | [2026-02-07T05:02:16,018][INFO ][o.o.p.PluginsService     ] [demo-node] PluginService:onIndexModule index:[otel-logs-2026-02-07/psH-t4nuTJ-1rKYuEIH4bw]
opensearch  | opensearch  | [2026-02-07T05:02:16,023][INFO ][o.o.c.m.MetadataMappingService] [demo-node] [otel-logs-2026-02-07/psH-t4nuTJ-1rKYuEIH4bw] update_mapping [_doc]
opensearch  | opensearch  | [2026-02-07T05:02:18,023][INFO ][o.o.p.PluginsService     ] [demo-node] PluginService:onIndexModule index:[otel-traces-2026-02-07/A5DJnCDgQ4mpNuf2yiHlTA]
opensearch  | opensearch  | [2026-02-07T05:02:18,030][INFO ][o.o.c.m.MetadataMappingService] [demo-node] [otel-traces-2026-02-07/A5DJnCDgQ4mpNuf2yiHlTA] update_mapping [_doc]
opensearch  | opensearch  | [2026-02-07T05:02:18,198][INFO ][o.o.p.PluginsService     ] [demo-node] PluginService:onIndexModule index:[otel-logs-2026-02-07/psH-t4nuTJ-1rKYuEIH4bw]
opensearch  | opensearch  | [2026-02-07T05:02:18,203][INFO ][o.o.c.m.MetadataMappingService] [demo-node] [otel-logs-2026-02-07/psH-t4nuTJ-1rKYuEIH4bw] update_mapping [_doc]
opensearch  | opensearch  | [2026-02-07T05:02:18,222][INFO ][o.o.p.PluginsService     ] [demo-node] PluginService:onIndexModule index:[otel-traces-2026-02-07/A5DJnCDgQ4mpNuf2yiHlTA]
opensearch  | opensearch  | [2026-02-07T05:02:18,227][INFO ][o.o.c.m.MetadataMappingService] [demo-node] [otel-traces-2026-02-07/A5DJnCDgQ4mpNuf2yiHlTA] update_mapping [_doc]
opensearch  | opensearch  | [2026-02-07T05:02:18,267][INFO ][o.o.p.PluginsService     ] [demo-node] PluginService:onIndexModule index:[otel-traces-2026-02-07/A5DJnCDgQ4mpNuf2yiHlTA]
opensearch  | opensearch  | [2026-02-07T05:02:19,324][INFO ][o.o.p.PluginsService     ] [demo-node] PluginService:onIndexModule index:[otel-traces-2026-02-07/A5DJnCDgQ4mpNuf2yiHlTA]
opensearch  | opensearch  | [2026-02-07T05:02:19,335][INFO ][o.o.c.m.MetadataMappingService] [demo-node] [otel-traces-2026-02-07/A5DJnCDgQ4mpNuf2yiHlTA] update_mapping [_doc]
opensearch  | opensearch  | [2026-02-07T05:02:19,359][INFO ][o.o.p.PluginsService     ] [demo-node] PluginService:onIndexModule index:[otel-traces-2026-02-07/A5DJnCDgQ4mpNuf2yiHlTA]
opensearch  | opensearch  | [2026-02-07T05:02:19,364][INFO ][o.o.c.m.MetadataMappingService] [demo-node] [otel-traces-2026-02-07/A5DJnCDgQ4mpNuf2yiHlTA] update_mapping [_doc]
opensearch  | opensearch  | [2026-02-07T05:02:19,390][INFO ][o.o.p.PluginsService     ] [demo-node] PluginService:onIndexModule index:[otel-traces-2026-02-07/A5DJnCDgQ4mpNuf2yiHlTA]
opensearch  | opensearch  | [2026-02-07T05:02:24,326][INFO ][o.o.p.PluginsService     ] [demo-node] PluginService:onIndexModule index:[otel-traces-2026-02-07/A5DJnCDgQ4mpNuf2yiHlTA]
opensearch  | opensearch  | [2026-02-07T05:02:24,332][INFO ][o.o.c.m.MetadataMappingService] [demo-node] [otel-traces-2026-02-07/A5DJnCDgQ4mpNuf2yiHlTA] update_mapping [_doc]
opensearch  | opensearch  | [2026-02-07T05:02:24,371][INFO ][o.o.p.PluginsService     ] [demo-node] PluginService:onIndexModule index:[otel-traces-2026-02-07/A5DJnCDgQ4mpNuf2yiHlTA]
opensearch  | opensearch  | [2026-02-07T05:02:24,381][INFO ][o.o.c.m.MetadataMappingService] [demo-node] [otel-traces-2026-02-07/A5DJnCDgQ4mpNuf2yiHlTA] update_mapping [_doc]
opensearch  | opensearch  | [2026-02-07T05:02:24,452][INFO ][o.o.p.PluginsService     ] [demo-node] PluginService:onIndexModule index:[otel-traces-2026-02-07/A5DJnCDgQ4mpNuf2yiHlTA]
opensearch  | opensearch  | [2026-02-07T05:02:24,457][INFO ][o.o.c.m.MetadataMappingService] [demo-node] [otel-traces-2026-02-07/A5DJnCDgQ4mpNuf2yiHlTA] update_mapping [_doc]
opensearch  | opensearch  | [2026-02-07T05:02:28,758][INFO ][o.o.p.PluginsService     ] [demo-node] PluginService:onIndexModule index:[otel-traces-2026-02-07/A5DJnCDgQ4mpNuf2yiHlTA]
opensearch  | opensearch  | [2026-02-07T05:02:28,771][INFO ][o.o.c.m.MetadataMappingService] [demo-node] [otel-traces-2026-02-07/A5DJnCDgQ4mpNuf2yiHlTA] update_mapping [_doc]
opensearch  | opensearch  | [2026-02-07T05:02:28,923][INFO ][o.o.p.PluginsService     ] [demo-node] PluginService:onIndexModule index:[otel-traces-2026-02-07/A5DJnCDgQ4mpNuf2yiHlTA]
opensearch  | opensearch  | [2026-02-07T05:02:28,930][INFO ][o.o.c.m.MetadataMappingService] [demo-node] [otel-traces-2026-02-07/A5DJnCDgQ4mpNuf2yiHlTA] update_mapping [_doc]
opensearch  | opensearch  | [2026-02-07T05:02:28,979][INFO ][o.o.p.PluginsService     ] [demo-node] PluginService:onIndexModule index:[otel-traces-2026-02-07/A5DJnCDgQ4mpNuf2yiHlTA]
opensearch  | opensearch  | [2026-02-07T05:02:29,000][INFO ][o.o.c.m.MetadataMappingService] [demo-node] [otel-traces-2026-02-07/A5DJnCDgQ4mpNuf2yiHlTA] update_mapping [_doc]
opensearch  | opensearch  | [2026-02-07T05:02:29,058][INFO ][o.o.p.PluginsService     ] [demo-node] PluginService:onIndexModule index:[otel-traces-2026-02-07/A5DJnCDgQ4mpNuf2yiHlTA]
opensearch  | opensearch  | [2026-02-07T05:02:29,069][INFO ][o.o.c.m.MetadataMappingService] [demo-node] [otel-traces-2026-02-07/A5DJnCDgQ4mpNuf2yiHlTA] update_mapping [_doc]
opensearch  | opensearch  | [2026-02-07T05:02:29,097][INFO ][o.o.p.PluginsService     ] [demo-node] PluginService:onIndexModule index:[otel-traces-2026-02-07/A5DJnCDgQ4mpNuf2yiHlTA]
opensearch  | opensearch  | [2026-02-07T05:02:29,102][INFO ][o.o.c.m.MetadataMappingService] [demo-node] [otel-traces-2026-02-07/A5DJnCDgQ4mpNuf2yiHlTA] update_mapping [_doc]
opensearch  | opensearch  | [2026-02-07T05:02:33,904][INFO ][o.o.p.PluginsService     ] [demo-node] PluginService:onIndexModule index:[otel-traces-2026-02-07/A5DJnCDgQ4mpNuf2yiHlTA]
opensearch  | opensearch  | [2026-02-07T05:02:33,915][INFO ][o.o.c.m.MetadataMappingService] [demo-node] [otel-traces-2026-02-07/A5DJnCDgQ4mpNuf2yiHlTA] update_mapping [_doc]
otel-collector  | otel-collector  | 2026-02-07T05:02:03.890Z	warn	internal/resourcedetection.go:166	Context was cancelled: %w	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "resourcedetection", "otelcol.component.kind": "processor", "otelcol.pipeline.id": "metrics", "otelcol.signal": "metrics", "error": "context deadline exceeded"}
otel-collector  | otel-collector  | 2026-02-07T05:02:03.890Z	info	internal/resourcedetection.go:188	detected resource information	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "resourcedetection", "otelcol.component.kind": "processor", "otelcol.pipeline.id": "metrics", "otelcol.signal": "metrics", "resource": {"host.name":"262f53319fbe","os.type":"linux"}}
otel-collector  | otel-collector  | 2026-02-07T05:02:03.890Z	info	spanmetricsconnector@v0.139.0/connector.go:215	Starting spanmetrics connector	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "spanmetrics", "otelcol.component.kind": "connector", "otelcol.signal": "traces", "otelcol.signal.output": "metrics"}
otel-collector  | otel-collector  | 2026-02-07T05:02:03.891Z	info	otlpreceiver@v0.139.0/otlp.go:120	Starting GRPC server	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "otlp", "otelcol.component.kind": "receiver", "endpoint": "[::]:4317"}
otel-collector  | otel-collector  | 2026-02-07T05:02:03.891Z	info	otlpreceiver@v0.139.0/otlp.go:178	Starting HTTP server	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "otlp", "otelcol.component.kind": "receiver", "endpoint": "[::]:4318"}
otel-collector  | otel-collector  | 2026-02-07T05:02:03.891Z	info	service@v0.139.0/service.go:245	Everything is ready. Begin running and processing data.	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}}
otel-collector  | otel-collector  | 2026-02-07T05:02:03.978Z	info	Logs	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 1}
otel-collector  | otel-collector  | 2026-02-07T05:02:04.095Z	info	Traces	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "resource spans": 1, "spans": 1}
otel-collector  | otel-collector  | 2026-02-07T05:02:04.144Z	info	Traces	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "resource spans": 1, "spans": 3}
otel-collector  | otel-collector  | 2026-02-07T05:02:04.216Z	info	Traces	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "resource spans": 1, "spans": 1}
otel-collector  | otel-collector  | 2026-02-07T05:02:04.332Z	info	Logs	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 1}
otel-collector  | otel-collector  | 2026-02-07T05:02:04.353Z	info	Traces	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "resource spans": 1, "spans": 4}
otel-collector  | otel-collector  | 2026-02-07T05:02:04.354Z	info	Logs	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 2}
otel-collector  | otel-collector  | 2026-02-07T05:02:04.355Z	info	Traces	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "resource spans": 1, "spans": 4}
otel-collector  | otel-collector  | 2026-02-07T05:02:04.565Z	info	Metrics	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 3, "data points": 3}
otel-collector  | otel-collector  | redis: 2026/02/07 05:02:04 redis.go:478: auto mode fallback: maintnotifications disabled due to handshake error: ERR unknown subcommand 'maint_notifications'. Try CLIENT HELP.
otel-collector  | otel-collector  | 2026-02-07T05:02:04.893Z	info	Metrics	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 4, "data points": 7}
otel-collector  | otel-collector  | 2026-02-07T05:02:04.894Z	info	Metrics	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 29, "data points": 34}
otel-collector  | otel-collector  | 2026-02-07T05:02:04.904Z	info	Metrics	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 2, "data points": 6}
otel-collector  | otel-collector  | 2026-02-07T05:02:04.913Z	error	scraperhelper@v0.139.0/obs_metrics.go:61	Error scraping metrics	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "hostmetrics", "otelcol.component.kind": "receiver", "otelcol.signal": "metrics", "error": "failed to read usage at /hostfs/etc/otelcol-config-extras.yml: no such file or directory; failed to read usage at /hostfs/etc/otelcol-config.yml: no such file or directory"}
otel-collector  | otel-collector  | go.opentelemetry.io/collector/scraper/scraperhelper.wrapObsMetrics.func1
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/scraper/scraperhelper@v0.139.0/obs_metrics.go:61
otel-collector  | otel-collector  | go.opentelemetry.io/collector/scraper.ScrapeMetricsFunc.ScrapeMetrics
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/scraper@v0.139.0/metrics.go:24
otel-collector  | otel-collector  | go.opentelemetry.io/collector/scraper/scraperhelper.scrapeMetrics
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/scraper/scraperhelper@v0.139.0/controller.go:256
otel-collector  | otel-collector  | go.opentelemetry.io/collector/scraper/scraperhelper.NewMetricsController.func1
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/scraper/scraperhelper@v0.139.0/controller.go:228
otel-collector  | otel-collector  | go.opentelemetry.io/collector/scraper/scraperhelper.(*controller[...]).startScraping.func1
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/scraper/scraperhelper@v0.139.0/controller.go:171
otel-collector  | otel-collector  | 2026-02-07T05:02:04.915Z	info	Metrics	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 8, "metrics": 26, "data points": 556}
otel-collector  | otel-collector  | 2026-02-07T05:02:04.930Z	info	Traces	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "resource spans": 1, "spans": 1}
otel-collector  | otel-collector  | 2026-02-07T05:02:04.980Z	info	Metrics	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 12, "metrics": 62, "data points": 109}
otel-collector  | otel-collector  | 2026-02-07T05:02:04.989Z	info	Logs	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 1}
otel-collector  | otel-collector  | 2026-02-07T05:02:05.141Z	info	Logs	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 1}
otel-collector  | otel-collector  | 2026-02-07T05:02:05.143Z	info	Logs	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 1}
otel-collector  | otel-collector  | 2026-02-07T05:02:05.307Z	info	Logs	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 1}
otel-collector  | otel-collector  | 2026-02-07T05:02:05.357Z	info	Logs	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 1}
otel-collector  | otel-collector  | 2026-02-07T05:02:05.376Z	info	Traces	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "resource spans": 1, "spans": 10}
otel-collector  | otel-collector  | 2026-02-07T05:02:05.386Z	info	Logs	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 12}
otel-collector  | otel-collector  | 2026-02-07T05:02:05.475Z	info	Logs	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 1}
otel-collector  | otel-collector  | 2026-02-07T05:02:06.468Z	info	Traces	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "resource spans": 1, "spans": 28}
otel-collector  | otel-collector  | 2026-02-07T05:02:07.318Z	error	opensearchexporter@v0.139.0/logger.go:36	Request failed.	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "opensearch", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "path": "/_bulk", "method": "POST", "duration": 2.328768003, "reason": "dial tcp 172.22.0.27:9200: connect: connection refused"}
otel-collector  | otel-collector  | github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*clientLogger).LogRoundTrip
otel-collector  | otel-collector  | 	github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.139.0/logger.go:36
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchtransport.(*Client).logRoundTrip
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchtransport/opensearchtransport.go:491
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchtransport.(*Client).Perform
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchtransport/opensearchtransport.go:318
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4.(*Client).Perform
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearch.go:236
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4.(*Client).Do
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearch.go:251
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchapi.(*Client).do
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchapi/opensearchapi.go:102
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchapi.Client.Bulk
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchapi/api_bulk.go:24
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchutil.(*worker).flush
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchutil/bulk_indexer.go:505
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchutil.(*bulkIndexer).Close
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchutil/bulk_indexer.go:237
otel-collector  | otel-collector  | github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logBulkIndexer).close
otel-collector  | otel-collector  | 	github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.139.0/log_bulk_indexer.go:41
otel-collector  | otel-collector  | github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logExporter).pushLogData
otel-collector  | otel-collector  | 	github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.139.0/sso_log_exporter.go:87
otel-collector  | otel-collector  | go.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/consumer@v1.45.0/logs.go:27
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper.NewLogs.RequestConsumeFromLogs.func2
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queuebatch/logs.go:125
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/sender.(*sender[...]).Send
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/sender/sender.go:31
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal.(*retrySender).Send
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/retry_sender.go:91
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal.(*obsReportSender[...]).Send
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/obs_report_sender.go:92
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal.NewQueueSender.func1
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queue_sender.go:48
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/queuebatch.(*disabledBatcher[...]).Consume
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queuebatch/disabled_batcher.go:23
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/queue.(*asyncQueue[...]).Start.func1
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queue/async_queue.go:49
otel-collector  | otel-collector  | 2026-02-07T05:02:07.318Z	error	opensearchexporter@v0.139.0/logger.go:36	Request failed.	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "opensearch", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "path": "/_bulk", "method": "POST", "duration": 2.963775679, "reason": "dial tcp 172.22.0.27:9200: connect: connection refused"}
otel-collector  | otel-collector  | github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*clientLogger).LogRoundTrip
otel-collector  | otel-collector  | 	github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.139.0/logger.go:36
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchtransport.(*Client).logRoundTrip
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchtransport/opensearchtransport.go:491
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchtransport.(*Client).Perform
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchtransport/opensearchtransport.go:318
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4.(*Client).Perform
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearch.go:236
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4.(*Client).Do
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearch.go:251
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchapi.(*Client).do
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchapi/opensearchapi.go:102
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchapi.Client.Bulk
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchapi/api_bulk.go:24
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchutil.(*worker).flush
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchutil/bulk_indexer.go:505
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchutil.(*bulkIndexer).Close
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchutil/bulk_indexer.go:237
otel-collector  | otel-collector  | github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logBulkIndexer).close
otel-collector  | otel-collector  | 	github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.139.0/log_bulk_indexer.go:41
otel-collector  | otel-collector  | github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logExporter).pushLogData
otel-collector  | otel-collector  | 	github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.139.0/sso_log_exporter.go:87
otel-collector  | otel-collector  | go.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/consumer@v1.45.0/logs.go:27
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper.NewLogs.RequestConsumeFromLogs.func2
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queuebatch/logs.go:125
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/sender.(*sender[...]).Send
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/sender/sender.go:31
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal.(*retrySender).Send
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/retry_sender.go:91
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal.(*obsReportSender[...]).Send
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/obs_report_sender.go:92
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal.NewQueueSender.func1
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queue_sender.go:48
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/queuebatch.(*disabledBatcher[...]).Consume
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queuebatch/disabled_batcher.go:23
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/queue.(*asyncQueue[...]).Start.func1
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queue/async_queue.go:49
otel-collector  | otel-collector  | 2026-02-07T05:02:07.318Z	error	opensearchexporter@v0.139.0/logger.go:36	Request failed.	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "opensearch", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "path": "/_bulk", "method": "POST", "duration": 1.941941831, "reason": "dial tcp 172.22.0.27:9200: connect: connection refused"}
otel-collector  | otel-collector  | github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*clientLogger).LogRoundTrip
otel-collector  | otel-collector  | 	github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.139.0/logger.go:36
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchtransport.(*Client).logRoundTrip
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchtransport/opensearchtransport.go:491
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchtransport.(*Client).Perform
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchtransport/opensearchtransport.go:318
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4.(*Client).Perform
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearch.go:236
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4.(*Client).Do
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearch.go:251
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchapi.(*Client).do
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchapi/opensearchapi.go:102
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchapi.Client.Bulk
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchapi/api_bulk.go:24
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchutil.(*worker).flush
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchutil/bulk_indexer.go:505
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchutil.(*bulkIndexer).Close
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchutil/bulk_indexer.go:237
otel-collector  | otel-collector  | github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*traceBulkIndexer).close
otel-collector  | otel-collector  | 	github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.139.0/trace_bulk_indexer.go:43
otel-collector  | otel-collector  | github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*ssoTracesExporter).pushTraceData
otel-collector  | otel-collector  | 	github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.139.0/sso_trace_exporter.go:77
otel-collector  | otel-collector  | go.opentelemetry.io/collector/consumer.ConsumeTracesFunc.ConsumeTraces
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/consumer@v1.45.0/traces.go:27
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper.NewTraces.RequestConsumeFromTraces.func2
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queuebatch/traces.go:124
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/sender.(*sender[...]).Send
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/sender/sender.go:31
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal.(*retrySender).Send
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/retry_sender.go:91
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal.(*obsReportSender[...]).Send
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/obs_report_sender.go:92
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal.NewQueueSender.func1
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queue_sender.go:48
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/queuebatch.(*disabledBatcher[...]).Consume
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queuebatch/disabled_batcher.go:23
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/queue.(*asyncQueue[...]).Start.func1
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queue/async_queue.go:49
otel-collector  | otel-collector  | 2026-02-07T05:02:07.318Z	error	internal/queue_sender.go:49	Exporting failed. Dropping data.	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "opensearch", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "error": "not retryable error: Permanent error: flush: dial tcp 172.22.0.27:9200: connect: connection refused\nPermanent error: Permanent error: flush: dial tcp 172.22.0.27:9200: connect: connection refused", "dropped_items": 1}
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal.NewQueueSender.func1
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queue_sender.go:49
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/queuebatch.(*disabledBatcher[...]).Consume
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queuebatch/disabled_batcher.go:23
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/queue.(*asyncQueue[...]).Start.func1
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queue/async_queue.go:49
otel-collector  | otel-collector  | 2026-02-07T05:02:07.318Z	error	internal/queue_sender.go:49	Exporting failed. Dropping data.	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "opensearch", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "error": "not retryable error: Permanent error: flush: dial tcp 172.22.0.27:9200: connect: connection refused\nPermanent error: Permanent error: flush: dial tcp 172.22.0.27:9200: connect: connection refused", "dropped_items": 1}
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal.NewQueueSender.func1
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queue_sender.go:49
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/queuebatch.(*disabledBatcher[...]).Consume
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queuebatch/disabled_batcher.go:23
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/queue.(*asyncQueue[...]).Start.func1
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queue/async_queue.go:49
otel-collector  | otel-collector  | 2026-02-07T05:02:07.318Z	error	opensearchexporter@v0.139.0/logger.go:36	Request failed.	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "opensearch", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "path": "/_bulk", "method": "POST", "duration": 2.387877255, "reason": "dial tcp 172.22.0.27:9200: connect: connection refused"}
otel-collector  | otel-collector  | github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*clientLogger).LogRoundTrip
otel-collector  | otel-collector  | 	github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.139.0/logger.go:36
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchtransport.(*Client).logRoundTrip
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchtransport/opensearchtransport.go:491
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchtransport.(*Client).Perform
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchtransport/opensearchtransport.go:318
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4.(*Client).Perform
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearch.go:236
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4.(*Client).Do
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearch.go:251
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchapi.(*Client).do
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchapi/opensearchapi.go:102
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchapi.Client.Bulk
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchapi/api_bulk.go:24
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchutil.(*worker).flush
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchutil/bulk_indexer.go:505
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchutil.(*bulkIndexer).Close
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchutil/bulk_indexer.go:237
otel-collector  | otel-collector  | github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*traceBulkIndexer).close
otel-collector  | otel-collector  | 	github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.139.0/trace_bulk_indexer.go:43
otel-collector  | otel-collector  | github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*ssoTracesExporter).pushTraceData
otel-collector  | otel-collector  | 	github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.139.0/sso_trace_exporter.go:77
otel-collector  | otel-collector  | go.opentelemetry.io/collector/consumer.ConsumeTracesFunc.ConsumeTraces
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/consumer@v1.45.0/traces.go:27
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper.NewTraces.RequestConsumeFromTraces.func2
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queuebatch/traces.go:124
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/sender.(*sender[...]).Send
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/sender/sender.go:31
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal.(*retrySender).Send
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/retry_sender.go:91
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal.(*obsReportSender[...]).Send
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/obs_report_sender.go:92
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal.NewQueueSender.func1
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queue_sender.go:48
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/queuebatch.(*disabledBatcher[...]).Consume
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queuebatch/disabled_batcher.go:23
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/queue.(*asyncQueue[...]).Start.func1
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queue/async_queue.go:49
otel-collector  | otel-collector  | 2026-02-07T05:02:07.318Z	error	opensearchexporter@v0.139.0/logger.go:36	Request failed.	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "opensearch", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "path": "/_bulk", "method": "POST", "duration": 2.964348919, "reason": "dial tcp 172.22.0.27:9200: connect: connection refused"}
otel-collector  | otel-collector  | github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*clientLogger).LogRoundTrip
otel-collector  | otel-collector  | 	github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.139.0/logger.go:36
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchtransport.(*Client).logRoundTrip
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchtransport/opensearchtransport.go:491
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchtransport.(*Client).Perform
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchtransport/opensearchtransport.go:318
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4.(*Client).Perform
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearch.go:236
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4.(*Client).Do
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearch.go:251
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchapi.(*Client).do
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchapi/opensearchapi.go:102
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchapi.Client.Bulk
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchapi/api_bulk.go:24
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchutil.(*worker).flush
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchutil/bulk_indexer.go:505
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchutil.(*bulkIndexer).Close
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchutil/bulk_indexer.go:237
otel-collector  | otel-collector  | github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*traceBulkIndexer).close
otel-collector  | otel-collector  | 	github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.139.0/trace_bulk_indexer.go:43
otel-collector  | otel-collector  | github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*ssoTracesExporter).pushTraceData
otel-collector  | otel-collector  | 	github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.139.0/sso_trace_exporter.go:77
otel-collector  | otel-collector  | go.opentelemetry.io/collector/consumer.ConsumeTracesFunc.ConsumeTraces
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/consumer@v1.45.0/traces.go:27
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper.NewTraces.RequestConsumeFromTraces.func2
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queuebatch/traces.go:124
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/sender.(*sender[...]).Send
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/sender/sender.go:31
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal.(*retrySender).Send
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/retry_sender.go:91
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal.(*obsReportSender[...]).Send
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/obs_report_sender.go:92
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal.NewQueueSender.func1
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queue_sender.go:48
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/queuebatch.(*disabledBatcher[...]).Consume
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queuebatch/disabled_batcher.go:23
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/queue.(*asyncQueue[...]).Start.func1
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queue/async_queue.go:49
otel-collector  | otel-collector  | 2026-02-07T05:02:07.318Z	error	internal/queue_sender.go:49	Exporting failed. Dropping data.	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "opensearch", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "error": "not retryable error: Permanent error: flush: dial tcp 172.22.0.27:9200: connect: connection refused\nPermanent error: flush: dial tcp 172.22.0.27:9200: connect: connection refused\nPermanent error: flush: dial tcp 172.22.0.27:9200: connect: connection refused\nPermanent error: flush: dial tcp 172.22.0.27:9200: connect: connection refused\nPermanent error: flush: dial tcp 172.22.0.27:9200: connect: connection refused\nPermanent error: flush: dial tcp 172.22.0.27:9200: connect: connection refused\nPermanent error: flush: dial tcp 172.22.0.27:9200: connect: connection refused\nPermanent error: flush: dial tcp 172.22.0.27:9200: connect: connection refused\nPermanent error: flush: dial tcp 172.22.0.27:9200: connect: connection refused\nPermanent error: flush: dial tcp 172.22.0.27:9200: connect: connection refused\nPermanent error: flush: dial tcp 172.22.0.27:9200: connect: connection refused\nPermanent error: flush: dial tcp 172.22.0.27:9200: connect: connection refused\nPermanent error: flush: dial tcp 172.22.0.27:9200: connect: connection refused\nPermanent error: flush: dial tcp 172.22.0.27:9200: connect: connection refused\nPermanent error: flush: dial tcp 172.22.0.27:9200: connect: connection refused\nPermanent error: flush: dial tcp 172.22.0.27:9200: connect: connection refused\nPermanent error: flush: dial tcp 172.22.0.27:9200: connect: connection refused\nPermanent error: flush: dial tcp 172.22.0.27:9200: connect: connection refused\nPermanent error: flush: dial tcp 172.22.0.27:9200: connect: connection refused\nPermanent error: flush: dial tcp 172.22.0.27:9200: connect: connection refused\nPermanent error: flush: dial tcp 172.22.0.27:9200: connect: connection refused\nPermanent error: flush: dial tcp 172.22.0.27:9200: connect: connection refused\nPermanent error: flush: dial tcp 172.22.0.27:9200: connect: connection refused\nPermanent error: flush: dial tcp 172.22.0.27:9200: connect: connection refused\nPermanent error: flush: dial tcp 172.22.0.27:9200: connect: connection refused\nPermanent error: flush: dial tcp 172.22.0.27:9200: connect: connection refused\nPermanent error: flush: dial tcp 172.22.0.27:9200: connect: connection refused\nPermanent error: flush: dial tcp 172.22.0.27:9200: connect: connection refused\nPermanent error: Permanent error: flush: dial tcp 172.22.0.27:9200: connect: connection refused", "dropped_items": 28}
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal.NewQueueSender.func1
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queue_sender.go:49
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/queuebatch.(*disabledBatcher[...]).Consume
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queuebatch/disabled_batcher.go:23
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/queue.(*asyncQueue[...]).Start.func1
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queue/async_queue.go:49
otel-collector  | otel-collector  | 2026-02-07T05:02:07.318Z	error	internal/queue_sender.go:49	Exporting failed. Dropping data.	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "opensearch", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "error": "not retryable error: Permanent error: flush: dial tcp 172.22.0.27:9200: connect: connection refused\nPermanent error: flush: dial tcp 172.22.0.27:9200: connect: connection refused\nPermanent error: flush: dial tcp 172.22.0.27:9200: connect: connection refused\nPermanent error: flush: dial tcp 172.22.0.27:9200: connect: connection refused\nPermanent error: flush: dial tcp 172.22.0.27:9200: connect: connection refused\nPermanent error: flush: dial tcp 172.22.0.27:9200: connect: connection refused\nPermanent error: flush: dial tcp 172.22.0.27:9200: connect: connection refused\nPermanent error: flush: dial tcp 172.22.0.27:9200: connect: connection refused\nPermanent error: flush: dial tcp 172.22.0.27:9200: connect: connection refused\nPermanent error: flush: dial tcp 172.22.0.27:9200: connect: connection refused\nPermanent error: Permanent error: flush: dial tcp 172.22.0.27:9200: connect: connection refused", "dropped_items": 10}
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal.NewQueueSender.func1
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queue_sender.go:49
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/queuebatch.(*disabledBatcher[...]).Consume
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queuebatch/disabled_batcher.go:23
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/queue.(*asyncQueue[...]).Start.func1
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queue/async_queue.go:49
otel-collector  | otel-collector  | 2026-02-07T05:02:07.318Z	error	internal/queue_sender.go:49	Exporting failed. Dropping data.	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "opensearch", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "error": "not retryable error: Permanent error: flush: dial tcp 172.22.0.27:9200: connect: connection refused\nPermanent error: flush: dial tcp 172.22.0.27:9200: connect: connection refused\nPermanent error: Permanent error: flush: dial tcp 172.22.0.27:9200: connect: connection refused", "dropped_items": 2}
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal.NewQueueSender.func1
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queue_sender.go:49
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/queuebatch.(*disabledBatcher[...]).Consume
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queuebatch/disabled_batcher.go:23
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/queue.(*asyncQueue[...]).Start.func1
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queue/async_queue.go:49
otel-collector  | otel-collector  | 2026-02-07T05:02:07.318Z	error	internal/queue_sender.go:49	Exporting failed. Dropping data.	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "opensearch", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "error": "not retryable error: Permanent error: flush: dial tcp 172.22.0.27:9200: connect: connection refused\nPermanent error: Permanent error: flush: dial tcp 172.22.0.27:9200: connect: connection refused", "dropped_items": 1}
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal.NewQueueSender.func1
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queue_sender.go:49
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/queuebatch.(*disabledBatcher[...]).Consume
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queuebatch/disabled_batcher.go:23
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/queue.(*asyncQueue[...]).Start.func1
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queue/async_queue.go:49
otel-collector  | otel-collector  | 2026-02-07T05:02:07.318Z	error	opensearchexporter@v0.139.0/logger.go:36	Request failed.	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "opensearch", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "path": "/_bulk", "method": "POST", "duration": 1.961045305, "reason": "dial tcp 172.22.0.27:9200: connect: connection refused"}
otel-collector  | otel-collector  | github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*clientLogger).LogRoundTrip
otel-collector  | otel-collector  | 	github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.139.0/logger.go:36
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchtransport.(*Client).logRoundTrip
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchtransport/opensearchtransport.go:491
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchtransport.(*Client).Perform
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchtransport/opensearchtransport.go:318
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4.(*Client).Perform
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearch.go:236
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4.(*Client).Do
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearch.go:251
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchapi.(*Client).do
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchapi/opensearchapi.go:102
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchapi.Client.Bulk
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchapi/api_bulk.go:24
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchutil.(*worker).flush
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchutil/bulk_indexer.go:505
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchutil.(*bulkIndexer).Close
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchutil/bulk_indexer.go:237
otel-collector  | otel-collector  | github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logBulkIndexer).close
otel-collector  | otel-collector  | 	github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.139.0/log_bulk_indexer.go:41
otel-collector  | otel-collector  | github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logExporter).pushLogData
otel-collector  | otel-collector  | 	github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.139.0/sso_log_exporter.go:87
otel-collector  | otel-collector  | go.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/consumer@v1.45.0/logs.go:27
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper.NewLogs.RequestConsumeFromLogs.func2
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queuebatch/logs.go:125
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/sender.(*sender[...]).Send
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/sender/sender.go:31
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal.(*retrySender).Send
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/retry_sender.go:91
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal.(*obsReportSender[...]).Send
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/obs_report_sender.go:92
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal.NewQueueSender.func1
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queue_sender.go:48
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/queuebatch.(*disabledBatcher[...]).Consume
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queuebatch/disabled_batcher.go:23
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/queue.(*asyncQueue[...]).Start.func1
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queue/async_queue.go:49
otel-collector  | otel-collector  | 2026-02-07T05:02:07.318Z	error	internal/queue_sender.go:49	Exporting failed. Dropping data.	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "opensearch", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "error": "not retryable error: Permanent error: flush: dial tcp 172.22.0.27:9200: connect: connection refused\nPermanent error: Permanent error: flush: dial tcp 172.22.0.27:9200: connect: connection refused", "dropped_items": 1}
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal.NewQueueSender.func1
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queue_sender.go:49
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/queuebatch.(*disabledBatcher[...]).Consume
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queuebatch/disabled_batcher.go:23
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/queue.(*asyncQueue[...]).Start.func1
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queue/async_queue.go:49
otel-collector  | otel-collector  | 2026-02-07T05:02:07.318Z	error	opensearchexporter@v0.139.0/logger.go:36	Request failed.	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "opensearch", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "path": "/_bulk", "method": "POST", "duration": 2.174993597, "reason": "dial tcp 172.22.0.27:9200: connect: connection refused"}
otel-collector  | otel-collector  | github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*clientLogger).LogRoundTrip
otel-collector  | otel-collector  | 	github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.139.0/logger.go:36
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchtransport.(*Client).logRoundTrip
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchtransport/opensearchtransport.go:491
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchtransport.(*Client).Perform
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchtransport/opensearchtransport.go:318
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4.(*Client).Perform
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearch.go:236
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4.(*Client).Do
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearch.go:251
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchapi.(*Client).do
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchapi/opensearchapi.go:102
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchapi.Client.Bulk
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchapi/api_bulk.go:24
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchutil.(*worker).flush
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchutil/bulk_indexer.go:505
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchutil.(*bulkIndexer).Close
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchutil/bulk_indexer.go:237
otel-collector  | otel-collector  | github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logBulkIndexer).close
otel-collector  | otel-collector  | 	github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.139.0/log_bulk_indexer.go:41
otel-collector  | otel-collector  | github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logExporter).pushLogData
otel-collector  | otel-collector  | 	github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.139.0/sso_log_exporter.go:87
otel-collector  | otel-collector  | go.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/consumer@v1.45.0/logs.go:27
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper.NewLogs.RequestConsumeFromLogs.func2
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queuebatch/logs.go:125
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/sender.(*sender[...]).Send
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/sender/sender.go:31
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal.(*retrySender).Send
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/retry_sender.go:91
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal.(*obsReportSender[...]).Send
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/obs_report_sender.go:92
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal.NewQueueSender.func1
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queue_sender.go:48
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/queuebatch.(*disabledBatcher[...]).Consume
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queuebatch/disabled_batcher.go:23
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/queue.(*asyncQueue[...]).Start.func1
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queue/async_queue.go:49
otel-collector  | otel-collector  | 2026-02-07T05:02:07.318Z	error	opensearchexporter@v0.139.0/logger.go:36	Request failed.	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "opensearch", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "path": "/_bulk", "method": "POST", "duration": 3.17362985, "reason": "dial tcp 172.22.0.27:9200: connect: connection refused"}
otel-collector  | otel-collector  | github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*clientLogger).LogRoundTrip
otel-collector  | otel-collector  | 	github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.139.0/logger.go:36
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchtransport.(*Client).logRoundTrip
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchtransport/opensearchtransport.go:491
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchtransport.(*Client).Perform
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchtransport/opensearchtransport.go:318
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4.(*Client).Perform
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearch.go:236
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4.(*Client).Do
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearch.go:251
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchapi.(*Client).do
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchapi/opensearchapi.go:102
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchapi.Client.Bulk
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchapi/api_bulk.go:24
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchutil.(*worker).flush
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchutil/bulk_indexer.go:505
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchutil.(*bulkIndexer).Close
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchutil/bulk_indexer.go:237
otel-collector  | otel-collector  | github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*traceBulkIndexer).close
otel-collector  | otel-collector  | 	github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.139.0/trace_bulk_indexer.go:43
otel-collector  | otel-collector  | github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*ssoTracesExporter).pushTraceData
otel-collector  | otel-collector  | 	github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.139.0/sso_trace_exporter.go:77
otel-collector  | otel-collector  | go.opentelemetry.io/collector/consumer.ConsumeTracesFunc.ConsumeTraces
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/consumer@v1.45.0/traces.go:27
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper.NewTraces.RequestConsumeFromTraces.func2
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queuebatch/traces.go:124
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/sender.(*sender[...]).Send
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/sender/sender.go:31
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal.(*retrySender).Send
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/retry_sender.go:91
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal.(*obsReportSender[...]).Send
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/obs_report_sender.go:92
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal.NewQueueSender.func1
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queue_sender.go:48
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/queuebatch.(*disabledBatcher[...]).Consume
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queuebatch/disabled_batcher.go:23
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/queue.(*asyncQueue[...]).Start.func1
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queue/async_queue.go:49
otel-collector  | otel-collector  | 2026-02-07T05:02:07.318Z	error	internal/queue_sender.go:49	Exporting failed. Dropping data.	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "opensearch", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "error": "not retryable error: Permanent error: flush: dial tcp 172.22.0.27:9200: connect: connection refused\nPermanent error: flush: dial tcp 172.22.0.27:9200: connect: connection refused\nPermanent error: flush: dial tcp 172.22.0.27:9200: connect: connection refused\nPermanent error: flush: dial tcp 172.22.0.27:9200: connect: connection refused\nPermanent error: Permanent error: flush: dial tcp 172.22.0.27:9200: connect: connection refused", "dropped_items": 4}
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal.NewQueueSender.func1
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queue_sender.go:49
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/queuebatch.(*disabledBatcher[...]).Consume
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queuebatch/disabled_batcher.go:23
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/queue.(*asyncQueue[...]).Start.func1
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queue/async_queue.go:49
otel-collector  | otel-collector  | 2026-02-07T05:02:07.318Z	error	internal/queue_sender.go:49	Exporting failed. Dropping data.	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "opensearch", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "error": "not retryable error: Permanent error: flush: dial tcp 172.22.0.27:9200: connect: connection refused\nPermanent error: flush: dial tcp 172.22.0.27:9200: connect: connection refused\nPermanent error: flush: dial tcp 172.22.0.27:9200: connect: connection refused\nPermanent error: Permanent error: flush: dial tcp 172.22.0.27:9200: connect: connection refused", "dropped_items": 3}
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal.NewQueueSender.func1
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queue_sender.go:49
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/queuebatch.(*disabledBatcher[...]).Consume
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queuebatch/disabled_batcher.go:23
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/queue.(*asyncQueue[...]).Start.func1
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queue/async_queue.go:49
otel-collector  | otel-collector  | 2026-02-07T05:02:07.318Z	error	internal/queue_sender.go:49	Exporting failed. Dropping data.	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "opensearch", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "error": "not retryable error: Permanent error: flush: dial tcp 172.22.0.27:9200: connect: connection refused\nPermanent error: Permanent error: flush: dial tcp 172.22.0.27:9200: connect: connection refused", "dropped_items": 1}
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal.NewQueueSender.func1
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queue_sender.go:49
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/queuebatch.(*disabledBatcher[...]).Consume
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queuebatch/disabled_batcher.go:23
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/queue.(*asyncQueue[...]).Start.func1
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queue/async_queue.go:49
otel-collector  | otel-collector  | 2026-02-07T05:02:07.318Z	error	opensearchexporter@v0.139.0/logger.go:36	Request failed.	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "opensearch", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "path": "/_bulk", "method": "POST", "duration": 2.962463196, "reason": "dial tcp 172.22.0.27:9200: connect: connection refused"}
otel-collector  | otel-collector  | github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*clientLogger).LogRoundTrip
otel-collector  | otel-collector  | 	github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.139.0/logger.go:36
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchtransport.(*Client).logRoundTrip
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchtransport/opensearchtransport.go:491
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchtransport.(*Client).Perform
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchtransport/opensearchtransport.go:318
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4.(*Client).Perform
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearch.go:236
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4.(*Client).Do
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearch.go:251
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchapi.(*Client).do
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchapi/opensearchapi.go:102
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchapi.Client.Bulk
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchapi/api_bulk.go:24
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchutil.(*worker).flush
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchutil/bulk_indexer.go:505
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchutil.(*bulkIndexer).Close
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchutil/bulk_indexer.go:237
otel-collector  | otel-collector  | github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*traceBulkIndexer).close
otel-collector  | otel-collector  | 	github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.139.0/trace_bulk_indexer.go:43
otel-collector  | otel-collector  | github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*ssoTracesExporter).pushTraceData
otel-collector  | otel-collector  | 	github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.139.0/sso_trace_exporter.go:77
otel-collector  | otel-collector  | go.opentelemetry.io/collector/consumer.ConsumeTracesFunc.ConsumeTraces
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/consumer@v1.45.0/traces.go:27
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper.NewTraces.RequestConsumeFromTraces.func2
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queuebatch/traces.go:124
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/sender.(*sender[...]).Send
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/sender/sender.go:31
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal.(*retrySender).Send
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/retry_sender.go:91
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal.(*obsReportSender[...]).Send
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/obs_report_sender.go:92
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal.NewQueueSender.func1
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queue_sender.go:48
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/queuebatch.(*disabledBatcher[...]).Consume
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queuebatch/disabled_batcher.go:23
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/queue.(*asyncQueue[...]).Start.func1
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queue/async_queue.go:49
otel-collector  | otel-collector  | 2026-02-07T05:02:07.318Z	error	opensearchexporter@v0.139.0/logger.go:36	Request failed.	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "opensearch", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "path": "/_bulk", "method": "POST", "duration": 3.10211368, "reason": "dial tcp 172.22.0.27:9200: connect: connection refused"}
otel-collector  | otel-collector  | github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*clientLogger).LogRoundTrip
otel-collector  | otel-collector  | 	github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.139.0/logger.go:36
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchtransport.(*Client).logRoundTrip
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchtransport/opensearchtransport.go:491
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchtransport.(*Client).Perform
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchtransport/opensearchtransport.go:318
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4.(*Client).Perform
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearch.go:236
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4.(*Client).Do
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearch.go:251
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchapi.(*Client).do
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchapi/opensearchapi.go:102
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchapi.Client.Bulk
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchapi/api_bulk.go:24
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchutil.(*worker).flush
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchutil/bulk_indexer.go:505
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchutil.(*bulkIndexer).Close
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchutil/bulk_indexer.go:237
otel-collector  | otel-collector  | github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*traceBulkIndexer).close
otel-collector  | otel-collector  | 	github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.139.0/trace_bulk_indexer.go:43
otel-collector  | otel-collector  | github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*ssoTracesExporter).pushTraceData
otel-collector  | otel-collector  | 	github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.139.0/sso_trace_exporter.go:77
otel-collector  | otel-collector  | go.opentelemetry.io/collector/consumer.ConsumeTracesFunc.ConsumeTraces
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/consumer@v1.45.0/traces.go:27
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper.NewTraces.RequestConsumeFromTraces.func2
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queuebatch/traces.go:124
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/sender.(*sender[...]).Send
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/sender/sender.go:31
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal.(*retrySender).Send
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/retry_sender.go:91
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal.(*obsReportSender[...]).Send
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/obs_report_sender.go:92
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal.NewQueueSender.func1
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queue_sender.go:48
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/queuebatch.(*disabledBatcher[...]).Consume
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queuebatch/disabled_batcher.go:23
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/queue.(*asyncQueue[...]).Start.func1
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queue/async_queue.go:49
otel-collector  | otel-collector  | 2026-02-07T05:02:07.318Z	error	opensearchexporter@v0.139.0/logger.go:36	Request failed.	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "opensearch", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "path": "/_bulk", "method": "POST", "duration": 3.338628205, "reason": "dial tcp 172.22.0.27:9200: connect: connection refused"}
otel-collector  | otel-collector  | github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*clientLogger).LogRoundTrip
otel-collector  | otel-collector  | 	github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.139.0/logger.go:36
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchtransport.(*Client).logRoundTrip
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchtransport/opensearchtransport.go:491
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchtransport.(*Client).Perform
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchtransport/opensearchtransport.go:318
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4.(*Client).Perform
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearch.go:236
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4.(*Client).Do
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearch.go:251
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchapi.(*Client).do
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchapi/opensearchapi.go:102
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchapi.Client.Bulk
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchapi/api_bulk.go:24
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchutil.(*worker).flush
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchutil/bulk_indexer.go:505
otel-collector  | otel-collector  | github.com/opensearch-project/opensearch-go/v4/opensearchutil.(*bulkIndexer).Close
otel-collector  | otel-collector  | 	github.com/opensearch-project/opensearch-go/v4@v4.5.0/opensearchutil/bulk_indexer.go:237
otel-collector  | otel-collector  | github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logBulkIndexer).close
otel-collector  | otel-collector  | 	github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.139.0/log_bulk_indexer.go:41
otel-collector  | otel-collector  | github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logExporter).pushLogData
otel-collector  | otel-collector  | 	github.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.139.0/sso_log_exporter.go:87
otel-collector  | otel-collector  | go.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/consumer@v1.45.0/logs.go:27
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper.NewLogs.RequestConsumeFromLogs.func2
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queuebatch/logs.go:125
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/sender.(*sender[...]).Send
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/sender/sender.go:31
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal.(*retrySender).Send
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/retry_sender.go:91
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal.(*obsReportSender[...]).Send
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/obs_report_sender.go:92
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal.NewQueueSender.func1
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queue_sender.go:48
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/queuebatch.(*disabledBatcher[...]).Consume
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queuebatch/disabled_batcher.go:23
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/queue.(*asyncQueue[...]).Start.func1
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queue/async_queue.go:49
otel-collector  | otel-collector  | 2026-02-07T05:02:07.995Z	info	Traces	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "resource spans": 1, "spans": 1}
otel-collector  | otel-collector  | 2026-02-07T05:02:08.214Z	info	Traces	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "resource spans": 1, "spans": 1}
otel-collector  | otel-collector  | 2026-02-07T05:02:08.329Z	info	Metrics	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 19, "data points": 19}
otel-collector  | otel-collector  | 2026-02-07T05:02:08.880Z	info	Metrics	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 48, "data points": 143}
otel-collector  | otel-collector  | 2026-02-07T05:02:14.190Z	info	Logs	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 1}
otel-collector  | otel-collector  | 2026-02-07T05:02:14.220Z	info	Traces	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "resource spans": 1, "spans": 1}
otel-collector  | otel-collector  | 2026-02-07T05:02:14.354Z	info	Traces	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "resource spans": 1, "spans": 4}
otel-collector  | otel-collector  | 2026-02-07T05:02:14.357Z	info	Logs	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 1}
otel-collector  | otel-collector  | 2026-02-07T05:02:14.893Z	info	Metrics	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 4, "data points": 7}
otel-collector  | otel-collector  | 2026-02-07T05:02:14.893Z	info	Metrics	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 29, "data points": 34}
otel-collector  | otel-collector  | 2026-02-07T05:02:14.910Z	info	Metrics	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 12, "metrics": 63, "data points": 110}
otel-collector  | otel-collector  | 2026-02-07T05:02:15.385Z	info	Traces	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "resource spans": 1, "spans": 5}
otel-collector  | otel-collector  | 2026-02-07T05:02:15.392Z	info	Logs	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 2}
otel-collector  | otel-collector  | 2026-02-07T05:02:16.008Z	info	Logs	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 2}
otel-collector  | otel-collector  | 2026-02-07T05:02:18.003Z	info	Traces	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "resource spans": 1, "spans": 1}
otel-collector  | otel-collector  | 2026-02-07T05:02:18.189Z	info	Logs	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 2}
otel-collector  | otel-collector  | 2026-02-07T05:02:18.197Z	info	Traces	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "resource spans": 1, "spans": 1}
otel-collector  | otel-collector  | 2026-02-07T05:02:18.328Z	info	Metrics	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 19, "data points": 19}
otel-collector  | otel-collector  | 2026-02-07T05:02:18.791Z	info	Traces	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "resource spans": 1, "spans": 4}
otel-collector  | otel-collector  | 2026-02-07T05:02:18.878Z	info	Metrics	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 48, "data points": 145}
otel-collector  | otel-collector  | 2026-02-07T05:02:19.049Z	info	Traces	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "resource spans": 1, "spans": 2}
otel-collector  | otel-collector  | 2026-02-07T05:02:19.222Z	info	Traces	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "resource spans": 1, "spans": 1}
otel-collector  | otel-collector  | 2026-02-07T05:02:22.011Z	info	Logs	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 27}
otel-collector  | otel-collector  | 2026-02-07T05:02:22.358Z	info	Logs	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 1}
otel-collector  | otel-collector  | 2026-02-07T05:02:23.012Z	info	Logs	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 2}
otel-collector  | otel-collector  | 2026-02-07T05:02:23.357Z	info	Logs	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 1}
otel-collector  | otel-collector  | 2026-02-07T05:02:24.013Z	info	Logs	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 2}
otel-collector  | otel-collector  | 2026-02-07T05:02:24.157Z	info	Traces	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "resource spans": 1, "spans": 3}
otel-collector  | otel-collector  | 2026-02-07T05:02:24.353Z	info	Traces	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "resource spans": 1, "spans": 2}
otel-collector  | otel-collector  | 2026-02-07T05:02:24.355Z	info	Traces	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "resource spans": 1, "spans": 2}
otel-collector  | otel-collector  | 2026-02-07T05:02:24.359Z	error	internal/queue_sender.go:49	Exporting failed. Dropping data.	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "opensearch", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "error": "not retryable error: Permanent error: {\"type\":\"illegal_argument_exception\",\"reason\":\"can't merge a non object mapping [attributes.upstream_cluster] with an object mapping\",\"caused_by\":{\"type\":\"\",\"reason\":\"\",\"caused_by\":null}}\nPermanent error: {\"type\":\"mapper_parsing_exception\",\"reason\":\"object mapping for [attributes.user_agent] tried to parse field [user_agent] as object, but found a concrete value\",\"caused_by\":{\"type\":\"\",\"reason\":\"\",\"caused_by\":null}}", "dropped_items": 2}
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal.NewQueueSender.func1
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queue_sender.go:49
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/queuebatch.(*disabledBatcher[...]).Consume
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queuebatch/disabled_batcher.go:23
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/queue.(*asyncQueue[...]).Start.func1
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queue/async_queue.go:49
otel-collector  | otel-collector  | 2026-02-07T05:02:24.361Z	error	internal/queue_sender.go:49	Exporting failed. Dropping data.	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "opensearch", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "error": "not retryable error: Permanent error: {\"type\":\"illegal_argument_exception\",\"reason\":\"can't merge a non object mapping [attributes.upstream_cluster] with an object mapping\",\"caused_by\":{\"type\":\"\",\"reason\":\"\",\"caused_by\":null}}\nPermanent error: {\"type\":\"mapper_parsing_exception\",\"reason\":\"object mapping for [attributes.user_agent] tried to parse field [user_agent] as object, but found a concrete value\",\"caused_by\":{\"type\":\"\",\"reason\":\"\",\"caused_by\":null}}", "dropped_items": 2}
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal.NewQueueSender.func1
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queue_sender.go:49
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/queuebatch.(*disabledBatcher[...]).Consume
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queuebatch/disabled_batcher.go:23
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/queue.(*asyncQueue[...]).Start.func1
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queue/async_queue.go:49
otel-collector  | otel-collector  | 2026-02-07T05:02:24.573Z	info	Logs	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 1}
otel-collector  | otel-collector  | 2026-02-07T05:02:24.893Z	info	Metrics	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 4, "data points": 7}
otel-collector  | otel-collector  | 2026-02-07T05:02:24.893Z	info	Metrics	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 29, "data points": 34}
otel-collector  | otel-collector  | 2026-02-07T05:02:24.905Z	info	Metrics	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 12, "metrics": 63, "data points": 110}
otel-collector  | otel-collector  | 2026-02-07T05:02:25.015Z	info	Logs	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 2}
otel-collector  | otel-collector  | 2026-02-07T05:02:25.389Z	info	Traces	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "resource spans": 1, "spans": 4}
otel-collector  | otel-collector  | 2026-02-07T05:02:25.397Z	info	Logs	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 3}
otel-collector  | otel-collector  | 2026-02-07T05:02:25.784Z	info	Logs	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 6}
otel-collector  | otel-collector  | 2026-02-07T05:02:27.282Z	info	Traces	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "resource spans": 1, "spans": 14}
otel-collector  | otel-collector  | 2026-02-07T05:02:27.874Z	info	Logs	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 3}
otel-collector  | otel-collector  | 2026-02-07T05:02:28.011Z	info	Traces	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "resource spans": 1, "spans": 1}
otel-collector  | otel-collector  | 2026-02-07T05:02:28.328Z	info	Metrics	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 19, "data points": 19}
otel-collector  | otel-collector  | 2026-02-07T05:02:28.739Z	info	Traces	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "resource spans": 1, "spans": 5}
otel-collector  | otel-collector  | 2026-02-07T05:02:28.879Z	info	Metrics	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 48, "data points": 145}
otel-collector  | otel-collector  | 2026-02-07T05:02:28.915Z	info	Traces	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "resource spans": 1, "spans": 4}
otel-collector  | otel-collector  | 2026-02-07T05:02:29.051Z	info	Traces	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "resource spans": 1, "spans": 6}
otel-collector  | otel-collector  | 2026-02-07T05:02:29.335Z	info	Logs	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 1}
otel-collector  | otel-collector  | 2026-02-07T05:02:29.353Z	info	Traces	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "resource spans": 1, "spans": 2}
otel-collector  | otel-collector  | 2026-02-07T05:02:29.355Z	info	Traces	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "resource spans": 1, "spans": 2}
otel-collector  | otel-collector  | 2026-02-07T05:02:29.358Z	info	Logs	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 1}
otel-collector  | otel-collector  | 2026-02-07T05:02:29.359Z	error	internal/queue_sender.go:49	Exporting failed. Dropping data.	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "opensearch", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "error": "not retryable error: Permanent error: {\"type\":\"illegal_argument_exception\",\"reason\":\"can't merge a non object mapping [attributes.upstream_cluster] with an object mapping\",\"caused_by\":{\"type\":\"\",\"reason\":\"\",\"caused_by\":null}}\nPermanent error: {\"type\":\"mapper_parsing_exception\",\"reason\":\"object mapping for [attributes.user_agent] tried to parse field [user_agent] as object, but found a concrete value\",\"caused_by\":{\"type\":\"\",\"reason\":\"\",\"caused_by\":null}}", "dropped_items": 2}
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal.NewQueueSender.func1
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queue_sender.go:49
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/queuebatch.(*disabledBatcher[...]).Consume
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queuebatch/disabled_batcher.go:23
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/queue.(*asyncQueue[...]).Start.func1
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queue/async_queue.go:49
otel-collector  | otel-collector  | 2026-02-07T05:02:29.359Z	error	internal/queue_sender.go:49	Exporting failed. Dropping data.	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "opensearch", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "error": "not retryable error: Permanent error: {\"type\":\"illegal_argument_exception\",\"reason\":\"can't merge a non object mapping [attributes.upstream_cluster] with an object mapping\",\"caused_by\":{\"type\":\"\",\"reason\":\"\",\"caused_by\":null}}\nPermanent error: {\"type\":\"mapper_parsing_exception\",\"reason\":\"object mapping for [attributes.user_agent] tried to parse field [user_agent] as object, but found a concrete value\",\"caused_by\":{\"type\":\"\",\"reason\":\"\",\"caused_by\":null}}", "dropped_items": 2}
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal.NewQueueSender.func1
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queue_sender.go:49
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/queuebatch.(*disabledBatcher[...]).Consume
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queuebatch/disabled_batcher.go:23
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal/queue.(*asyncQueue[...]).Start.func1
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/queue/async_queue.go:49
otel-collector  | otel-collector  | 2026-02-07T05:02:29.360Z	info	Logs	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 1}
otel-collector  | otel-collector  | 2026-02-07T05:02:29.543Z	info	Logs	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 3}
otel-collector  | otel-collector  | 2026-02-07T05:02:30.399Z	info	Logs	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 1}
otel-collector  | otel-collector  | 2026-02-07T05:02:34.893Z	info	Metrics	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 1, "metrics": 4, "data points": 7}
otel-collector  | otel-collector  | 2026-02-07T05:02:34.893Z	error	internal/base_exporter.go:114	Exporting failed. Rejecting data.	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "error": "sending queue is full", "rejected_items": 34}
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/base_exporter.go:114
otel-collector  | otel-collector  | go.opentelemetry.io/collector/exporter/exporterhelper/internal.NewMetricsRequest.newConsumeMetrics.func1
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/exporter/exporterhelper@v0.139.0/internal/new_request.go:176
otel-collector  | otel-collector  | go.opentelemetry.io/collector/consumer.ConsumeMetricsFunc.ConsumeMetrics
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/consumer@v1.45.0/metrics.go:27
otel-collector  | otel-collector  | go.opentelemetry.io/collector/service/internal/refconsumer.refMetrics.ConsumeMetrics
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/service@v0.139.0/internal/refconsumer/metrics.go:29
otel-collector  | otel-collector  | go.opentelemetry.io/collector/internal/fanoutconsumer.(*metricsConsumer).ConsumeMetrics
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/internal/fanoutconsumer@v0.139.0/metrics.go:71
otel-collector  | otel-collector  | go.opentelemetry.io/collector/processor/processorhelper.NewMetrics.func1
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/processor/processorhelper@v0.139.0/metrics.go:71
otel-collector  | otel-collector  | go.opentelemetry.io/collector/consumer.ConsumeMetricsFunc.ConsumeMetrics
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/consumer@v1.45.0/metrics.go:27
otel-collector  | otel-collector  | go.opentelemetry.io/collector/service/internal/refconsumer.refMetrics.ConsumeMetrics
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/service@v0.139.0/internal/refconsumer/metrics.go:29
otel-collector  | otel-collector  | go.opentelemetry.io/collector/processor/processorhelper.NewMetrics.func1
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/processor/processorhelper@v0.139.0/metrics.go:71
otel-collector  | otel-collector  | go.opentelemetry.io/collector/consumer.ConsumeMetricsFunc.ConsumeMetrics
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/consumer@v1.45.0/metrics.go:27
otel-collector  | otel-collector  | go.opentelemetry.io/collector/service/internal/refconsumer.refMetrics.ConsumeMetrics
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/service@v0.139.0/internal/refconsumer/metrics.go:29
otel-collector  | otel-collector  | go.opentelemetry.io/collector/consumer.ConsumeMetricsFunc.ConsumeMetrics
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/consumer@v1.45.0/metrics.go:27
otel-collector  | otel-collector  | go.opentelemetry.io/collector/internal/fanoutconsumer.(*metricsConsumer).ConsumeMetrics
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/internal/fanoutconsumer@v0.139.0/metrics.go:60
otel-collector  | otel-collector  | go.opentelemetry.io/collector/scraper/scraperhelper.scrapeMetrics
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/scraper/scraperhelper@v0.139.0/controller.go:265
otel-collector  | otel-collector  | go.opentelemetry.io/collector/scraper/scraperhelper.NewMetricsController.func1
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/scraper/scraperhelper@v0.139.0/controller.go:228
otel-collector  | otel-collector  | go.opentelemetry.io/collector/scraper/scraperhelper.(*controller[...]).startScraping.func1
otel-collector  | otel-collector  | 	go.opentelemetry.io/collector/scraper/scraperhelper@v0.139.0/controller.go:175
otel-collector  | otel-collector  | 2026-02-07T05:02:34.908Z	info	Metrics	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "metrics", "resource metrics": 12, "metrics": 63, "data points": 110}
otel-collector  | otel-collector  | 2026-02-07T05:02:36.358Z	info	Logs	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 1}
otel-collector  | otel-collector  | 2026-02-07T05:02:36.997Z	info	Logs	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 2}
otel-collector  | otel-collector  | 2026-02-07T05:02:37.359Z	info	Logs	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "logs", "resource logs": 1, "log records": 1}
otel-collector  | otel-collector  | 2026-02-07T05:02:38.019Z	info	Traces	{"resource": {"service.instance.id": "c20124e0-10a2-4043-a279-02e6d70b0bc4", "service.name": "otelcol-contrib", "service.version": "0.139.0"}, "otelcol.component.id": "debug", "otelcol.component.kind": "exporter", "otelcol.signal": "traces", "resource spans": 1, "spans": 1}
payment  | payment  | {
payment  | payment  |   resource: {
payment  | payment  |     attributes: {
payment  | payment  |       'process.pid': 1,
payment  | payment  |       'process.executable.name': '/nodejs/bin/node',
payment  | payment  |       'process.executable.path': '/nodejs/bin/node',
payment  | payment  |       'process.command_args': [
payment  | payment  |         '/nodejs/bin/node',
payment  | payment  |         '--require=./opentelemetry.js',
payment  | payment  |         '/usr/src/app/node_modules/thread-stream/lib/worker.js'
payment  | payment  |       ],
payment  | payment  |       'process.runtime.version': '22.21.1',
payment  | payment  |       'process.runtime.name': 'nodejs',
payment  | payment  |       'process.runtime.description': 'Node.js',
payment  | payment  |       'process.command': '/usr/src/app/node_modules/thread-stream/lib/worker.js',
payment  | payment  |       'process.owner': 'nonroot',
payment  | payment  |       'os.type': 'linux',
payment  | payment  |       'os.version': '6.16.8-200.fc42.x86_64',
payment  | payment  |       'host.name': '76cddf2e36e9',
payment  | payment  |       'host.arch': 'amd64',
payment  | payment  |       'service.namespace': 'opentelemetry-demo',
payment  | payment  |       'service.version': '2.1.3',
payment  | payment  |       'service.name': 'payment'
payment  | payment  |     }
payment  | payment  |   },
payment  | payment  |   instrumentationScope: { name: 'payment-logger', version: '1.0.0', schemaUrl: undefined },
payment  | payment  |   timestamp: 1770440519903000,
payment  | payment  |   traceId: undefined,
payment  | payment  |   spanId: undefined,
payment  | payment  |   traceFlags: undefined,
payment  | payment  |   severityText: 'info',
payment  | payment  |   severityNumber: 9,
payment  | payment  |   body: 'payment gRPC server started on 0.0.0.0:50051',
payment  | payment  |   attributes: {}
payment  | payment  | }
postgresql  | postgresql  | 
postgresql  | postgresql  | PostgreSQL Database directory appears to contain a database; Skipping initialization
postgresql  | postgresql  | 
postgresql  | postgresql  | 2026-02-07 05:01:59.207 UTC [1] LOG:  starting PostgreSQL 17.6 (Debian 17.6-2.pgdg13+1) on x86_64-pc-linux-gnu, compiled by gcc (Debian 14.2.0-19) 14.2.0, 64-bit
postgresql  | postgresql  | 2026-02-07 05:01:59.209 UTC [1] LOG:  listening on IPv4 address "0.0.0.0", port 5432
postgresql  | postgresql  | 2026-02-07 05:01:59.209 UTC [1] LOG:  listening on IPv6 address "::", port 5432
postgresql  | postgresql  | 2026-02-07 05:01:59.211 UTC [1] LOG:  listening on Unix socket "/var/run/postgresql/.s.PGSQL.5432"
postgresql  | postgresql  | 2026-02-07 05:01:59.215 UTC [29] LOG:  database system was interrupted; last known up at 2026-02-03 04:50:29 UTC
postgresql  | postgresql  | 2026-02-07 05:01:59.264 UTC [29] LOG:  database system was not properly shut down; automatic recovery in progress
postgresql  | postgresql  | 2026-02-07 05:01:59.267 UTC [29] LOG:  redo starts at 0/11B8CEF0
postgresql  | postgresql  | 2026-02-07 05:01:59.267 UTC [29] LOG:  invalid record length at 0/11B8CF28: expected at least 24, got 0
postgresql  | postgresql  | 2026-02-07 05:01:59.267 UTC [29] LOG:  redo done at 0/11B8CEF0 system usage: CPU: user: 0.00 s, system: 0.00 s, elapsed: 0.00 s
postgresql  | postgresql  | 2026-02-07 05:01:59.274 UTC [27] LOG:  checkpoint starting: end-of-recovery immediate wait
postgresql  | postgresql  | 2026-02-07 05:01:59.291 UTC [27] LOG:  checkpoint complete: wrote 3 buffers (0.0%); 0 WAL file(s) added, 0 removed, 0 recycled; write=0.004 s, sync=0.002 s, total=0.022 s; sync files=2, longest=0.001 s, average=0.001 s; distance=0 kB, estimate=0 kB; lsn=0/11B8CF28, redo lsn=0/11B8CF28
postgresql  | postgresql  | 2026-02-07 05:01:59.297 UTC [1] LOG:  database system is ready to accept connections
product-catalog  | product-catalog  | 2026/02/04 06:29:45 exporter export timeout: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing: dial tcp 172.22.0.24:4317: connect: connection refused"
product-reviews  | product-reviews  | 2026-02-07 05:02:05,466 INFO [main] [product_reviews_server.py:92] [trace_id=b5c33bc86ea531937469da251cb3a5d6 span_id=162b8c24b027ee62 resource.service.name=product-reviews trace_sampled=True] - Receive GetProductReviews for product id:9SIQT8TOJO
product-reviews  | product-reviews  | 2026-02-07 05:02:05,479 INFO [main] [product_reviews_server.py:127] [trace_id=b5c33bc86ea531937469da251cb3a5d6 span_id=b613e925eb345e02 resource.service.name=product-reviews trace_sampled=True] -   username: deep_sky_master, description: The RASA V2 is a dream come true for deep-sky imaging. The f/2.2 speed drastically cuts down exposure times. My best astrophotography investment yet., score: 5.0
product-reviews  | product-reviews  | 2026-02-07 05:02:05,480 INFO [main] [product_reviews_server.py:127] [trace_id=b5c33bc86ea531937469da251cb3a5d6 span_id=b613e925eb345e02 resource.service.name=product-reviews trace_sampled=True] -   username: pro_astro, description: Unbelievable performance for wide-field astrophotography. The short focal length makes guiding less critical. Produces stunning, detailed images., score: 5.0
product-reviews  | product-reviews  | 2026-02-07 05:02:05,480 INFO [main] [product_reviews_server.py:127] [trace_id=b5c33bc86ea531937469da251cb3a5d6 span_id=b613e925eb345e02 resource.service.name=product-reviews trace_sampled=True] -   username: imaging_guru, description: This OTA is a beast! The fast optics mean more data in less time. If you're serious about deep-sky imaging, this is the one., score: 4.5
product-reviews  | product-reviews  | 2026-02-07 05:02:05,480 INFO [main] [product_reviews_server.py:127] [trace_id=b5c33bc86ea531937469da251cb3a5d6 span_id=b613e925eb345e02 resource.service.name=product-reviews trace_sampled=True] -   username: advanced_scope, description: Worth every penny for the quality and speed it offers. My images have never been sharper or more vibrant. A truly professional piece of equipment., score: 5.0
product-reviews  | product-reviews  | 2026-02-07 05:02:05,481 INFO [main] [product_reviews_server.py:127] [trace_id=b5c33bc86ea531937469da251cb3a5d6 span_id=b613e925eb345e02 resource.service.name=product-reviews trace_sampled=True] -   username: precision_optics, description: The engineering behind this RASA is exceptional. It's incredibly efficient for capturing faint objects. A high-end choice for dedicated imagers., score: 4.5
product-reviews  | product-reviews  | 2026-02-07 05:02:08,276 INFO [main] [product_reviews_server.py:104] [trace_id=1ea80a8f4d793170839cd05941725d20 span_id=a3eccac4f4d19cd1 resource.service.name=product-reviews trace_sampled=True] - Receive AskProductAIAssistant for product id:LS4PSXUNUM, question: Can you summarize the product reviews?
product-reviews  | product-reviews  | 2026-02-07 05:02:08,280 INFO [main] [product_reviews_server.py:165] [trace_id=1ea80a8f4d793170839cd05941725d20 span_id=43ce59ef42ff6f42 resource.service.name=product-reviews trace_sampled=True] - llmRateLimitError feature flag: False
product-reviews  | product-reviews  | 2026-02-07 05:02:08,364 INFO [httpx] [_client.py:1025] [trace_id=1ea80a8f4d793170839cd05941725d20 span_id=a6da0890a1054a2b resource.service.name=product-reviews trace_sampled=True] - HTTP Request: POST http://llm:8000/v1/chat/completions "HTTP/1.1 200 OK"
product-reviews  | product-reviews  | 2026-02-07 05:02:08,376 INFO [main] [product_reviews_server.py:228] [trace_id=1ea80a8f4d793170839cd05941725d20 span_id=43ce59ef42ff6f42 resource.service.name=product-reviews trace_sampled=True] - Response message: ChatCompletionMessage(content='requesting a tool call', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageFunctionToolCall(id='call', function=Function(arguments='{"product_id": "LS4PSXUNUM"}', name='fetch_product_reviews'), type='function')])
product-reviews  | product-reviews  | 2026-02-07 05:02:08,377 INFO [main] [product_reviews_server.py:232] [trace_id=1ea80a8f4d793170839cd05941725d20 span_id=43ce59ef42ff6f42 resource.service.name=product-reviews trace_sampled=True] - Model wants to call 1 tool(s)
product-reviews  | product-reviews  | 2026-02-07 05:02:08,377 INFO [main] [product_reviews_server.py:242] [trace_id=1ea80a8f4d793170839cd05941725d20 span_id=43ce59ef42ff6f42 resource.service.name=product-reviews trace_sampled=True] - Processing tool call: 'fetch_product_reviews' with arguments: {'product_id': 'LS4PSXUNUM'}
product-reviews  | product-reviews  | 2026-02-07 05:02:08,387 INFO [main] [product_reviews_server.py:248] [trace_id=1ea80a8f4d793170839cd05941725d20 span_id=43ce59ef42ff6f42 resource.service.name=product-reviews trace_sampled=True] - Function response for fetch_product_reviews: '[["night_walker", "The red light is perfect for preserving night vision during astronomy sessions. The hand warmer is an unexpected bonus. Very practical device.", 5.0], ["star_party_goer", "This flashlight is indispensable for star parties. The red mode is gentle on the eyes, and the power bank feature is super handy. Love it!", 4.5], ["camper_chris", "Rugged and versatile, this flashlight is great for camping and night walks. The hand warmer function is a game-changer on cold nights. Highly recommend.", 4.5], ["emergency_kit", "A fantastic multi-tool for my emergency kit. The red light is useful, and the power bank means I can charge my phone. Great design.", 4.0], ["astro_accessory", "Every astronomer needs one of these. The red light is essential, and the hand warmer and power bank make it incredibly useful. A top-tier accessory.", 5.0]]'
product-reviews  | product-reviews  | 2026-02-07 05:02:08,389 INFO [main] [product_reviews_server.py:270] [trace_id=1ea80a8f4d793170839cd05941725d20 span_id=43ce59ef42ff6f42 resource.service.name=product-reviews trace_sampled=True] - llmInaccurateResponse feature flag: False
product-reviews  | product-reviews  | 2026-02-07 05:02:08,390 INFO [main] [product_reviews_server.py:290] [trace_id=1ea80a8f4d793170839cd05941725d20 span_id=43ce59ef42ff6f42 resource.service.name=product-reviews trace_sampled=True] - Invoking the LLM with the following messages: '[{'role': 'system', 'content': "You are a helpful assistant that answers related to a specific product. Use tools as needed to fetch the product reviews and product information. Keep the response brief with no more than 1-2 sentences. If you don't know the answer, just say you don't know."}, {'role': 'user', 'content': 'Answer the following question about product ID:LS4PSXUNUM: Can you summarize the product reviews?'}, ChatCompletionMessage(content='requesting a tool call', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageFunctionToolCall(id='call', function=Function(arguments='{"product_id": "LS4PSXUNUM"}', name='fetch_product_reviews'), type='function')]), {'tool_call_id': 'call', 'role': 'tool', 'name': 'fetch_product_reviews', 'content': '[["night_walker", "The red light is perfect for preserving night vision during astronomy sessions. The hand warmer is an unexpected bonus. Very practical device.", 5.0], ["star_party_goer", "This flashlight is indispensable for star parties. The red mode is gentle on the eyes, and the power bank feature is super handy. Love it!", 4.5], ["camper_chris", "Rugged and versatile, this flashlight is great for camping and night walks. The hand warmer function is a game-changer on cold nights. Highly recommend.", 4.5], ["emergency_kit", "A fantastic multi-tool for my emergency kit. The red light is useful, and the power bank means I can charge my phone. Great design.", 4.0], ["astro_accessory", "Every astronomer needs one of these. The red light is essential, and the hand warmer and power bank make it incredibly useful. A top-tier accessory.", 5.0]]'}, {'role': 'user', 'content': 'Based on the tool results, answer the original question about product ID:LS4PSXUNUM. Keep the response brief with no more than 1-2 sentences.'}]'
product-reviews  | product-reviews  | 2026-02-07 05:02:08,400 INFO [httpx] [_client.py:1025] [trace_id=1ea80a8f4d793170839cd05941725d20 span_id=3b2c951dcdc2dadd resource.service.name=product-reviews trace_sampled=True] - HTTP Request: POST http://llm:8000/v1/chat/completions "HTTP/1.1 200 OK"
product-reviews  | product-reviews  | 2026-02-07 05:02:08,401 INFO [main] [product_reviews_server.py:301] [trace_id=1ea80a8f4d793170839cd05941725d20 span_id=43ce59ef42ff6f42 resource.service.name=product-reviews trace_sampled=True] - Returning an AI assistant response: 'This 3-in-1 device is highly valued by users for its essential red flashlight mode, which preserves night vision during astronomy sessions and star parties. Its integrated hand warmer and portable power bank features make it an incredibly versatile, rugged, and practical multi-tool for various outdoor activities and emergencies.'
product-reviews  | product-reviews  | 2026-02-07 05:02:22,263 INFO [main] [product_reviews_server.py:92] [trace_id=7d6d571abda97c964128008487fa1bc8 span_id=8573582a9b0c7938 resource.service.name=product-reviews trace_sampled=True] - Receive GetProductReviews for product id:OLJCESPC7Z
product-reviews  | product-reviews  | 2026-02-07 05:02:22,276 INFO [main] [product_reviews_server.py:127] [trace_id=7d6d571abda97c964128008487fa1bc8 span_id=e9ac5eb9056c08e1 resource.service.name=product-reviews trace_sampled=True] -   username: stargazer_mike, description: Great entry-level telescope! Easy to set up and provides clear views of the moon and brighter planets. Highly recommend for new astronomers., score: 4.5
product-reviews  | product-reviews  | 2026-02-07 05:02:22,276 INFO [main] [product_reviews_server.py:127] [trace_id=7d6d571abda97c964128008487fa1bc8 span_id=e9ac5eb9056c08e1 resource.service.name=product-reviews trace_sampled=True] -   username: nightskylover, description: For the price, this Explorascope delivers excellent performance. I was able to see Jupiter's moons clearly. A fantastic purchase for casual viewing., score: 4.0
product-reviews  | product-reviews  | 2026-02-07 05:02:22,277 INFO [main] [product_reviews_server.py:127] [trace_id=7d6d571abda97c964128008487fa1bc8 span_id=e9ac5eb9056c08e1 resource.service.name=product-reviews trace_sampled=True] -   username: beginner_astro, description: A bit tricky to get used to the manual controls, but once you do, it's very rewarding. Saw the Orion Nebula for the first time! Good value., score: 3.5
product-reviews  | product-reviews  | 2026-02-07 05:02:22,277 INFO [main] [product_reviews_server.py:127] [trace_id=7d6d571abda97c964128008487fa1bc8 span_id=e9ac5eb9056c08e1 resource.service.name=product-reviews trace_sampled=True] -   username: celestial_explorer, description: Perfect for camping trips. It's lightweight and portable, making it easy to take anywhere. The views are surprisingly good for its size., score: 4.0
product-reviews  | product-reviews  | 2026-02-07 05:02:22,277 INFO [main] [product_reviews_server.py:127] [trace_id=7d6d571abda97c964128008487fa1bc8 span_id=e9ac5eb9056c08e1 resource.service.name=product-reviews trace_sampled=True] -   username: telescope_fan, description: Not the most powerful scope, but it's great for kids and beginners. My children love looking at the moon with it. A solid choice for family fun., score: 3.0
prometheus  | prometheus  | time=2026-02-07T05:02:04.568Z level=INFO source=compact.go:565 msg="write block started" component=tsdb mint=1770379202579 maxt=1770386400000 ulid=01KGV7N0RRY2D8VWTKNEYNG0NZ
prometheus  | prometheus  | time=2026-02-07T05:02:04.642Z level=INFO source=compact.go:599 msg="write block completed" component=tsdb mint=1770379202579 maxt=1770386400000 ulid=01KGV7N0RRY2D8VWTKNEYNG0NZ duration=74.864696ms ooo=false
prometheus  | prometheus  | time=2026-02-07T05:02:04.644Z level=INFO source=db.go:1825 msg="Deleting obsolete block" component=tsdb block=01KGV7N0RRY2D8VWTKNEYNG0NZ
prometheus  | prometheus  | time=2026-02-07T05:02:04.644Z level=INFO source=head.go:1420 msg="Head GC started" component=tsdb caller=truncateMemory
prometheus  | prometheus  | time=2026-02-07T05:02:04.650Z level=INFO source=head.go:1424 msg="Head GC completed" component=tsdb caller=truncateMemory duration=5.740462ms
prometheus  | prometheus  | time=2026-02-07T05:02:04.650Z level=INFO source=compact.go:565 msg="write block started" component=tsdb mint=1770386402579 maxt=1770393600000 ulid=01KGV7N0VAFYPYEWC6YYQZN471
prometheus  | prometheus  | time=2026-02-07T05:02:04.730Z level=INFO source=compact.go:599 msg="write block completed" component=tsdb mint=1770386402579 maxt=1770393600000 ulid=01KGV7N0VAFYPYEWC6YYQZN471 duration=79.445765ms ooo=false
prometheus  | prometheus  | time=2026-02-07T05:02:04.730Z level=WARN source=db.go:1661 msg="Overlapping blocks found during reloadBlocks" component=tsdb detail="[mint: 1770386402579, maxt: 1770393600000, range: 1h59m57s, blocks: 2]: <ulid: 01KGSQF9MT7NMMP4WC87BPMJ70, mint: 1770386400000, maxt: 1770393600000, range: 2h0m0s>, <ulid: 01KGV7N0VAFYPYEWC6YYQZN471, mint: 1770386402579, maxt: 1770393600000, range: 1h59m57s>"
prometheus  | prometheus  | time=2026-02-07T05:02:04.730Z level=INFO source=head.go:1420 msg="Head GC started" component=tsdb caller=truncateMemory
prometheus  | prometheus  | time=2026-02-07T05:02:04.734Z level=INFO source=head.go:1424 msg="Head GC completed" component=tsdb caller=truncateMemory duration=4.332062ms
prometheus  | prometheus  | time=2026-02-07T05:02:04.735Z level=INFO source=compact.go:565 msg="write block started" component=tsdb mint=1770393602579 maxt=1770400800000 ulid=01KGV7N0XZDYH7ED6S204M1YF2
prometheus  | prometheus  | time=2026-02-07T05:02:04.876Z level=INFO source=compact.go:599 msg="write block completed" component=tsdb mint=1770393602579 maxt=1770400800000 ulid=01KGV7N0XZDYH7ED6S204M1YF2 duration=140.919359ms ooo=false
prometheus  | prometheus  | time=2026-02-07T05:02:04.878Z level=INFO source=db.go:1825 msg="Deleting obsolete block" component=tsdb block=01KGSQF9MT7NMMP4WC87BPMJ70
prometheus  | prometheus  | time=2026-02-07T05:02:04.879Z level=INFO source=db.go:1825 msg="Deleting obsolete block" component=tsdb block=01KGV7N0VAFYPYEWC6YYQZN471
prometheus  | prometheus  | time=2026-02-07T05:02:04.879Z level=INFO source=head.go:1420 msg="Head GC started" component=tsdb caller=truncateMemory
prometheus  | prometheus  | time=2026-02-07T05:02:04.905Z level=INFO source=head.go:1424 msg="Head GC completed" component=tsdb caller=truncateMemory duration=26.311908ms
prometheus  | prometheus  | time=2026-02-07T05:02:04.906Z level=INFO source=compact.go:565 msg="write block started" component=tsdb mint=1770400800000 maxt=1770408000000 ulid=01KGV7N13AA2BRQAV49H20K1H0
prometheus  | prometheus  | time=2026-02-07T05:02:04.912Z level=INFO source=compact.go:590 msg="write block resulted in empty block" component=tsdb mint=1770400800000 maxt=1770408000000 duration=6.031784ms
prometheus  | prometheus  | time=2026-02-07T05:02:04.912Z level=INFO source=head.go:1420 msg="Head GC started" component=tsdb caller=truncateMemory
prometheus  | prometheus  | time=2026-02-07T05:02:04.913Z level=INFO source=head.go:1424 msg="Head GC completed" component=tsdb caller=truncateMemory duration=1.322009ms
prometheus  | prometheus  | time=2026-02-07T05:02:04.913Z level=INFO source=compact.go:565 msg="write block started" component=tsdb mint=1770408000000 maxt=1770415200000 ulid=01KGV7N13HXW2M1MVCJQFW87NR
prometheus  | prometheus  | time=2026-02-07T05:02:04.925Z level=INFO source=compact.go:590 msg="write block resulted in empty block" component=tsdb mint=1770408000000 maxt=1770415200000 duration=11.841156ms
prometheus  | prometheus  | time=2026-02-07T05:02:04.925Z level=INFO source=head.go:1420 msg="Head GC started" component=tsdb caller=truncateMemory
prometheus  | prometheus  | time=2026-02-07T05:02:04.927Z level=INFO source=head.go:1424 msg="Head GC completed" component=tsdb caller=truncateMemory duration=1.48183ms
prometheus  | prometheus  | time=2026-02-07T05:02:04.927Z level=INFO source=compact.go:565 msg="write block started" component=tsdb mint=1770415200000 maxt=1770422400000 ulid=01KGV7N13ZRB28HE80F1K0ZE6X
prometheus  | prometheus  | time=2026-02-07T05:02:04.939Z level=INFO source=compact.go:590 msg="write block resulted in empty block" component=tsdb mint=1770415200000 maxt=1770422400000 duration=12.855286ms
prometheus  | prometheus  | time=2026-02-07T05:02:04.940Z level=INFO source=head.go:1420 msg="Head GC started" component=tsdb caller=truncateMemory
prometheus  | prometheus  | time=2026-02-07T05:02:04.941Z level=INFO source=head.go:1424 msg="Head GC completed" component=tsdb caller=truncateMemory duration=1.450822ms
prometheus  | prometheus  | time=2026-02-07T05:02:04.941Z level=INFO source=compact.go:565 msg="write block started" component=tsdb mint=1770422400000 maxt=1770429600000 ulid=01KGV7N14D74HBYMQMZBXH8E4A
prometheus  | prometheus  | time=2026-02-07T05:02:04.953Z level=INFO source=compact.go:590 msg="write block resulted in empty block" component=tsdb mint=1770422400000 maxt=1770429600000 duration=11.935955ms
prometheus  | prometheus  | time=2026-02-07T05:02:04.953Z level=INFO source=head.go:1420 msg="Head GC started" component=tsdb caller=truncateMemory
prometheus  | prometheus  | time=2026-02-07T05:02:04.954Z level=INFO source=head.go:1424 msg="Head GC completed" component=tsdb caller=truncateMemory duration=1.260802ms
prometheus  | prometheus  | time=2026-02-07T05:02:04.954Z level=INFO source=compact.go:565 msg="write block started" component=tsdb mint=1770429600000 maxt=1770436800000 ulid=01KGV7N14TGP8ZDCDCA5CJTPB3
prometheus  | prometheus  | time=2026-02-07T05:02:04.965Z level=INFO source=compact.go:590 msg="write block resulted in empty block" component=tsdb mint=1770429600000 maxt=1770436800000 duration=10.134182ms
prometheus  | prometheus  | time=2026-02-07T05:02:04.965Z level=INFO source=head.go:1420 msg="Head GC started" component=tsdb caller=truncateMemory
prometheus  | prometheus  | time=2026-02-07T05:02:04.967Z level=INFO source=head.go:1424 msg="Head GC completed" component=tsdb caller=truncateMemory duration=1.848929ms
prometheus  | prometheus  | time=2026-02-07T05:02:04.967Z level=INFO source=checkpoint.go:100 msg="Creating checkpoint" component=tsdb from_segment=263 to_segment=264 mint=1770436800000
prometheus  | prometheus  | time=2026-02-07T05:02:05.282Z level=INFO source=head.go:1385 msg="WAL checkpoint complete" component=tsdb first=263 last=264 duration=315.259797ms
prometheus  | prometheus  | time=2026-02-07T05:02:05.286Z level=INFO source=db.go:1423 msg="out-of-order compaction started" component=tsdb
prometheus  | prometheus  | time=2026-02-07T05:02:05.286Z level=INFO source=compact.go:565 msg="write block started" component=tsdb mint=1770379200000 maxt=1770386400000 ulid=01KGV7N1F663XTNWASS0CFZJA9
prometheus  | prometheus  | time=2026-02-07T05:02:05.305Z level=INFO source=compact.go:599 msg="write block completed" component=tsdb mint=1770379200000 maxt=1770386400000 ulid=01KGV7N1F663XTNWASS0CFZJA9 duration=19.260438ms ooo=true
prometheus  | prometheus  | time=2026-02-07T05:02:05.305Z level=INFO source=compact.go:565 msg="write block started" component=tsdb mint=1770386400000 maxt=1770393600000 ulid=01KGV7N1FSC2CBT1GP2EECA77V
prometheus  | prometheus  | time=2026-02-07T05:02:05.327Z level=INFO source=compact.go:599 msg="write block completed" component=tsdb mint=1770386400000 maxt=1770393600000 ulid=01KGV7N1FSC2CBT1GP2EECA77V duration=21.938599ms ooo=true
prometheus  | prometheus  | time=2026-02-07T05:02:05.327Z level=INFO source=compact.go:565 msg="write block started" component=tsdb mint=1770393600000 maxt=1770400800000 ulid=01KGV7N1GF15X54DQ737HXQ0QA
prometheus  | prometheus  | time=2026-02-07T05:02:05.346Z level=INFO source=compact.go:599 msg="write block completed" component=tsdb mint=1770393600000 maxt=1770400800000 ulid=01KGV7N1GF15X54DQ737HXQ0QA duration=18.928926ms ooo=true
prometheus  | prometheus  | time=2026-02-07T05:02:05.346Z level=INFO source=db.go:1458 msg="out-of-order compaction completed" component=tsdb duration=60.337718ms ulids="[01KGV7N1F663XTNWASS0CFZJA9 01KGV7N1FSC2CBT1GP2EECA77V 01KGV7N1GF15X54DQ737HXQ0QA]"
prometheus  | prometheus  | time=2026-02-07T05:02:05.347Z level=WARN source=db.go:1661 msg="Overlapping blocks found during reloadBlocks" component=tsdb detail="[mint: 1770393602579, maxt: 1770400800000, range: 1h59m57s, blocks: 2]: <ulid: 01KGV7N1GF15X54DQ737HXQ0QA, mint: 1770393600000, maxt: 1770400800000, range: 2h0m0s>, <ulid: 01KGV7N0XZDYH7ED6S204M1YF2, mint: 1770393602579, maxt: 1770400800000, range: 1h59m57s>"
prometheus  | prometheus  | time=2026-02-07T05:02:05.348Z level=INFO source=db.go:1825 msg="Deleting obsolete block" component=tsdb block=01KGV7N1FSC2CBT1GP2EECA77V
prometheus  | prometheus  | time=2026-02-07T05:02:05.349Z level=INFO source=db.go:1825 msg="Deleting obsolete block" component=tsdb block=01KGV7N1F663XTNWASS0CFZJA9
prometheus  | prometheus  | time=2026-02-07T05:02:05.349Z level=INFO source=head.go:1420 msg="Head GC started" component=tsdb caller=truncateOOO
prometheus  | prometheus  | time=2026-02-07T05:02:05.350Z level=INFO source=head.go:1424 msg="Head GC completed" component=tsdb caller=truncateOOO duration=1.402724ms
prometheus  | prometheus  | time=2026-02-07T05:02:05.358Z level=INFO source=compact.go:798 msg="Found overlapping blocks during compaction" component=tsdb ulid=01KGV7N1H7C5E0ZW3HVJJNCJ2C
prometheus  | prometheus  | time=2026-02-07T05:02:06.158Z level=INFO source=compact.go:537 msg="compact blocks" component=tsdb count=2 mint=1770393600000 maxt=1770400800000 ulid=01KGV7N1H7C5E0ZW3HVJJNCJ2C sources="[01KGV7N1GF15X54DQ737HXQ0QA 01KGV7N0XZDYH7ED6S204M1YF2]" duration=806.700662ms
prometheus  | prometheus  | time=2026-02-07T05:02:06.159Z level=INFO source=db.go:1825 msg="Deleting obsolete block" component=tsdb block=01KGV7N1GF15X54DQ737HXQ0QA
prometheus  | prometheus  | time=2026-02-07T05:02:06.160Z level=INFO source=db.go:1825 msg="Deleting obsolete block" component=tsdb block=01KGV7N0XZDYH7ED6S204M1YF2
quote  | quote  | Listening on: 0.0.0.0:8090
recommendation  | recommendation  | 2026-02-07 05:02:25,144 INFO [main] [recommendation_server.py:47] [trace_id=2dcaa6601d3413a2867b9eaf9529b30e span_id=beab4a058ffc54fa resource.service.name=recommendation trace_sampled=True] - Receive ListRecommendations for product ids:['66VCHSJNUP', '0PUK6V6EV0', 'OLJCESPC7Z', '9SIQT8TOJO', 'L9ECAV7KIM']
recommendation  | recommendation  | 2026-02-07 05:02:40,116 INFO [main] [recommendation_server.py:47] [trace_id=015321810f4a3d6fb6ab13b8116fde0f span_id=8c497c86e3dd158c resource.service.name=recommendation trace_sampled=True] - Receive ListRecommendations for product ids:['1YMWWN1N4O', '2ZYFJ3GM2N', 'OLJCESPC7Z', '66VCHSJNUP', '6E92ZMYYFZ']
valkey-cart  | valkey-cart  | 1:M 07 Feb 2026 05:01:58.473 # WARNING Memory overcommit must be enabled! Without it, a background save or replication may fail under low memory condition. Being disabled, it can also cause failures without low memory condition, see https://github.com/jemalloc/jemalloc/issues/1328. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.
valkey-cart  | valkey-cart  | 1:M 07 Feb 2026 05:01:58.473 * oO0OoO0OoO0Oo Valkey is starting oO0OoO0OoO0Oo
valkey-cart  | valkey-cart  | 1:M 07 Feb 2026 05:01:58.473 * Valkey version=9.0.0, bits=64, commit=00000000, modified=0, pid=1, just started
valkey-cart  | valkey-cart  | 1:M 07 Feb 2026 05:01:58.473 # Warning: no config file specified, using the default config. In order to specify a config file use valkey-server /path/to/valkey.conf
valkey-cart  | valkey-cart  | 1:M 07 Feb 2026 05:01:58.474 * Increased maximum number of open files to 10032 (it was originally set to 1024).
valkey-cart  | valkey-cart  | 1:M 07 Feb 2026 05:01:58.474 * monotonic clock: POSIX clock_gettime
valkey-cart  | valkey-cart  | 1:M 07 Feb 2026 05:01:58.474 * Running mode=standalone, port=6379.
valkey-cart  | valkey-cart  | 1:M 07 Feb 2026 05:01:58.475 * Server initialized
valkey-cart  | valkey-cart  | 1:M 07 Feb 2026 05:01:58.475 * Cleaning slot migration log in anticipation of a load operation.
valkey-cart  | valkey-cart  | 1:M 07 Feb 2026 05:01:58.475 * Loading RDB produced by Valkey version 9.0.0
valkey-cart  | valkey-cart  | 1:M 07 Feb 2026 05:01:58.475 * RDB age 44756 seconds
valkey-cart  | valkey-cart  | 1:M 07 Feb 2026 05:01:58.475 * RDB memory usage when created 1.23 Mb
valkey-cart  | valkey-cart  | 1:M 07 Feb 2026 05:01:58.475 * Done loading RDB, keys loaded: 1, keys expired: 142.
valkey-cart  | valkey-cart  | 1:M 07 Feb 2026 05:01:58.475 * DB loaded from disk: 0.001 seconds
valkey-cart  | valkey-cart  | 1:M 07 Feb 2026 05:01:58.475 * Ready to accept connections tcp